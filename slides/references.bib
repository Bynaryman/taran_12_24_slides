% ------------------------------------------------------------------------
% SAMPLE BIBLIOGRAPHY FILE
% ------------------------------------------------------------------------
@misc{cve-2008-1368,
  key =          {CVE-2008-1368},
  title =        {Publication quality tables in \LaTeX*},
  howpublished = {},
  institution  = {NIST},
  day =       17,
  month =     {March},
  year =         2008,
  note =         {[online] \url{http://nvd.nist.gov/nvd.cfm?cvename=CVE-2008-1368}},
  url = {http://nvd.nist.gov/nvd.cfm?cvename=CVE-2008-1368}
}

@MISC{prime-number-theorem,
   author = "Charles Louis Xavier Joseph de la Vall{\'e}e Poussin",
   note = "A strong form of the prime number theorem, 19th century" }

@BOOK{texbook,
   author = "Donald E. Knuth",
   title= "The {{\TeX}book}",
   publisher = "Addison-Wesley",
   year = 1984 }

@BOOK{latex,
   author = "Leslie Lamport",
   title = "{\LaTeX:} {A} Document Preparation System",
   publisher = "Addison-Wesley",
   year = 1986 }

@article{Ancey1996,
author = {Ancey, Christophe and Coussot, Philippe and Evesque, Pierre},
journal = {Mechanics of Cohesive-frictional Materials},
number = {4},
pages = {385--403},
title = {Examination of the possibility of a fluid-mechanics treatment of dense granular flows},
url = {http://doi.wiley.com/10.1002/(SICI)1099-1484(199610)1:4<385::AID-CFM20>3.0.CO;2-0},
volume = {1},
year = {1996}
}

@BOOK{RR73,
 author={H. Radjavi and P. Rosenthal},
 title={Invariant {Subspaces}},
 publisher={Springer-Verlag},
 address={New York},
 year={1973},
}

@BOOK{Aup91,
 author={B. Aupetit},
 title={A {Primer} on {Spectral} {Theory}},
 publisher={Springer-Verlag},
 address={New York},
 year={1991},
}

@BOOK{Dou72,
 author={R. G. Douglas},
 title={Banach {Algebra} {Techniques} in {Operator} {Theory}},
 publisher={Academic Press},
 address={New York},
 year={1972},
}

@BOOK{Hal82,
 author={P. R. Halmos},
 title={A {Hilbert} {Space} {Problem} {Book}},
 edition={Second},
 publisher={Springer-Verlag},
 address={New York},
 year={1982},
}

@BOOK{Rud73,
 author={W. Rudin},
 title={Functional {Analysis}},
 publisher={McGraw-Hill},
 address={New York},
 year={1973},
}

@BOOK{Con90,
 author={J. B. Conway},
 title={A {Course} in {Functional} {Analysis}},
 edition={Second},
 publisher={Springer-Verlag},
 address={New York},
 year={1990},
}

@BOOK{Con78,
 author={J. B. Conway},
 title={Functions of {One} {Complex} {Variable}},
 publisher={Springer-Verlag},
 address={New York},
 year={1978},
}

@BOOK{KR83,
 author={R. V. Kadison and J. R. Ringrose},
 title={Fundamentals of the {Theory} of {Operator} {Algebras},
        {Part} {I}},
 publisher={Academic Press},
 address={New York},
 year={1983},
}

@BOOK{KR86,
 author={R. V. Kadison and J. R. Ringrose},
 title={Fundamentals of the {Theory} of {Operator} {Algebras},
        {Part} {II}},
 publisher={Academic Press},
 address={New York},
 year={1986},
}

@INBOOK{SFPT,
 author={N. Dunford and J. T. Schwartz},
 title={Linear {Operators},
        {Part} {I}: {General} {Theory}},
 pages={456},
 publisher={Interscience},
 address={New York},
 year={1957},
}

@BOOK{DS57,
 author={N. Dunford and J. T. Schwartz},
 title={Linear {Operators},
        {Part} {I}: {General} {Theory}},
 publisher={Interscience},
 address={New York},
 year={1957},
}

@BOOK{Gan59,
 author={F. R. Gantmacher},
 title={Applications of the {Theory} of {Matrices}},
 publisher={Interscience},
 address={New York},
 year={1959},
}

@BOOK{Pau86,
 author={Vern I. Paulsen},
 title={Completely bounded maps and dilations},
 series={Pitman Research Notes in Mathematics Series},
 volume={146},
 publisher={Longman Scientific \& Technical},
 address={Harlow UK},
 year={1986},
}

@BOOK{Dav88,
 author={Kenneth R. Davidson},
 title={Nest algebras},
 series={Pitman Research Notes in Mathematics Series},
 volume={191},
 publisher={Longman Scientific \& Technical},
 address={Harlow UK},
 year={1988},
}

@BOOK{Spi65,
 author={Michael Spivak},
 title={Calculus on {Manifolds}},
 publisher={The Benjamin/Cummings Publishing Company},
 address={New York},
 year={1965},
}

@BOOK{Dev68,
 author={Allen Devinaz},
 title={Advanced {Calculus}},
 publisher={Holt, Rinehart and Winston},
 address={New York},
 year={1968},
}

@BOOK{Gam90,
 editor={R. V. Gamkerlidze},
 title={Analysis {I}{I}: {Convex} {Analysis} and
        {Approximation} {Theory}},
 series={Encyclopaedia of Mathematical Sciences},
 volume={14},
 publisher={Springer-Verlag},
 address={New York},
 year={1990},
}

@BOOK{Hen93,
 author={Peter Henderson},
 title={Object-oriented specification and design with {C}$++$},
 publisher={McGraw-Hill},
 address={London},
 year={1993},
}

@ARTICLE{Rea85,
 author={C. J. Read},
 title={A solution to the invariant subspace problem on the space $l_1$},
 journal={Bull. London Math. Soc.},
 volume={17},
 year={1985},
 pages={305-317},
}

@ARTICLE{Enf87,
 author={P. Enflo},
 title={On the invariant subspaces problem for {Banach} spaces},
 journal={Acta. Math.},
 note={Seminare Maurey-Schwartz (1975-1976)},
 volume={158},
 year={1987},
 pages={213-313},
}

@ARTICLE{Dau75,
 author={J. Daughtry},
 title={An invariant subspace theorem},
 journal={Proc. Amer. Math. Soc.},
 volume={49},
 year={1975},
 pages={267-268},
}

@ARTICLE{KPS75,
 author={H. W. Kim and C. Pearcy and A. L. Shields},
 title={Rank-One Commutators and Hyperinvariant Subspaces},
 journal={Michigan Math. J.},
 volume={22},
 number={3},
 year={1975},
 pages={193-194},
}


@ARTICLE{Rad87,
 author={H. Radjavi},
 title={The {Engel}-{Jacobson} {Theorem} {Revisited}},
 journal={J. Alg.},
 volume={111},
 year={1987},
 pages={427-430},
}

@ARTICLE{MOR91,
 author={B. Mathes and M. Omladi\v{c} and H. Radjavi},
 title={Linear {Spaces} of {Nilpotent} {Operators}},
 journal={Linear Algebra Appl.},
 volume={149},
 year={1991},
 pages={215-225},
}

@ARTICLE{Lom73,
 author={V. I. Lomonosov},
 title={Invariant subspaces for operators commuting with compact
        operators},
 journal={Functional Anal. Appl.},
 volume=7,
 year=1973,
 pages="213-214",
}

@ARTICLE{Lom91,
 author={V. I. Lomonosov},
 title={An extension of {Burnside}'s theorem to infinite
        dimensional spaces},
 journal={Israel J. Math},
 volume=75,
 year=1991,
 pages="329-339",
}

@ARTICLE{Lom92,
 author={V. I. Lomonosov},
 title={On {Real} {Invariant} {Subspaces} of {Bounded} {Operators} with
       {Compact} {Imaginary} {Part}},
 journal={Proc. Amer. Math. Soc.},
 volume=115,
 number=3,
 month=jul,
 year=1992,
 pages="775-777",
}

@ARTICLE{dB59,
 author={L. de Branges},
 title={The {Stone}-{Weierstrass} {Theorem}},
 journal={Proc. Amer. Math. Soc.},
 volume=10,
 year=1959,
 pages="822-824",
}

@ARTICLE{dB93,
 author={L. de Branges},
 title={A construction of invariant subspaces},
 journal={Math. Nachr.},
 volume=163,
 year=1993,
 pages="163-175",
}

@ARTICLE{AAB95,
 author={Y. A. Abramovich and C. D. Aliprantis and O. Burkinshaw},
 title={Another Characterization of the Invariant Subspace Problem},
 journal={Operator Theory in Function Spaces and Banach Lattices.
          {\em The A.C.\,Zaanen Anniversary Volume},
          Operator Theory: Advances and Applications},
 volume={75},
 year={1995},
 pages={15-31},
 note={Birkh\"auser Verlag},
}

@ARTICLE{LM65,
 author={Ju. I. Ljubi\v{c} and V. I. Macaev},
 title={On Operators with a Separable Spectrum},
 journal={Amer. Math. Soc. Transl. (2)},
 volume={47},
 year={1965},
 pages={89-129},
}

% ------------------------------------------------------------------------

@MASTERSTHESIS{Sim90,
 author={A. Simoni\v{c}},
 title={Grupe Operatorjev s Pozitivnim Spektrom},
 school={Univerza v Ljubljani, FNT, Oddelek za Matematiko},
 year={1990},
}

@UNPUBLISHED{Sim91,
 author={A. Simoni\v{c}},
 title={Notes on {Subharmonic} {Functions}},
 note={Lecture Notes, Dalhousie University,
       Department of Mathematics, Statistics, \& Computing Science},
 year={1991},
}

@ARTICLE{Sim92,
 author={A. Simoni\v{c}},
 title={Matrix {Groups} with {Positive} {Spectra}},
 journal={Linear Algebra Appl.},
 volume={173},
 year={1992},
 pages={57-76},
}

@PHDTHESIS{Sim94,
 author={A. Simoni\v{c}},
 title={An {Extension} of {Lomonosov's} {Techniques} to {Non}-{Compact}
       {Operators}},
 school={Dalhousie University,
         Department of Mathematics, Statistics, \& Computing Science},
 year={1994},
}

@ARTICLE{Sim96a,
 author={A. Simoni\v{c}},
 title={A {Construction} of {Lomonosov} {Functions} and
       {Applications} to the {Invariant} {Subspace} {Problem}},
 journal={Pacific J. Math.},
 volume={175},
 pages={257-270},
 year={1996},
}

@ARTICLE{Sim96b,
 author={A. Simoni\v{c}},
 title={An extension of {Lomonosov's} {Techniques} to non-compact
       {Operators}},
 journal={Trans. Amer. Math. Soc.},
 volume={348},
 pages={975-995},
 year={1996},
}

@inproceedings{doe,
	author = {Jack Dongarra and Jeffrey Hittinger and John Bell and Luis Chacon},
	title = {Applied Mathematics Research for Exascale Computing},
	year = {2014}
}
@inproceedings{ac1,
    booktitle = {{18th IEEE European Test Symposium (ETS)}},
    author = {Jie Han and Michael Orshansky},
    title = {{Approximate Computing: An Emerging Paradigm For Energy-Efficient Design}},
    year = {{2013}}
}
@inproceedings{ac2,
	booktitle = {{Proceedings of the 52nd Annual Design Automation Conference (DAC'15)}},
	author = {Swagath Venkataraman and Srimat T. Chakradhar and Kaushik Roy and Anand Raghunathan},
	title = {Approximate Computing and the Quest for Computing Efficiency},
	year = {2015}
}
@inproceedings{ompss,
	booktitle = {{Parallel Processing Letters}},
	author = {Alex Duran and Eduard Ayguad\'{e} and Rosa Maria Badia and Jesus Labarta and Xavier Martorell and Judit Planes},
	title = {OmpSs: A PROPOSAL FOR PROGRAMMING HETEROGENEOUS MULTI-CORE ARCHITECTURES},
	year = {2011}
}

@inproceedings{acit,
	booktitle = {{Proceedings of the 51nd Annual Design Automation Conference (DAC'14)}},
	author = {Qian Zhang and Feng Yuan and Rong Ye and Qiang Xu},
	title = {ApproxIt: An Approximate Computing Framework for Iterative Methods},
	year = {2014}
}
@inproceedings{cam,
	booktitle = {{Cambridge University Press}},
	author = {M. Mitzenmacher and E. Upfal},
	title = {Probability and Computing: Randomized Algorithms and Probabilistic Analysis},
	year = {2005}
}
@inproceedings{inherent,
	booktitle = {{Proceedings of the 50th Annual Design Automation Conference (DAC'13)}},
	author = {V.K. Chippa and K. Roy and S.T. Charadhar and A. Raghunathan},
	title = {Analysis and characterization of inherent application resilience for approximate computing},
	year = {2013}
}
@inproceedings{circuit1,
	booktitle = {{IEEE Transactions on VLSI Systems}},
	author = {V.K. Chippa and K. Roy and S.T. Charadhar and A. Raghunathan},
	title = {Scalable effor hardware design},
	year = {2014}
}
@inproceedings{circuit2,
	booktitle = {{Proceeding DATE}},
	author = {Nilanjan Banerjee and Georgios Karakonstantis and Kaushik Roy},
	title = {Process Variation Tolerant Low Power DCT Architecture},
	year = {2007}
}
@inproceedings{circuit3,
	booktitle = {{ACM/IEEE International Symposium on Low Power Electronics and Design}},
	author = {Nilanjan Banerjee and Hwan Choi Jung and Kaushik Roy},
	title = {A process variation aware low power synthesis methodology for fixed-point FIR filters},
	year = {2007}
}
@inproceedings{circuit4,
	booktitle = {{ACM/IEEE International Conference on Computer-Aided Design}},
	author = {Georgios Karakonstantis and Nilanjan Banerjee and Kaushik Roy and Chaitali Chakrabarti},
	title = {Design methodology to trade off power, output quality and error resiliency: application to color interpolation filtering},
	year = {2007}
}
@inproceedings{circuit5,
	booktitle = {{Design, Automation \& Test in Europe}},
	author = {Debabrata Mohapatra and Vinay K. Chippa and Anand Raghunathan and Kaushik Roy},
	title = {Design of voltage-scable meta-functions for approximate computing},
	year = {2011}
}
@inproceedings{adder1,
	booktitle = {{IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS}},
	author = {Vaibhav Gupta and Debabrata Mohapatra and Anand Raghunathan},
	title = {Low-Power Digital Signal Processing Using Approximate Adders},
	year = {2013}
}
@inproceedings{adder2,
	booktitle = {{Proceedings of the 13th IEEE International Conference on Nanotechnology}},
	author = {Zhixi Yang and Ajaypat Jain and Jinghang Liang and Jie Han and Fabrizio Lombardi},
	title = {Approximate XOR/XNOR-based Adders for Inexact Computing},
	year = {2013}
}
@inproceedings{adders1,
	booktitle = {{IEEE Computer}},
	author = {Shih-Lien Lu},
	title = {Speeding up processing with approximation circuits},
	year = {2004}
}
@inproceedings{adders2,
	booktitle = {{Design, Automation and Test in Europe, 2008. DATE '08}},
	author = {Ajay K. Verma and Philip Brisk and Paolo Ienne},
	title = {Variable Latency Speculative Addition: A New Paradigm for Arithmetic Circuit Design},
	year = {2008}
}
@inproceedings{adders3,
	booktitle = {{IEEE Transactions on Very Large Scale Integration (VLSI) Systems}},
	author = {Ning Zhu and Wang Lin Goh and Weijia Zhang and Kiat Seng Yeo and Zhihui Kong},
	title = {Design of Low-Power High-Speed Truncation-Error-Tolerant Adder and Its Application in Digital Signal Processing},
	year = {2011}
}
@inproceedings{adders4,
	booktitle = {{International SoC Design Conference (ISOCC)}},
	author = {Ning Zhu and Wang Ling Goh, Kiat Seng Yeo},
	title = {Ultra low-power high-speed flexible Probabilistic Adder for Error-Tolerant Applications},
	year = {2011}
}
@inproceedings{mul1,
	booktitle = {{IEEE Transactions on Circuits and Systems I}},
	author = {H.R. Mahdiani and A. Ahmadi and S.M. Fakhraie and C. Lucas},
	title = {Bio-Inspired Imprecise Computational Blocks for Efficient VLSI Implementation of Soft-Computing Applications},
	year = {2010}
}
@inproceedings{mul2,
	booktitle = {{IEEE International Conference on Electron Devices and Solid-State Circuits (EDSSC)}},
	author = {Khaing Yin Kyaw and Wang Ling Goh and Kiat Seng Yeo},
	title = {Low-power high-speed multiplier for error-tolerant application},
	year = {2010}
}
@inproceedings{mul3,
	booktitle = {{International Conference on VLSI Design}},
	author = {P. Kulkarni and P. Gupta and M. Ercegovac},
	title = {Trading Accuracy for Power with an Underdesigned Multiplier Architecture},
	year = {2011}
}
@inproceedings{syn1,
	booktitle = {{Proceeding on DATE}},
	author = {D. Shin and S.K. Gupta},
	title = {Approximate logic synthesis for error tolerant applications},
	year = {2010}
}
@inproceedings{syn2,
	booktitle = {{Proceedings on DATE}},
	author = {D. Shin and S.K. Gupta},
	title = {A new circuit simplification method for error tolerant applications},
	year = {2011}
}
@inproceedings{significance,
	booktitle = {{Proceedings of the 2009 ACM/IEEE international symposium on Low power electronics and design}},
	author = {Debabrata Mohapatra and Georgios Karakonstantis and Kaushik Roy},
	title = {Significance driven computation: a voltage-scalable, variation-aware, quality-tuning motion estimator},
	year = {2009}
}
@inproceedings{effort1,
	booktitle = {{47th IEEE/ACM Design Automation Conference (DAC)}},
	author = {V.K. Chippa and D. Mophapatra and A. Raghunathan and K. Roy and S.T. Chakradhar},
	title = {Scalable effort hardware design: Exploiting algorithmic resilience for energy efficiency},
	year = {2010}
}
@inproceedings{effort2,
	booktitle = {{Proceedings of Asilomar conf. on signals}},
	author = {V.K. Chippa and D. Mophapatra and A. Raghunathan and K. Roy and S.T. Chakradhar},
	title = {Approximate computing: An integrated hardware approach},
	year = {2013}
}
@inproceedings{effort3,
	booktitle = {{Proceedings of DAC}},
	author = {V.K. Chippa and D. Mophapatra and A. Raghunathan and K. Roy and S.T. Chakradhar},
	title = {Scalable effort hardware design: Exploiting algorithmic resilience for energy efficiency},
	year = {2010}
}
@inproceedings{effort4,
	booktitle = {{Proceedings of DAC}},
	author = {V.K. Chippa and D. Mophapatra and A. Raghunathan and K. Roy and S.T. Chakradhar},
	title = {Dynamic effort scaling: Managing the quality-efficiency tradeoff},
	year = {2011}
}
@inproceedings{archi,
	booktitle = {{Proceedings of MICRO}},
	author = {S. Venkataramani},
	title = {Quality programmable vector processors for approximate computing},
	year = {2013}
}
@inproceedings{perforation,
	booktitle = {{MIT Computer Science and Artificial Intelligence Laboratory Technical Report}},
	author = {Henry Hoffmann and Sasa Misailovic and Stelios Sidiroglou and Anant Agarwal and Martin Rinard},
	title = {Using Code Perforation to Improve Performance, Reduce Energy Consumption, and Respond to Failures},
	year = {2009}
}
@inproceedings{profile1,
	booktitle = {{OOPSLA'07}},
	author = {Martin Rinard},
	title = {Using Early Phase Termination To Eliminate Load Imbalances At Barrier Synchronization Points},
	year = {2007}
}
@inproceedings{profile2,
	booktitle = {{ICS'06}},
	author = {Martin Rinard},
	title = {Probabilistic Accuracy Bounds for Fault-Tolerant Computations that Discard Tasks},
	year = {2006}
}
@manual{plasma,
	title = {PLASMA Users’ Guide, Parallel Linear Algebra Software for Multicore Archtectures, Version 2.0},
	organization = {University of Tennessee},
	year = {2009}
}
@article{itref,
	author = {Moler, Cleve B.},
	title = {Iterative Refinement in Floating Point},
	journal = {J. ACM},
	issue_date = {April 1967},
	volume = {14},
	number = {2},
	month = apr,
	year = {1967},
	issn = {0004-5411},
	pages = {316--321},
	numpages = {6},
	url = {http://doi.acm.org/10.1145/321386.321394},
	doi = {10.1145/321386.321394},
	acmid = {321394},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@article{tensorflow,
  author    = {Mart{\'{\i}}n Abadi and
               Ashish Agarwal and
               Paul Barham and
               Eugene Brevdo and
               Zhifeng Chen and
               Craig Citro and
               Gregory S. Corrado and
               Andy Davis and
               Jeffrey Dean and
               Matthieu Devin and
               Sanjay Ghemawat and
               Ian J. Goodfellow and
               Andrew Harp and
               Geoffrey Irving and
               Michael Isard and
               Yangqing Jia and
               Rafal J{\'{o}}zefowicz and
               Lukasz Kaiser and
               Manjunath Kudlur and
               Josh Levenberg and
               Dan Man{\'{e}} and
               Rajat Monga and
               Sherry Moore and
               Derek Gordon Murray and
               Chris Olah and
               Mike Schuster and
               Jonathon Shlens and
               Benoit Steiner and
               Ilya Sutskever and
               Kunal Talwar and
               Paul A. Tucker and
               Vincent Vanhoucke and
               Vijay Vasudevan and
               Fernanda B. Vi{\'{e}}gas and
               Oriol Vinyals and
               Pete Warden and
               Martin Wattenberg and
               Martin Wicke and
               Yuan Yu and
               Xiaoqiang Zheng},
  title     = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
               Systems},
  journal   = {CoRR},
  volume    = {abs/1603.04467},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.04467},
  archivePrefix = {arXiv},
  eprint    = {1603.04467},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AbadiABBCCCDDDG16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{horovod,
  author    = {Alexander Sergeev and
               Mike Del Balso},
  title     = {Horovod: fast and easy distributed deep learning in TensorFlow},
  journal   = {CoRR},
  volume    = {abs/1802.05799},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05799},
  archivePrefix = {arXiv},
  eprint    = {1802.05799},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05799},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{rnn,
author={A. Graves and M. Liwicki and S. Fernández and R. Bertolami and H. Bunke and J. Schmidhuber},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={A Novel Connectionist System for Unconstrained Handwriting Recognition},
year={2009},
volume={31},
number={5},
pages={855-868},
keywords={handwriting recognition;handwritten character recognition;hidden Markov models;image segmentation;recurrent neural nets;unconstrained handwriting text recognition;connectionist system;overlapping character segmentation;language modeling;hidden Markov models;recurrent neural network;unconstrained handwriting databases;Handwriting recognition;Hidden Markov models;Character recognition;Text recognition;Speech;Recurrent neural networks;Labeling;Databases;Robustness;Size measurement;Handwriting recognition;online handwriting;offline handwriting;connectionist temporal classification;bidirectional long short-term memory;recurrent neural networks;hidden Markov model.;Unconstrained handwriting recognition;Recurrent neural networks;Online handwriting recognition;Offline handwriting recognition;Long Short-Term Memory;Connectionist temporal classification;Algorithms;Automatic Data Processing;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique},
doi={10.1109/TPAMI.2008.137},
ISSN={0162-8828},
month={May},}

@article{rnn1,
  author    = {Xiangang Li and
               Xihong Wu},
  title     = {Constructing Long Short-Term Memory based Deep Recurrent Neural Networks
               for Large Vocabulary Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1410.4281},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.4281},
  archivePrefix = {arXiv},
  eprint    = {1410.4281},
  timestamp = {Mon, 13 Aug 2018 16:47:15 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LiW14a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}
@article{zfnet,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1311.2901v3},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {doctoradoBSC},
number = {PART 1},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@inproceedings{inception,
title	= {Going Deeper with Convolutions},
author	= {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year	= {2015},
URL	= {http://arxiv.org/abs/1409.4842},
booktitle	= {Computer Vision and Pattern Recognition (CVPR)}
}

@article{vgg,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3Ã—3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16â€“19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa- tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
journal = {ICLR Conference Paper},
mendeley-groups = {doctoradoBSC},
pages = {1--9},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {https://arxiv.org/abs/1409.1556},
year = {2015}
}

@article{resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
mendeley-groups = {doctoradoBSC},
pages = {1--12},
title = {{Deep Residual Learning for Image Recognition}},
year = {2015}
}

@article{Wu2018,
abstract = {Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as "WAGE" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.},
archivePrefix = {arXiv},
arxivId = {1802.04680},
author = {Wu, Shuang and Li, Guoqi and Chen, Feng and Shi, Luping},
eprint = {1802.04680},
mendeley-groups = {doctoradoBSC},
month = feb,
pages = {1--14},
title = {{Training and Inference with Integers in Deep Neural Networks}},
url = {http://arxiv.org/abs/1802.04680},
year = {2018}
}

@article{Sze2017,
abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
archivePrefix = {arXiv},
arxivId = {1703.09039},
author = {Sze, Vivienne and Chen, Yu Hsin and Yang, Tien Ju and Emer, Joel S.},
doi = {10.1109/JPROC.2017.2761740},
eprint = {1703.09039},
isbn = {9781538612330},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {ASIC,VLSI,computer architecture,convolutional neural networks,dataflow processing,deep learning,deep neural networks,energy-efficient accelerators,low power,machine learning,spatial architectures},
mendeley-groups = {doctoradoBSC},
number = {12},
pages = {2295--2329},
pmid = {22164016},
title = {{Efficient Processing of Deep Neural Networks: A Tutorial and Survey}},
volume = {105},
year = {2017}
}

@article{Sung2014,
abstract = {Feedforward deep neural networks that employ multiple hidden layers show high performance in many applications, but they demand complex hardware for implementation. The hardware complexity can be much lowered by minimizing the word-length of weights ...},
author = {Sung, Wonyong and Hwang, Kyuyeon},
doi = {10.1109/SiPS.2014.6986082},
isbn = {978-1-4799-6588-5},
issn = {2162-3562},
journal = {SiPS},
keywords = {Deep Learning,Machine Learning},
mendeley-groups = {doctoradoBSC},
pages = {174--179},
title = {{Fixed-point feedforward deep neural network design using weights +1, 0, and -1.}},
url = {http://dx.doi.org/10.1109/SiPS.2014.6986082{\%}5Cnfile:///Files/EF/EF6EB072-FF5E-4EDF-B10C-887AD4BF059A.pdf{\%}5Cnpapers3://publication/doi/10.1109/SiPS.2014.6986082},
year = {2014}
}

@article{Kalamkar2019,
abstract = {This paper presents the first comprehensive empirical study demonstrating the efficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classification, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 floating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision floating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the flow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorflow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.},
archivePrefix = {arXiv},
arxivId = {1905.12322},
author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
eprint = {1905.12322},
mendeley-groups = {doctoradoBSC},
pages = {1--10},
title = {{A Study of BFLOAT16 for Deep Learning Training}},
url = {http://arxiv.org/abs/1905.12322},
year = {2019}
}

@article{Luk2005,
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium{\textregistered}, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
author = {Luk, Chi Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
doi = {10.1145/1064978.1065034},
isbn = {1595930566},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {Dynamic compilation,Instrumentation,Program analysis tools},
mendeley-groups = {doctoradoBSC},
number = {6},
pages = {190--200},
title = {{Pin: Building customized program analysis tools with dynamic instrumentation}},
volume = {40},
year = {2005}
}

@article{Koster2017,
abstract = {Deep neural networks are commonly developed and trained in 32-bit floating point format. Significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit floating point format training and inference, designed to support modern deep network topologies without modifications. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range. We validate Flexpoint by training AlexNet, a deep residual network and a generative adversarial network, using a simulator implemented with the neon deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit floating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.},
archivePrefix = {arXiv},
arxivId = {1711.02213},
author = {K{\"{o}}ster, Urs and Webb, Tristan J. and Wang, Xin and Nassar, Marcel and Bansal, Arjun K. and Constable, William H. and Elibol, OÄŸuz H. and Gray, Scott and Hall, Stewart and Hornof, Luke and Khosrowshahi, Amir and Kloss, Carey and Pai, Ruby J. and Rao, Naveen},
eprint = {1711.02213},
isbn = {9781538642146},
issn = {10495258},
mendeley-groups = {doctoradoBSC},
pages = {1--14},
title = {{Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks}},
url = {http://arxiv.org/abs/1711.02213},
year = {2017}
}

@article{Dean2012,
abstract = {Abstract Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of ...},
author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {doctoradoBSC},
pages = {1223--1231},
title = {{Large scale distributed deep networks}},
volume = {2},
year = {2012}
}

@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
mendeley-groups = {doctoradoBSC},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}

@article{Gong2018,
abstract = {High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents the efficient inference techniques of IntelCaffe, the first Intel optimized deep learning framework that supports efficient 8-bit low precision inference and model optimization techniques of convolutional neural networks on Intel Xeon Scalable Processors. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with neglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe. All these techniques have been open-sourced on IntelCaffe GitHub1, and the artifact is provided to reproduce the result on Amazon AWS Cloud.},
archivePrefix = {arXiv},
arxivId = {1805.08691},
author = {Gong, Jiong and Shen, Haihao and Zhang, Guoming and Liu, Xiaoli and Li, Shane and Jin, Ge and Maheshwari, Niharika and Fomenko, Evarist and Segal, Eden},
doi = {10.1007/BF02653965},
eprint = {1805.08691},
isbn = {0360-2141},
issn = {03602141},
mendeley-groups = {doctoradoBSC},
month = may,
pages = {1--4},
title = {{Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe}},
url = {http://arxiv.org/abs/1805.08691},
year = {2018}
}

@online{tensorcores,
author = {nVidia},
title = {nVidia Tensor Cores},
url = {https://www.nvidia.com/en-us/data-center/tensorcore/}
}

@article{Paper2018,
author = {Intel Intel},
month = nov,
pages = {1--7},
title = {{BFLOAT16-Hardware Numerics Definition}},
url = {https://software.intel.com/sites/default/files/managed/40/8b/bf16-hardware-numerics-definition-white-paper.pdf},
year = {2018}
}

@techreport{IEEE754_1985,
  title = {{IEEE Standard for Binary Floating-Point Arithmetic. IEEE STD 754-1985}},
  author = {{Institute of Electrical and Electronics Engineers}},
  institution = {IEEE},
  year = {1985},
  type = {Standard},
  number = {IEEE 754-1985},
  url = {https://ieeexplore.ieee.org/document/30711},
  doi = {10.1109/IEEESTD.1985.82928}
}

@techreport{IEEE754_2008,
  title = {{IEEE Standard for Floating-Point Arithmetic. IEEE STD 754-2008}},
  author = {{Institute of Electrical and Electronics Engineers}},
  institution = {IEEE},
  year = {2008},
  type = {Standard},
  number = {IEEE 754-2008},
  url = {https://ieeexplore.ieee.org/document/4610935},
  doi = {10.1109/IEEESTD.2008.4610935}
}

@techreport{IEEE754_2019,
  title = {{IEEE Standard for Floating-Point Arithmetic. IEEE STD 754-2019}},
  author = {{Institute of Electrical and Electronics Engineers}},
  institution = {IEEE},
  year = {2019},
  type = {Standard},
  number = {IEEE 754-2019},
  url = {https://ieeexplore.ieee.org/document/8766229},
  doi = {doi:10.1109/IEEESTD.2019.8766229}
}

@techreport{nvidia_hopper_2022,
	author = {Nvidia},
	title = {{NVIDIA} {Hopper} {Architecture} {In}-{Depth}},
	year = {2022},
	url = {https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/},
	journal = {NVIDIA Technical Blog}
}

@misc{dotzel_fliqs_2023,
	title = {{FLIQS}: {One}-{Shot} {Mixed}-{Precision} {Floating}-{Point} and {Integer} {Quantization} {Search}},
	shorttitle = {{FLIQS}},
	url = {http://arxiv.org/abs/2308.03290},
	doi = {10.48550/arXiv.2308.03290},
	abstract = {Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With the improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision quantization methods have performed a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our floating-point and integer quantization search (FLIQS) on multiple convolutional networks and vision transformer models to discover Pareto-optimal models. Our approach discovers models that improve upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With the proposed integer quantization search, we increase the accuracy of ResNet-18 on ImageNet by 1.31\% points and ResNet-50 by 0.90\% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98\% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69\% points with similar model cost on a MobileNetV2 search space.},
	publisher = {arXiv},
	author = {Dotzel, Jordan and Wu, Gang and Li, Andrew and Umar, Muhammad and Ni, Yun and Abdelfattah, Mohamed S. and Zhang, Zhiru and Cheng, Liqun and Dixon, Martin G. and Jouppi, Norman P. and Le, Quoc V. and Li, Sheng},
	month = aug,
	year = {2023},
	note = {arXiv:2308.03290 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{tortorella_redmule_2023,
	title = {{RedMule}: {A} {Mixed}-{Precision} {Matrix}-{Matrix} {Operation} {Engine} for {Flexible} and {Energy}-{Efficient} {On}-{Chip} {Linear} {Algebra} and {TinyML} {Training} {Acceleration}},
	shorttitle = {{RedMule}},
	url = {http://arxiv.org/abs/2301.03904},
	doi = {10.48550/arXiv.2301.03904},
	abstract = {The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with \{sign, exponent, mantissa\}=(\{1,4,3\}, \{1,5,2\}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V cores sharing a tightly-coupled data memory and implement the resulting system in a 22 nm technology. At its best efficiency point (@ 470 MHz, 0.65 V), the RedMulE-augmented PULP cluster achieves 755 GFLOPS/W and 920 GFLOPS/W during regular General Matrix-Matrix Multiplication (GEMM), and up to 1.19 TFLOPS/W and 1.67 TFLOPS/W when executing GEMM-Ops, respectively, for FP16 and FP8 input/output tensors. In its best performance point (@ 613 MHz, 0.8 V), RedMulE achieves up to 58.5 GFLOPS and 117 GFLOPS for FP16 and FP8, respectively, with 99.4\% utilization of the array of Computing Elements and consuming less than 60 mW on average, thus enabling on-device training of deep learning models in TinyML application scenarios while retaining the flexibility to tackle other classes of common linear algebra problems efficiently.},
	publisher = {arXiv},
	author = {Tortorella, Yvan and Bertaccini, Luca and Benini, Luca and Rossi, Davide and Conti, Francesco},
	month = may,
	year = {2023},
	note = {arXiv:2301.03904 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
}

@article{elster_nvidia_2022,
	title = {Nvidia {Hopper} {GPU} and {Grace} {CPU} {Highlights}},
	volume = {24},
	issn = {1558-366X},
	url = {https://ieeexplore.ieee.org/abstract/document/9789536},
	doi = {10.1109/MCSE.2022.3163817},
	abstract = {At GTC 2022, Nvidia announced a new product family that aims to cover from small enterprise workloads through exascale high performance computing (HPC) and trillion-parameter AI models. This column highlights the most interesting features of their new Hopper graphical processing unit (GPU) and Grace central processing unit (CPU) computer chips and the Hopper product family. We also discuss some of the history behind Nvidia technologies and their most useful features for computational scientists, such as the Hopper DPX dynamic programming (DP) instruction set, increased number of SMs, and FP 8 tensor core availability. Also included are descriptions of the new Hopper Clustered SMs architecture and updated NVSwitch technologies that integrate their new ARM-based Grace CPU.},
	number = {2},
	journal = {Computing in Science \& Engineering},
	author = {Elster, Anne C. and Haugdahl, Tor A.},
	month = mar,
	year = {2022},
	note = {Conference Name: Computing in Science \& Engineering},
	pages = {95--100},
}

@misc{intel_flexpoint:_2017,
	title = {Flexpoint: {Numerical} {Innovation} {Underlying} the {Intel}® {Nervana}™ {Neural} {Network} {Processor}},
	shorttitle = {Flexpoint},
	author = {Intel},
	url = {https://www.intel.ai/flexpoint-numerical-innovation-underlying-intel-nervana-neural-network-processor/},
	abstract = {The neon™ deep learning framework was created by Nervana Systems to deliver industry-leading performance. As of 2018, the neon framework is no longer being supported. We recommend customers to consider Intel optimized frameworks listed here. We are delighted to announce our research team’s new paper, Flexpoint: an adaptive numerical format for efficient training of deep [...]Read More...},
	language = {en-US},
	journal = {Intel AI},
	month = nov,
	year = {2017},
	keywords = {flexpoint},
}

@article{koster_flexpoint:_nodate,
	title = {Flexpoint: {An} {Adaptive} {Numerical} {Format} for {Efficient} {Training} of {Deep} {Neural} {Networks}},
	abstract = {Deep neural networks are commonly developed and trained in 32-bit ﬂoating point format. Signiﬁcant gains in performance and energy efﬁciency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit ﬂoating point format training and inference, designed to support modern deep network topologies without modiﬁcations. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overﬂows and maximize available dynamic range. We validate Flexpoint by training AlexNet [1], a deep residual network [2, 3] and a generative adversarial network [4], using a simulator implemented with the neon deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit ﬂoating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.},
	language = {en},
	author = {Köster, Urs and Webb, Tristan and Wang, Xin and Nassar, Marcel and Bansal, Arjun K and Constable, William and Elibol, Oguz and Gray, Scott and Hall, Stewart and Hornof, Luke and Khosrowshahi, Amir and Kloss, Carey and Pai, Ruby J and Rao, Naveen},
	pages = {11},
}

@inproceedings{boroumand_google_2018,
	address = {Williamsburg, VA, USA},
	title = {Google {Workloads} for {Consumer} {Devices}: {Mitigating} {Data} {Movement} {Bottlenecks}},
	isbn = {978-1-4503-4911-6},
	shorttitle = {Google {Workloads} for {Consumer} {Devices}},
	url = {http://dl.acm.org/citation.cfm?doid=3173162.3173177},
	doi = {10.1145/3173162.3173177},
	abstract = {We are experiencing an explosive growth in the number of consumer devices, including smartphones, tablets, web-based computers such as Chromebooks, and wearable devices. For this class of devices, energy efficiency is a first-class concern due to the limited battery capacity and thermal power budget. We find that data movement is a major contributor to the total system energy and execution time in consumer devices. The energy and performance costs of moving data between the memory system and the compute units are significantly higher than the costs of computation. As a result, addressing data movement is crucial for consumer devices. In this work, we comprehensively analyze the energy and performance impact of data movement for several widely-used Google consumer workloads: (1) the Chrome web browser; (2) TensorFlow Mobile, Google’s machine learning framework; (3) video playback, and (4) video capture, both of which are used in many video services such as YouTube and Google Hangouts. We find that processing-inmemory (PIM) can significantly reduce data movement for all of these workloads, by performing part of the computation close to memory. Each workload contains simple primitives and functions that contribute to a significant amount of the overall data movement. We investigate whether these primitives and functions are feasible to implement using PIM, given the limited area and power constraints of consumer devices. Our analysis shows that offloading these primitives to PIM logic, consisting of either simple cores or specialized accelerators, eliminates a large amount of data movement, and significantly reduces total system energy (by an average of 55.4\% across the workloads) and execution time (by an average of 54.2\%).},
	language = {en},
	booktitle = {Proceedings of the {Twenty}-{Third} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}  - {ASPLOS} '18},
	publisher = {ACM Press},
	author = {Boroumand, Amirali and Ranganathan, Parthasarathy and Mutlu, Onur and Ghose, Saugata and Kim, Youngsok and Ausavarungnirun, Rachata and Shiu, Eric and Thakur, Rahul and Kim, Daehyun and Kuusela, Aki and Knies, Allan},
	year = {2018},
	keywords = {data\_movement},
	pages = {316--331},
}
@INPROCEEDINGS{horowitz,
    author={M. {Horowitz}},
    booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
    title={1.1 Computing's energy problem (and what we can do about it)},
    year={2014},
    volume={},
    number={},
    pages={10-14},
    abstract={Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices.},
    keywords={performance evaluation;power aware computing;search engines;innovative computing devices;compute engines;computing performance;transistors;power;voltage scaling;CMOS integrated circuits;Hardware;Transistors;Voltage control;CMOS technology;Energy efficiency;Logic gates},
    doi={10.1109/ISSCC.2014.6757323},
    ISSN={0193-6530},
    month=Feb
}

@article{in_memory,
  author    = {Onur Mutlu and
               Saugata Ghose and
               Juan G{\'{o}}mez{-}Luna and
               Rachata Ausavarungnirun},
  title     = {Processing Data Where It Makes Sense: Enabling In-Memory Computation},
  journal   = {CoRR},
  volume    = {abs/1903.03988},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.03988},
  archivePrefix = {arXiv},
  eprint    = {1903.03988},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-03988},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{train_8b,
	title = {Sigmoid {Numbers} for {Julia}.},
	url = {https://github.com/interplanetary-robot/SigmoidNumbers},
	author = {Isaac Yonemoto},
	publisher = {Interplanetary Robot and Electrical Brain Company},
	month = jul,
	year = {2018}
}

@article{posit4,
    title={Posit Arithmetic},
    author={J. L. Gustafson},
    year={2017},
    month={Oct},
    journal={Mathmetica Notebook}
}

@article{kulisch,
    author = {Kulisch, U. and Miranker, W.},
    title = {The Arithmetic of the Digital Computer: A New Approach},
    journal = {SIAM Review},
    volume = {28},
    number = {1},
    pages = {1-40},
    year = {1986},
    doi = {10.1137/1028001},

    URL = {
            https://doi.org/10.1137/1028001

    },
    eprint = {
            https://doi.org/10.1137/1028001

    }

}

@INPROCEEDINGS{univ_gen,
    author={M. K. {Jaiswal} and H. K. -. {So}},
    booktitle={2018 Design, Automation Test in Europe Conference Exhibition (DATE)},
    title={Universal number posit arithmetic generator on FPGA},
    year={2018},
    volume={},
    number={},
    pages={1159-1162},
    keywords={field programmable gate arrays;floating point arithmetic;mantissa field size;run-time variation;exponent size;ES bits;exponent-bit;run-time varying length;exponent component;posit number system format;universal number posit arithmetic generator;basic posit arithmetic architectures;posit conversion;generic hardware generator;posit arithmetic algorithmic development;adequate hardware arithmetic architectures;recent development;hardware design challenge;Hardware;Computer architecture;Data mining;Heuristic algorithms;Generators;Standards;Field programmable gate arrays;Unum;Posit;FPGA;Digital Arithmetic;Adder;Sub-tractor;Multiplier},
    doi={10.23919/DATE.2018.8342187},
    ISSN={1558-1101},
    month=March
}

@INPROCEEDINGS{archi_gen,
    author={M. K. {Jaiswal} and H. K. -. {So}},
    booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)},
    title={Architecture Generator for Type-3 Unum Posit Adder/Subtractor},
    year={2018},
    volume={},
    number={},
    pages={1-5},
    keywords={adders;floating point arithmetic;hardware description languages;public domain software;architecture generator;type-3 unum posit adder/subtractor;hardware architecture aspect;universal number system;algorithmic flow;posit addition/subtraction arithmetic;floating point;posit format;regime-bits;run-time varying length;exponent-bits;ES bits;mantissa precision;run-time variation;hardware design challenge;Verilog HDL generator;Hardware Description Language;posit adder/subtractor arithmetic;Hardware design languages;Hardware;Adders;Dynamic range;Standards;Generators;Open source software;Unum;Posit;FPGA;Multi-Precision;Digital Arithmetic;Adder;Subtractor},
    doi={10.1109/ISCAS.2018.8351142},
    ISSN={2379-447X},
    month=May,
}

@INPROCEEDINGS{PAU,
    author={R. {Chaurasiya} and J. {Gustafson} and R. {Shrestha} and J. {Neudorfer} and S. {Nambiar} and K. {Niyogi} and F. {Merchant} and R. {Leupers}},
    booktitle={2018 IEEE 36th International Conference on Computer Design (ICCD)},
    title={Parameterized Posit Arithmetic Hardware Generator},
    year={2018},
    pages={334-341},
    keywords={adders;application specific integrated circuits;field programmable gate arrays;floating point arithmetic;integrated circuit design;low-power electronics;multiplying circuits;ASIC;application specific integrated circuit;FPGA;field programmable gate array;IEEE 754-2008 technical standard compliant FPU;parameterized Posit Arithmetic hardware generator;n-bit IEEE 754-2008 compliant adders;m-bit PAU adder;bit-width pre-synthesis;PAU multipliers;parameterized PAU generator;simpler hardware design;Posit Arithmetic Units;hardware implementation;Adders;Hardware;Generators;Standards;Computer architecture;Field programmable gate arrays;Open area test sites;computer arithmetic;floating point arithmetic;numerical error;posit arithmetic},
    doi={10.1109/ICCD.2018.00057},
    ISSN={2576-6996},
    month=Oct,
}

@inproceedings{Chen_MM,
 author = {Chen, Jianyu and Al-Ars, Zaid and Hofstee, H. Peter},
 title = {A Matrix-multiply Unit for Posits in Reconfigurable Logic Leveraging (Open)CAPI},
 booktitle = {Proceedings of the Conference for Next Generation Arithmetic},
 series = {CoNGA '18},
 year = {2018},
 isbn = {978-1-4503-6414-0},
 location = {Singapore, Singapore},
 pages = {1:1--1:5},
 articleno = {1},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3190339.3190340},
 doi = {10.1145/3190339.3190340},
 acmid = {3190340},
 publisher = {ACM},
 keywords = {dot-product, matrix-multiplier, posit number},
}

@article{van_dam_enabling_2018,
	title = {Enabling {High} {Performance} {Posit} {Arithmetic} {Applications} {Using} {Hardware} {Acceleration}},
	url = {https://repository.tudelft.nl/islandora/object/uuid%3A943f302f-7667-4d88-b225-3cd0cd7cf37c},
	abstract = {The demand for higher precision arithmetic is increasing due to the rapid development of new computing paradigms. The novel posit number representation system, as introduced by John L. Gustafson, claims to be able to provide more accurate answers to mathematical problems with equal or less number of bits compared to the well-established IEEE 754 floating point standard. In this work, the performance of the posit number format in terms of decimal accuracy is analyzed and compared to alternative number representations. A framework for performing high-precision posit arithmetic in reconfigurable logic is presented. The supported arithmetic operations can be performed without rounding off intermediate results, minimizing the loss of decimal accuracy. The proposed posit arithmetic units achieve approximately 250 MPOPS for addition, 160 MPOPS for multiplication and 180 MPOPS for accumulation operations. A hardware accelerator for performing Level 1 BLAS operations on (sparse) posit column vectors is presented. For the calculation of the vector dot product for an input vector length of 10{\textasciicircum}6 elements, a speedup of approximately 15000x compared to software is achieved. The decimal accuracy is improved by one decimal of accuracy on average compared to posit emulation in software, and two additional decimals of accuracy are achieved compared to calculation using the IEEE 754 floating point format. A study of the application of posit arithmetic in the field of bioinformatics is performed. The effect on decimal accuracy of the pair-HMM forward algorithm by replacing traditional floating point arithmetic with posit arithmetic is analyzed. It is shown that the maximum achievable decimal accuracy using posit arithmetic is higher compared to the IEEE floating point format for the same number of required bits. The design of a hardware accelerator for the pair-HMM forward algorithm using posit arithmetic is proposed for two different interfaces: a streaming-based accelerator and an accelerator interfacing with Apache Arrow columnar data, both connected by the CAPI (SNAP) platform. Overall, the posit number format beats the IEEE floating point number format in terms of decimal accuracy, ranging from an improvement of 0.5 to 1 additional decimal of accuracy for the performed test cases. A throughput of 1.6 and 1 giga cell updates per second is measured for both accelerator implementations, respectively.\&lt;br/\&gt;},
	language = {en},
	author = {van Dam, Laurens},
	year = {2018},
}

@inproceedings{Carmichael_2019,
   title={Deep Positron: A Deep Neural Network Using the Posit Number System},
   booktitle={2019 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
   author={Carmichael, Zachariah and Langroudi, Hamed F. and Khazanov, Char and Lillie, Jeffrey and Gustafson, John L. and Kudithipudi, Dhireesha},
   year={2019},
   month=Mar,
   publisher={IEEE},
   url={http://dx.doi.org/10.23919/date.2019.8715262},
   doi={10.23919/date.2019.8715262},
}


@misc{fletcher_2019,
    title={Fletcher : A framework to integrate FPGA accelerators with Apache Arrow},
    url={https://github.com/abs-tudelft/fletcher},
    journal={GitHub},
    year={2019}
}

@article{adm_pcie_9v3,
	title = {{ADM}-{PCIE}-9V3 {Support} \& {Development} {Kit} {Release}: 1.1.0 {V}1.1},
	language = {en},
	year = {2018},
	pages = {10},
}

@misc{bus_capi,
	author = {Morgan, Timothy Prickett},
	title = {Opening {Up} {The} {Server} {Bus} {For} {Coherent} {Acceleration}},
	url = {https://www.nextplatform.com/2016/10/17/opening-server-bus-coherent-acceleration/},
	abstract = {When IBM started to use the word "open" in conjunction with its Power architecture more than three years with the formation of the OpenPower Foundation},
	language = {en-US},

	journal = {The Next Platform},
	month = oct,
	year = {2016},
}

@misc{bus_capi2,
	author = {Morgan, Timothy Prickett},
	title = {Big {Blue} {Aims} {For} {The} {Sky} {With} {Power}9},
	url = {https://www.nextplatform.com/2016/08/24/big-blue-aims-sky-power9/},
	abstract = {Intel has the kind of control in the datacenter that only one vendor in the history of data processing has ever enjoyed. That other company is, of course,},
	language = {en-US},

	journal = {The Next Platform},
	month = aug,
	year = {2016},
}

@misc{nvlink2,
	title = {Nvidia's {NVLink} 2.0 will first appear in {Power}9 servers next year},
	url = {https://www.pcworld.com/article/3110615/nvidias-nvlink-20-will-first-appear-in-power9-servers-next-year.html},
	abstract = {Graphics processors with Nvidia's NVLink throughput technology have just started coming out, but a successor to the interconnect is already on its way.},
	language = {en},

	journal = {PCWorld},
	month = aug,
	year = {2016},
}

@misc{wiki_capi,
	author = {Wikipedia},
	title = {Coherent {Accelerator} {Processor} {Interface}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Coherent\_Accelerator\_Processor\_Interface\&oldid=912091796},
	abstract = {Coherent Accelerator Processor Interface (CAPI), is a high-speed processor expansion bus standard, initially designed to be layered on top of PCI Express, for directly connecting CPUs to external accelerators like GPUs, ASICs, FPGAs or fast storage. It offers low latency, high speed, direct memory access connectivity between devices of different instruction set architectures.
More details and documentation on CAPI can be found on the OpenCAPI Consortium website and IBM Portal for OpenPOWER.},
	language = {en},

	journal = {Wikipedia},
	month = aug,
	year = {2019},
	note = {Page Version ID: 912091796},
}

@misc{snap_git,
	author = {IBM},
	title = {{CAPI} {SNAP} {Framework} {Hardware} and {Software}. {Contribute} to open-power/snap development by creating an account on {GitHub}},
	url = {https://github.com/open-power/snap},
	publisher = {OpenPOWER},
	month =  apr,
	year = {2019},
	note = {original-date: 2016-09-20T20:10:34Z},
	keywords = {create action}
}

@misc{mn,
	title = {{MareNostrum}},
	url = {https://www.bsc.es/marenostrum/marenostrum},
	language = {en},

	journal = {BSC-CNS},
	keywords = {mn},
}

@misc{p9,
	title = {{IBM} {Power} {System} {AC}922 - {Overview} - {United} {States}},
	url = {https://www.ibm.com/us-en/marketplace/power-systems-ac922},

	keywords = {p9},
}

@misc{vandam_blas_fletcher,
	title = {Posit {Arithmetic} {Accelerator} interfacing with {Apache} {Arrow} \& {CAPI} {SNAP}: lvandam/posit\_blas\_hdl},
	copyright = {Apache-2.0},
	shorttitle = {Posit {Arithmetic} {Accelerator} interfacing with {Apache} {Arrow} \& {CAPI} {SNAP}},
	url = {https://github.com/lvandam/posit_blas_hdl},

	author = {Dam, Laurens van},
	month = jul,
	year = {2019},
	note = {original-date: 2018-07-03T13:18:16Z}
}

@misc{vandam_pairhmm_fletcher,
	title = {Pair-{HMM} {Accelerator} with {Posit} {Arithmetic} using {CAPI} {SNAP} \& {Apache} {Arrow}: lvandam/pairhmm\_posit\_hdl\_arrow},
	copyright = {Apache-2.0},
	shorttitle = {Pair-{HMM} {Accelerator} with {Posit} {Arithmetic} using {CAPI} {SNAP} \& {Apache} {Arrow}},
	url = {https://github.com/lvandam/pairhmm_posit_hdl_arrow},

	author = {Dam, Laurens van},
	month = jul,
	year = {2019},
	note = {original-date: 2018-05-31T10:02:17Z}
}

@misc{deepfloat_git,
	title = {An exploration of log domain "alternative floating point" for hardware {ML}/{AI} accelerators.: facebookresearch/deepfloat},
	copyright = {View license},
	shorttitle = {An exploration of log domain "alternative floating point" for hardware {ML}/{AI} accelerators.},
	url = {https://github.com/facebookresearch/deepfloat},

	publisher = {Facebook Research},
	month = aug,
	year = {2019},
	note = {original-date: 2018-09-27T21:04:49Z}
}

@misc{hopkins2019stochastic,
    title={Stochastic rounding and reduced-precision fixed-point arithmetic for solving neural ODEs},
    author={Michael Hopkins and Mantas Mikaitis and Dave R. Lester and Steve Furber},
    year={2019},
    eprint={1904.11263},
    archivePrefix={arXiv},
    primaryClass={cs.DS}
}

@misc{top500,
	title = {Home {\textbar} {TOP}500 {Supercomputer} {Sites}},
	url = {https://www.top500.org/},

}

@misc{mnist,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},

	keywords = {mnist, lacun},
}

@article{pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@misc{zynq1,
	title = {{PYNQ} - {Python} productivity for {Zynq}},
	author = {pynq},
	year = {2019},
	url = {http://www.pynq.io/},
	abstract = {PYNQ is an open-source project from Xilinx that makes it easy to design embedded systems with Xilinx Zynq All Programmab},

	journal = {PYNQ - Python productivity for Zynq},
}

@misc{zynq2,
	author = {Xilinx},
	year = {2021},
	title = {Zynq-7000 {SoC}},
	url = {https://www.xilinx.com/products/silicon-devices/soc/zynq-7000.html},
	abstract = {Zynq-7000 SoC devices integrate the software programmability of an ARM-based processor with the hardware programmability of an FPGA, enabling key analytics and hardware acceleration while integrating CPU, DSP, ASSP, and mixed signal functionality on a single device.},
	language = {en},

	journal = {Xilinx},
}

@misc{virtex2,
	author = {Design-reuse.com},
	year = {2003},
	title = {Xilinx {Announces} {Virtex}-{II} {Pro} {FPGA} {Development} {Board}},
	url = {https://www.design-reuse.com/news/4889/xilinx-virtex-ii-pro-fpga-development-board.html},
	abstract = {Xilinx Announces Virtex-II Pro FPGA Development Board},
	language = {en},

	journal = {Design And Reuse},
}


@misc{axi,
	title = {{AMBA} Documentation},
	url = {https://developer.arm.com/architectures/system-architectures/amba/documentation},
	abstract = {The documentation for AMBA.},
	language = {en},
	journal = {ARM Developer},
	author = {{Arm Ltd}},
	year = {2023},
	keywords = {axi, psec},
}

@article{pcie_throughput,
	title = {Understanding {Performance} of {PCI} {Express} {Systems}},
	language = {en},
	author = {Lawley, Jason},
	year = {2014},
	pages = {16},
}

@article{interstellar,
        title = {Interstellar: {Using} {Halide}'s {Scheduling} {Language} to {Analyze} {DNN} {Accelerators}},
        shorttitle = {Interstellar},
        url = {http://arxiv.org/abs/1809.04070},
        doi = {10.1145/3373376.3378514},
        abstract = {We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware
dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization. How resources are allocated, especially in the memory system, has a large impact on energy and performance. By optimizing hardware resource allocation while keeping throughput constant, we achieve up to 4.2X energy improvement for Convolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long Short-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively.},
        language = {en},

        journal = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
        author = {Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff Ou and Pu, Jing and Nayak, Ankita and Bell, Steven Emberton and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and Kozyrakis, Christos and Horowitz, Mark},
        month = mar,
        year = {2020},
        note = {arXiv: 1809.04070},
        keywords = {C.1.4, C.3, C.4, Computer Science - Distributed, Parallel, and Cluster Computing},
        pages = {369--383},
        annote = {Comment: Published as a conference paper at ASPLOS 2020},
}
@inproceedings{flopoco1,
  author = {de Dinechin, Florent},
  title = {Reflections on 10 years of {FloPoCo}},
  booktitle = {26th IEEE Symposium of Computer Arithmetic (ARITH-26)},
  year = {2019},
  month = jun,
  pdf = {https://hal.inria.fr/hal-02161527/document},
  nopublisher = {IEEE}
}
@article{flopoco2,
  author = {de Dinechin, Florent and Pasca, Bogdan},
  title = {Designing Custom Arithmetic Data Paths with {FloPoCo}},
  journal = {{IEEE} Design \& Test of Computers},
  volume = 28,
  number = 4,
  pages = {18-27},
  year = 2011,
  month = jul,
  nourl = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5753874},
  pdf = {http://perso.citi-lab.fr/fdedinec/recherche/publis/2011-DaT-FloPoCo.pdf}
}

@inproceedings{klower_posits_2019,
	address = {New York, NY, USA},
	series = {{CoNGA}'19},
	title = {Posits as an alternative to floats for weather and climate models},
	isbn = {978-1-4503-7139-1},
	url = {https://dl.acm.org/doi/10.1145/3316279.3316281},
	doi = {10.1145/3316279.3316281},
	abstract = {Posit numbers, a recently proposed alternative to floating-point numbers, claim to have smaller arithmetic rounding errors in many applications. By studying weather and climate models of low and medium complexity (the Lorenz system and a shallow water model) we present benefits of posits compared to floats at 16 bit. As a standardised posit processor does not exist yet, we emulate posit arithmetic on a conventional CPU. Using a shallow water model, forecasts based on 16-bit posits with 1 or 2 exponent bits are clearly more accurate than half precision floats. We therefore propose 16 bit with 2 exponent bits as a standard posit format, as its wide dynamic range of 32 orders of magnitude provides a great potential for many weather and climate models. Although the focus is on geophysical fluid simulations, the results are also meaningful and promising for reduced precision posit arithmetic in the wider field of computational fluid dynamics.},
	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic} 2019},
	publisher = {Association for Computing Machinery},
	author = {Klöwer, Milan and Düben, Peter D. and Palmer, Tim N.},
	month = mar,
	year = {2019},
	keywords = {climate projections, Computational fluid dynamics, computer arithmetic, floating point, posits, reduced precision, weather forecast},
	pages = {1--8},
}

@article{bailey_nas_1991,
	title = {{THE} {NAS} {PARALLEL} {BENCHMARKS}},
	year = {1991},
	abstract = {A new set of benchmarks has been developed for the performance evaluation of highly parallel supercomputers. These benchmarks consist of ﬁve parallel kernels and three simulated application benchmarks. Together they mimic the computation and data movement characteristics of large scale computational ﬂuid dynamics (CFD) applications.},
	language = {en},
	author = {Bailey, D and Barszcz, E and Barton, J and Browning, D and Carter, R and Dagum, L and Fineberg, S and Frederickson, P and Lasinski, T and Schreiber, R and Simon, H and Venkatakrishnan, V and Weeratunga, S},
}
@article{bailey_nas_1995,
	title = {{THE} {NAS} {PARALLEL} {BENCHMARKS} 2.0},
	year = {1995},
	language = {en},
	author = {Bailey, D},
}

@inproceedings{chien_posit_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Posit {NPB}: {Assessing} the {Precision} {Improvement} in {HPC} {Scientific} {Applications}},
	isbn = {978-3-030-43229-4},
	shorttitle = {Posit {NPB}},
	doi = {10.1007/978-3-030-43229-4_26},
	abstract = {Floating-point operations can significantly impact the accuracy and performance of scientific applications on large-scale parallel systems. Recently, an emerging floating-point format called Posit has attracted attention as an alternative to the standard IEEE floating-point formats because it could enable higher precision than IEEE formats using the same number of bits. In this work, we first explored the feasibility of Posit encoding in representative HPC applications by providing a 32-bit Posit NAS Parallel Benchmark (NPB) suite. Then, we evaluate the accuracy improvement in different HPC kernels compared to the IEEE 754 format. Our results indicate that using Posit encoding achieves optimized precision, ranging from 0.6 to 1.4 decimal digit, for all tested kernels and proxy-applications. Also, we quantified the overhead of the current software implementation of Posit encoding as 4\$\${\textbackslash}times \$\$–19\$\${\textbackslash}times \$\$ that of IEEE 754 hardware implementation. Our study highlights the potential of hardware implementations of Posit to benefit a broad range of HPC applications.},
	language = {en},
	booktitle = {Parallel {Processing} and {Applied} {Mathematics}},
	publisher = {Springer International Publishing},
	author = {Chien, Steven W. D. and Peng, Ivy B. and Markidis, Stefano},
	editor = {Wyrzykowski, Roman and Deelman, Ewa and Dongarra, Jack and Karczewski, Konrad},
	year = {2020},
	keywords = {Floating point precision, HPC, NPB, Posit},
	pages = {301--310},
}

@INPROCEEDINGS{horo,  author={M. {Horowitz}},  booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},   title={1.1 Computing's energy problem (and what we can do about it)},   year={2014},  volume={},  number={},  pages={10-14},}

@misc{tinkering,
Author = {Naigang Wang and Jungwook Choi and Daniel Brand and Chia-Yu Chen and Kailash Gopalakrishnan},
Title = {Training Deep Neural Networks with 8-bit Floating Point Numbers},
Year = {2018},
Eprint = {arXiv:1812.08011},
}


@inproceedings{de_dinechin_fpga-specific_2008,
	address = {Taipei, Taiwan},
	title = {An {FPGA}-specific approach to floating-point accumulation and sum-of-products},
	isbn = {978-1-4244-2795-6 978-1-4244-3783-2},
	url = {http://ieeexplore.ieee.org/document/4762363/},
	doi = {10.1109/FPT.2008.4762363},
	abstract = {This article studies two common situations where the ﬂexibility of FPGAs allows one to design application-speciﬁc ﬂoating-point operators which are more efﬁcient and more accurate than those offered by processors and GPUs. First, for applications involving the addition of a large number of ﬂoating-point values, an ad-hoc accumulator is proposed. By tailoring its parameters to the numerical requirements of the application, it can be made arbitrarily accurate, at an area cost comparable for most applications to that of a standard ﬂoating-point adder, and at a higher frequency. The second example is the sum-of-product operation, which is the building block of matrix computations. A novel architecture is proposed that feeds the previous accumulator out of a ﬂoating-point multiplier without its rounding logic, again improving both area and accuracy. These architectures are implemented within the FloPoCo generator, freely available under the GPL.},
	language = {en},

	booktitle = {2008 {International} {Conference} on {Field}-{Programmable} {Technology}},
	publisher = {IEEE},
	author = {de Dinechin, Florent and Pasca, Bogdan and Cret, Octavian and Tudoran, Radu},
	month = dec,
	year = {2008},
	pages = {33--40},
}

@inproceedings{istoan_automating_2017,
	address = {Lausanne, Switzerland},
	title = {Automating the pipeline of arithmetic datapaths},
	url = {https://hal.inria.fr/hal-01373937},
	abstract = {This article presents the new framework for semi-automatic circuit pipelining that will be used in future releases of the FloPoCo generator. From a single description of an operator or datapath, optimized implementations are obtained automatically for a wide range of FPGA targets and a wide range of frequency/latency trade-offs. Compared to previous versions of FloPoCo, the level of abstraction has been raised, enabling easier development, shorter generator code, and better pipeline optimization. The proposed approach is also more flexible than fully automatic pipelining approaches based on retiming: In the proposed technique, the incremental construction of the pipeline along with the circuit graph enables architectural design decisions that depend on the pipeline. These allow pipeline-dependent changes to the circuit graph for finer optimization.},

	booktitle = {Design, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE} 2017)},
	author = {Istoan, Matei and de Dinechin, Florent},
	month = mar,
	year = {2017},
}


@article{carmichael_deep_2019,
	title = {Deep {Positron}: {A} {Deep} {Neural} {Network} {Using} the {Posit} {Number} {System}},
	shorttitle = {Deep {Positron}},
	url = {http://arxiv.org/abs/1812.01762},
	abstract = {The recent surge of interest in Deep Neural Networks (DNNs) has led to increasingly complex networks that tax computational and memory resources. Many DNNs presently use 16-bit or 32-bit ﬂoating point operations. Signiﬁcant performance and power gains can be obtained when DNN accelerators support low-precision numerical formats. Despite considerable research, there is still a knowledge gap on how low-precision operations can be realized for both DNN training and inference. In this work, we propose a DNN architecture, Deep Positron, with posit numerical format operating successfully at ≤8 bits for inference. We propose a precision-adaptable FPGA soft core for exact multiply-and-accumulate for uniform comparison across three numerical formats, ﬁxed, ﬂoating-point and posit. Preliminary results demonstrate that 8-bit posit has better accuracy than 8-bit ﬁxed or ﬂoating-point for three different low-dimensional datasets. Moreover, the accuracy is comparable to 32-bit ﬂoatingpoint on a Xilinx Virtex-7 FPGA device. The trade-offs between DNN performance and hardware resources, i.e. latency, power, and resource utilization, show that posit outperforms in accuracy and latency at 8-bit and below.},
	language = {en},

	journal = {arXiv:1812.01762 [cs]},
	author = {Carmichael, Zachariah and Langroudi, Hamed F. and Khazanov, Char and Lillie, Jeffrey and Gustafson, John L. and Kudithipudi, Dhireesha},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.01762},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 6 pages, Design, Automation and Test in Europe 2019},
}

@inproceedings{chen_matrix-multiply_2018,
	address = {Singapore, Singapore},
	title = {A matrix-multiply unit for posits in reconfigurable logic leveraging (open){CAPI}},
	isbn = {978-1-4503-6414-0},
	url = {http://dl.acm.org/citation.cfm?doid=3190339.3190340},
	doi = {10.1145/3190339.3190340},
	abstract = {In this paper, we present the design in reconfigurable logic of a matrix multiplier for matrices of 32-bit posit numbers with es=2 [1]. Vector dot products are computed without intermediate rounding as suggested by the proposed posit standard to maximally retain precision. An initial implementation targets the CAPI 1.0 interface on the POWER8 processor and achieves about 10Gpops (Giga posit operations per second). Follow-on implementations targeting CAPI 2.0 and OpenCAPI 3.0 on POWER9 are expected to achieve up to 64Gpops. Our design is available under a permissive open source license at https://github.com/ChenJianyunp/Unum\_matrix\_multiplier. We hope the current work, which works on CAPI 1.0, along with future community contributions, will help enable a more extensive exploration of this proposed new format.},
	language = {en},

	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic} on - {CoNGA} '18},
	publisher = {ACM Press},
	author = {Chen, Jianyu and Al-Ars, Zaid and Hofstee, H. Peter},
	year = {2018},
	pages = {1--5},
}

@article{noauthor_adm_pcie_9v3_2018,
	author={AlphaData},
	title = {{ADM}-{PCIE}-{9V3} {Support} \& {Development} {Kit} {Release}: 1.1.0 {V1}.1},
	language = {en},
	year = {2018},
	pages = {10},
}

@misc{noauthor_virtex_nodate,
	author={Xilinx},
	year = {2016},
	title = {Virtex {UltraScale}+},
	url = {https://www.xilinx.com/products/silicon-devices/fpga/virtex-ultrascale-plus.html},
	abstract = {Virtex® UltraScale+™ devices provide the highest performance and integration capabilities in a FinFET node, including both the highest serial I/O and signal processing bandwidth, as well as the highest on-chip memory density.},
	language = {en},

	journal = {Xilinx},
}

@misc{noauthor_pci_2020,
	title = {{PCI} {Express}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=PCI\_Express\&oldid=982964877},
	abstract = {PCI Express (Peripheral Component Interconnect Express), officially abbreviated as PCIe or PCI-e, is a high-speed serial computer expansion bus standard, designed to replace the older PCI, PCI-X and AGP bus standards. It is the common motherboard interface for personal computers' graphics cards, hard drives, SSDs, Wi-Fi and Ethernet hardware connections.  PCIe has numerous improvements over the older standards, including higher maximum system bus throughput, lower I/O pin count and smaller physical footprint, better performance scaling for bus devices, a more detailed error detection and reporting mechanism (Advanced Error Reporting, AER), and native hot-swap functionality. More recent revisions of the PCIe standard provide hardware support for I/O virtualization.
Defined by its number of lanes, the PCI Express electrical interface is also used in a variety of other standards, most notably the laptop expansion card interface ExpressCard and computer storage interfaces SATA Express, U.2 (SFF-8639) and M.2.
Format specifications are maintained and developed by the PCI-SIG (PCI Special Interest Group), a group of more than 900 companies that also maintain the conventional PCI specifications.},
	language = {en},

	journal = {Wikipedia},
	month = oct,
	author = {Wikipedia},
	year = {2020},
	note = {Page Version ID: 982964877},
}


@misc{mercado_mightymercadopysigmoid_2020,
	copyright = {MIT License, MIT License},
	url = {https://github.com/mightymercado/PySigmoid},
	title = {A Python Implementation of Posits and Quires (Drop-in replacement for IEEE Floats)},
	author = {Mercado, Ken},
	month = jun,
	year = {2020},
}

@article{johnson_rethinking_2018,
	title = {Rethinking floating point for deep learning},
	url = {http://arxiv.org/abs/1811.01721},
	journal = {arXiv:1811.01721 [cs]},
	author = {Johnson, Jeff},
	year = {2018},
	note = {arXiv: 1811.01721}
}


@article{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classiﬁcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	language = {en},

	journal = {arXiv:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:/home/binaryman/Zotero/storage/NS4CN5TZ/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf}
}


@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},

	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/binaryman/Zotero/storage/FKB5MVJI/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}


@article{beliakov_parallel_2013,
	title = {A {Parallel} {Algorithm} for {Calculation} of {Large} {Determinants} with {High} {Accuracy} for {GPUs} and {MPI} clusters},
	url = {http://arxiv.org/abs/1308.1536},
	abstract = {We present a parallel algorithm for calculating very large determinants with arbitrary precision on computer clusters. This algorithm minimises data movements between the nodes and computes not only the determinant but also all minors corresponding to a particular row or column at a little extra cost, and also the determinants and minors of all submatrices in the top left corner at no extra cost. We implemented the algorithm in arbitrary precision arithmetic, suitable for very ill conditioned matrices, and empirically estimated the loss of precision. The algorithm was applied to studies of Riemann's zeta function.},

	journal = {arXiv:1308.1536 [cs, math]},
	author = {Beliakov, Gleb and Matiyasevich, Yuri},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.1536},
	keywords = {65F40, 68W10, 11M26, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, D.1.3, G.1.0, G.1.3, Mathematics - Number Theory, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/binaryman/Zotero/storage/XNY56YCR/Beliakov and Matiyasevich - 2013 - A Parallel Algorithm for Calculation of Large Dete.pdf:application/pdf;arXiv.org Snapshot:/home/binaryman/Zotero/storage/XDHYU8E9/1308.html:text/html}
}


@article{zhang_qed_1996,
	title = {QED Corrections of $O(m{c}^{2}{\ensuremath{\alpha}}^{7}\mathrm{ln}\ensuremath{\alpha})$ to the Fine Structure Splittings of Helium and He-like Ions},
	volume = {77},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.77.1715},
	doi = {10.1103/PhysRevLett.77.1715},
	abstract = {A reformulation of the external potential Bethe-Salpeter formalism is developed for two-electron atoms. QED and relativistic corrections to energy levels of order α7mc2lnα are derived and expressed in terms of expectation values of nonrelativistic operators. Corrections of order α7mc2 from exchange diagrams are also found. The total contributions of order α7mc2lnα to the 1s2p3PJ fine structure intervals of helium are Δν01=82.6 kHz and Δν12=−10.0 kHz. Results are given for He-like ions up to Z=12 and compared with experiment.},
	number = {9},

	journal = {Physical Review Letters},
	author = {Zhang, Tao and Yan, Zong-Chao and Drake, G. W. F.},
	month = aug,
	year = {1996},
	note = {Publisher: American Physical Society},
	pages = {1715--1718},
	file = {APS Snapshot:/home/binaryman/Zotero/storage/XCFUJWQA/PhysRevLett.77.html:text/html}
}

@article{ellis_one-loop_2009,
	title = {One-loop amplitudes for {W}+3 jet production in hadron collisions},
	volume = {2009},
	issn = {1029-8479},
	url = {http://arxiv.org/abs/0810.2762},
	doi = {10.1088/1126-6708/2009/01/012},
	abstract = {We employ the recently developed method of generalized \$D\$-dimensional unitarity to compute one-loop virtual corrections to all scattering amplitudes relevant for the production of a \$W\$ boson in association with three jets in hadronic collisions, treating all quarks as massless.},
	number = {01},

	journal = {Journal of High Energy Physics},
	author = {Ellis, R. Keith and Giele, W. T. and Kunszt, Zoltan and Melnikov, Kirill and Zanderighi, Giulia},
	month = jan,
	year = {2009},
	note = {arXiv: 0810.2762},
	keywords = {High Energy Physics - Phenomenology},
	pages = {012--012},
	annote = {Comment: 26 pages, 5 figures, v2 to agree with published version},
	file = {arXiv Fulltext PDF:/home/binaryman/Zotero/storage/LP555RL4/Ellis et al. - 2009 - One-loop amplitudes for W+3 jet production in hadr.pdf:application/pdf;arXiv.org Snapshot:/home/binaryman/Zotero/storage/3NNYT53W/0810.html:text/html}
}

@article{golomb_run-length_2006,
	title = {Run-length encodings ({Corresp}.)},
	volume = {12},
	issn = {0018-9448},
	url = {https://doi.org/10.1109/TIT.1966.1053907},
	doi = {10.1109/TIT.1966.1053907},
	abstract = {First Page of the Article},
	number = {3},

	journal = {IEEE Transactions on Information Theory},
	author = {Golomb, S.},
	month = sep,
	year = {2006},
	pages = {399--401},
}


@inproceedings{uguen_evaluating_2019,
	address = {Barcelona, Spain},
	title = {Evaluating the {Hardware} {Cost} of the {Posit} {Number} {System}},
	isbn = {978-1-72814-884-7},
	url = {https://ieeexplore.ieee.org/document/8892116/},
	doi = {10.1109/FPL.2019.00026},
	abstract = {The posit number system is proposed as a replacement of IEEE ﬂoating-point numbers. It is a ﬂoating-point system that trades exponent bits for signiﬁcand bits, depending on the magnitude of the numbers. Thus, it provides more precision for numbers around 1, at the expense of lower precision for very large or very small numbers. Several works have demonstrated that this trade-off can improve the accuracy of applications. However, the variable-length exponent and signiﬁcand encoding impacts the hardware cost of posit arithmetic. The objective of the present work is to enable application-level evaluations of the posit system that include performance and resource consumption. To this purpose, this article introduces an open-source hardware implementation of the posit number system, in the form of a C++ templatized library compatible with Vivado HLS. This library currently implements addition, subtraction and multiplication for custom-size posits. In addition, the posit standard also mandates the presence of the “quire”, a large accumulator able to perform exact sums of products. The proposed library includes the ﬁrst open-source parameterized hardware quire.},
	language = {en},

	booktitle = {2019 29th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	publisher = {IEEE},
	author = {Uguen, Yohann and Forget, Luc and de Dinechin, Florent},
	month = sep,
	year = {2019},
	pages = {106--113},
	file = {Uguen et al. - 2019 - Evaluating the Hardware Cost of the Posit Number S.pdf:/home/binaryman/Zotero/storage/8G8DQ474/Uguen et al. - 2019 - Evaluating the Hardware Cost of the Posit Number S.pdf:application/pdf}
}

@inproceedings{forget_hardware_2019,
	title = {Hardware cost evaluation of the posit number system},
	url = {https://www.semanticscholar.org/paper/Hardware-cost-evaluation-of-the-posit-number-system-Forget-Uguen/b96f0c0fa3e5f4d51b46d8c0a560e2702b81f058},
	abstract = {The posit number system is proposed as a replacement of IEEE floats. It encodes floating-point values with tapered precision: numbers whose exponent is close to 0 have more precision than IEEE floats, while numbers with high-magnitude exponents have lower precision, because their encoding takes bits from the significand. In addition, the posit standard mandates the presence of the "quire", a Kulisch-like large accumulator able to perform exact sums of products. Several works have demonstrated that posit arithmetic can provide improved accuracy at the application level. However, the variable-length fields of posit encoding impacts the hardware cost of posit arithmetic. Existing comparisons of posit hardware versus float hardware are unconvincing, and the overhead of the exact accumulator has not been studied in detail so far. This work aims at filling this gap. To this purpose, it introduces an open-source tool to compare the respective costs of floats and posits on an application basis. A C++ templatized library compatible with Vivado HLS implements operators for custom size posits and their associated quire. These architectures are evaluated on recent FPGA hardware and compared to their IEEE-754 counterpart. The standard 32 bits posit adder is found to be twice as large as the corresponding floating-point adder. Posit multiplication requires about 7 times more LUTs and a few more DSPs for a latency which is 2x worst than the IEEE-754 32 bit multiplier. Furthermore , the cost of the posit 32 quire is shown to be the same as a 32 bits floating-point Kulisch accumulator.},
	author = {Forget, Luc and Uguen, Yohann and Dinechin, F. D.},
	month = jun,
	year = {2019},
}

@inproceedings{dedinechin:hal-01959581,
  TITLE = {{Posits: the good, the bad and the ugly}},
  AUTHOR = {de Dinechin, Florent and Forget, Luc and Muller, Jean-Michel and Uguen, Yohann},
  URL = {https://hal.inria.fr/hal-01959581},
  BOOKTITLE = {{CoNGA 2019 - Conference on Next-Generation Arithmetic}},
  ADDRESS = {Singapore, Singapore},
  PUBLISHER = {{ACM Press}},
  PAGES = {1-10},
  YEAR = {2019},
  MONTH = Mar,
  DOI = {10.1145/3316279.3316285},
  KEYWORDS = {Floating-point ; Numerical analysis ; Posits},
  PDF = {https://hal.inria.fr/hal-01959581v4/file/a6-de_Dinechin.pdf},
  HAL_ID = {hal-01959581},
  HAL_VERSION = {v4},
}

@inproceedings{de_dinechin_posits_2019,
	address = {New York, NY, USA},
	series = {{CoNGA}'19},
	title = {Posits: the good, the bad and the ugly},
	isbn = {978-1-4503-7139-1},
	shorttitle = {Posits},
	url = {https://dl.acm.org/doi/10.1145/3316279.3316285},
	doi = {10.1145/3316279.3316285},
	abstract = {Many properties of the IEEE-754 floating-point number system are taken for granted in modern computers and are deeply embedded in compilers and low-level software routines such as elementary functions or BLAS. This article reviews such properties on the posit number system. Some are still true. Some are no longer true, but sensible work-arounds are possible, and even represent exciting challenges for the community. Some represent a danger if posits are to replace floating point completely. This study helps framing where posits are better than floating-point, where they are worse, what is the cost of posit hardware, and what tools are missing in the posit landscape. For general-purpose computing, using posits as a storage format could be a way to reap their benefits without losing those of classical floating-point.},
	booktitle = {Proceedings of the {Conference} for {Next} {Generation} {Arithmetic} 2019},
	publisher = {Association for Computing Machinery},
	author = {de Dinechin, Florent and Forget, Luc and Muller, Jean-Michel and Uguen, Yohann},
	month = mar,
	year = {2019},
	keywords = {floating-point, numerical analysis, Posits},
	pages = {1--10},
}

@inproceedings{buoncristiani_evaluating_2020,
	title = {Evaluating the {Numerical} {Stability} of {Posit} {Arithmetic}},
	doi = {10.1109/IPDPS47924.2020.00069},
	abstract = {The Posit number format has been proposed by John Gustafson as an alternative to the IEEE 754 standard floatingpoint format. Posits offer a unique form of tapered precision whereas IEEE floating-point numbers provide the same relative precision across most of their representational range. Posits are argued to have a variety of advantages including better numerical stability and simpler exception handling.The objective of this paper is to evaluate the numerical stability of Posits for solving linear systems where we evaluate Conjugate Gradient Method to demonstrate an iterative solver and Cholesky-Factorization to demonstrate a direct solver. We show that Posits do not consistently improve stability across a wide range of matrices, but we demonstrate that a simple rescaling of the underlying matrix improves convergence rates for Conjugate Gradient Method and reduces backward error for Cholesky Factorization. We also demonstrate that 16-bit Posit outperforms Float16 for mixed precision iterative refinement - especially when used in conjunction with a recently proposed matrix re-scaling strategy proposed by Nicholas Higham.},
	booktitle = {2020 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Buoncristiani, Nicholas and Shah, Sanjana and Donofrio, David and Shalf, John},
	month = may,
	year = {2020},
	note = {ISSN: 1530-2075},
	keywords = {Cholesky factorization, conjugate gradient method, conjugate gradient methods, exception handling, floating point arithmetic, floating-point, Gradient methods, IEEE floating-point numbers, iterative methods, Iterative methods, linear algebra, Linear algebra, Linear systems, matrix decomposition, mixed precision iterative refinement, numerical stability, Numerical stability, Open area test sites, Posit, Posit arithmetic, Posit number format, relative precision, standard floatingpoint format, Standards, tapered precision},
	pages = {612--621},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/C66L5ARL/9139786.html:text/html}
}

@book{kung_systolic_1978,
	series = {{CMU}-{CS}},
	title = {Systolic {Arrays} for ({VLSI})},
	url = {https://books.google.fr/books?id=pAKfHAAACAAJ},
	publisher = {Carnegie-Mellon University, Department of Computer Science},
	author = {Kung, H.T. and Leiserson, C.E. and SCIENCE, CARNEGIE-MELLON UNIV PITTSBURGH PA Dept of COMPUTER and Department, Carnegie-Mellon University Computer Science},
	year = {1978}
}

@article{aso_formal_1988,
	title = {Formal description of systolic algorithms and an analysis of the information flow},
	volume = {19},
	copyright = {Copyright © 1988 Wiley Periodicals, Inc., A Wiley Company},
	issn = {1520-684X},
	doi = {10.1002/scj.4690190602},
	abstract = {Due to the development of VSLI fabrication technologies systolic algorithms are now being implemented as practical hardware. As a result, it is becoming more important to know their design methodologies and the scope of their object problems. In this paper as a basis to know the scope of the object problems we formalize systolic algorithms as mathematical objects in a strict manner. In other words, it is formalized by a triple systolic array, a data location space, and a set of timing functions. Moreover, equivalence between systolic arrays is defined. Under this formulation we define a data dependency graph as a tool for analyzing the flow of information, and show that wiring structures can be classified into several types. We give a condition that it is reduced equivalently to a simple memory-type one. Finally, we consider the data flow processed in a systolic array and give not only an equation for data flow, but also the initial arrangement for a necessary encounter for interaction between data flows.},
	language = {en},
	number = {6},
	journal = {Systems and Computers in Japan},
	author = {Aso, Hirotome and Inagaki, Yasuyoshi},
	year = {1988},
	pages = {14--24},
}

@article{sun-yuan_kung_optimal_1987,
	title = {Optimal {Systolic} {Design} for the {Transitive} {Closure} and the {Shortest} {Path} {Problems}},
	volume = {C-36},
	issn = {1557-9956},
	doi = {10.1109/TC.1987.1676945},
	abstract = {Due to VLSI technological progress, algorithm- oriented array architectures, such as systolic arrays, appear to be very effective, feasible, and economic. This paper discusses how to design systolic arrays for the transitive closure and the shortest path problems. We shall focus on the Warshall algorithm for the transitive closure problem and the Floyd algorithm for the shortest path problem. These two algorithms share exactly the same structural formulation; therefore, they lead to the same systolic array design. In this paper, we first present a general method for mapping algorithms to systolic arrays. Using this methodology, two new systolic designs for the Warshall-Floyd algorithm will be derived. The first one is a spiral array, which is easy to derive and can be further simplified to a hexagonal array. The other is an orthogonal systolic array which is optimal in terms of pipelining rate, block pipelining rate, and the number of input/output connections.},
	number = {5},
	journal = {IEEE Transactions on Computers},
	author = {Sun-Yuan Kung and Sheng-Chun Lo and Lewis},
	month = may,
	year = {1987},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Algorithm mappings, optimal algorithms, shortest path problem, systolic arrays, transitive closure problems, VLSI algorithms, VLSI architectures},
	pages = {603--614},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/75WS8DPW/1676945.html:text/html}
}

@article{DinechinPasca2011-DaT,
  author = {de Dinechin, Florent and Pasca, Bogdan},
  title = {Designing Custom Arithmetic Data Paths with {FloPoCo}},
  journal = {{IEEE} Design \& Test of Computers},
  volume = 28,
  number = 4,
  pages = {18-27},
  year = 2011,
  month = jul,
  nourl = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5753874},
  pdf = {http://perso.citi-lab.fr/fdedinec/recherche/publis/2011-DaT-FloPoCo.pdf}

}

@article{genc_gemmini_2019,
	title = {Gemmini: {An} {Agile} {Systolic} {Array} {Generator} {Enabling} {Systematic} {Evaluations} of {Deep}-{Learning} {Architectures}},
	shorttitle = {Gemmini},
	url = {http://arxiv.org/abs/1911.09925},
	abstract = {Advances in deep learning and neural networks have resulted in rapid development of hardware accelerators that support them. A large majority of ASIC accelerators, however, target a single hardware design point to accelerate the main computational kernels of deep neural networks such as convolutions or matrix multiplication. On the other hand, the spectrum of use-cases for neural network accelerators, ranging from edge devices to cloud, presents a prime opportunity for agile hardware design and generator methodologies. We present Gemmini1 - an open source and agile systolic array generator enabling systematic evaluations of deep-learning architectures. Gemmini generates a custom ASIC accelerator for matrix multiplication based on a systolic array architecture, complete with additional functions for neural network inference. Gemmini runs with the RISC-V ISA, and is integrated with the Rocket Chip System-on-Chip generator ecosystem, including Rocket in-order cores and BOOM out-of-order cores. Through an elaborate design space exploration case study, this work demonstrates the selection processes of various parameters for the use-case of inference on edge devices. Selected design points achieve two to three orders of magnitude speedup in deep neural network inference compared to the baseline execution on a host processor. Gemmini-generated accelerators were used in the fabrication of test systems-on-chip in TSMC 16nm and Intel 22FFL process technologies.},
	language = {en},

	journal = {arXiv:1911.09925 [cs]},
	author = {Genc, Hasan and Haj-Ali, Ameer and Iyer, Vighnesh and Amid, Alon and Mao, Howard and Wright, John and Schmidt, Colin and Zhao, Jerry and Ou, Albert and Banister, Max and Shao, Yakun Sophia and Nikolic, Borivoje and Stoica, Ion and Asanovic, Krste},
	month = dec,
	year = {2019},
	note = {arXiv: 1911.09925},
	keywords = {Computer Science - Machine Learning, Computer Science - Hardware Architecture, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	file = {Genc et al. - 2019 - Gemmini An Agile Systolic Array Generator Enablin.pdf:/home/binaryman/Zotero/storage/4BU7QRGC/Genc et al. - 2019 - Gemmini An Agile Systolic Array Generator Enablin.pdf:application/pdf}
}

@article{kungand_systolic_1981,
	title = {A {Systolic} 2-1) {Convolution} {Chip}},
	abstract = {This paper describes a chip for performing the 2-D (two-dimensional) convolution in signal and image processing. The chip, based on a systolic design, consists of essentially only one type of simple cells, which are mesh-interconnected in a regular and modular way, and achieves high performance through extensive concurrent and pipelined use of these cells. Denoting by u the cycle time of the basic cell, the chip allows convolving a kxk window with an nxn image in O(n 2u/k) time, using a total of k3 basic cells. The total number of cells is optimal in the sense that the usual sequential algorithm takes O(n 2k2u) time. Furthermore, because of the modularity of the design, the number of cells used by the chip can be easily adjusted to achieve any desirable balance between I/O and computation speeds.},
	year = {1981},
	language = {en},
	author = {Kung'and, H T and Song, W},
	pages = {15},
}

@article{lee_parallel_1987,
	title = {Parallel 2-{D} {Convolution} on a {Mesh} {Connected} {Array} {Processor}},
	volume = {PAMI-9},
	issn = {0162-8828},
	url = {https://ieeexplore.ieee.org/document/4767947},
	doi = {10.1109/TPAMI.1987.4767947},
	abstract = {In this correspondence, a parallel 2-D convolution scheme is presented. The processing structure is a mesh connected array processor consisting of the same number of simple processing elements as the number of pixels in the image. For most windows considered, the number of computation steps required is the same as that of the coefficients of a convolution window. The proposed scheme can be easily extended to convolution windows of arbitrary size and shape. The basic idea of the proposed scheme is to apply the 1-D systolic concept to 2-D convolution on a mesh structure. The computation is carried out along a path called a convolution path in a systolic manner. The efficiency of the scheme is analyzed for windows of various shapes. The ideal convolution path is a Hamiltonian path ending at the center of the window, the length of which is equal to the number of window coefficients. The simple architecture and control strategy make the proposed scheme suitable for VLSI implementation.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lee, S.-Y. and Aggarwal, J. K.},
	month = jul,
	year = {1987},
	pages = {590--594},
	annote = {[TLDR] The basic idea of the proposed scheme is to apply the 1-D systolic concept to 2-D convolution on a mesh structure to make the scheme suitable for VLSI implementation.},
}

@article{shukla_kernel-independent_1988,
	title = {A kernel-independent, pipelined architecture for real-time 2-{D} convolution},
	url = {http://ieeexplore.ieee.org/document/5225/},
	doi = {10.1109/ISCA.1988.5225},
	abstract = {A flexible, fault-tolerant architecture to perform 2-D convolution of images for kernels of arbitrary shapes and sizes is proposed. The convolved image is output online as the input image is generated and received, providing optimal turnaround time. The regularity of the computation and the raster-scan input of pixels are used to pipeline computations as well as memory operations. Throughput of (kl/u) is achieved for a variety of kernel sizes with a k*l array of multiply-accumulate cells, where u is the time for a single fixed-point multiply-accumulate operation. It is shown that by queuing accesses at memory modules the limitation placed by access time on the throughput can be removed. Larger, as well as smaller kernels can be accommodated on a fixed-size array with a single pass of the image and a minor change in the computation flow. The host enjoys absence of any control overhead and only has to supply the pixels and collect the output pixels in simple raster-scan fashion for any kernel size.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	journal = {[1988] The 15th Annual International Symposium on Computer Architecture. Conference Proceedings},
	author = {Shukla, S.B. and Agrawal, D.P.},
	year = {1988},
	note = {Conference Name: [1988] The 15th Annual International Symposium on Computer Architecture. Conference Proceedings
ISBN: 9780818608612
Place: Honolulu, HI, USA
Publisher: IEEE Comput. Soc. Press},
	pages = {160--166},
	annote = {[TLDR] A flexible, fault-tolerant architecture to perform 2-D convolution of images for kernels of arbitrary shapes and sizes is proposed, where the convolved image is output online as the input image is generated and received, providing optimal turnaround time.},
}

@article{haule_high-speed_1989,
	title = {High-speed 2-{D} hardware convolution architecture based on {VLSI} systolic arrays},
	url = {https://www.semanticscholar.org/paper/High-speed-2-D-hardware-convolution-architecture-on-Haule-Malowany/e8303ec1191e091642f84329ffd5aa0d14049537},
	abstract = {The design of a special-purpose systolic image convolution processor for use in a robot vision system is described. It presents a high-speed two-dimensional hardware convolution architecture based on VSLI systolic arrays for image processing applications. An architecture for the parallel processing of the generalized two-dimensional convolution is summarized. A VLSI convolution chip was designed to accommodate various convolution window sizes. The number of coefficients being handled is directly proportional to the number of systolic computing elements (processors) used. In the present design three such processors are configured on one VLSI chip. Signed coefficients and unsigned data of eight bits are supported. All processing and interprocessor communications are performed bit-serially. The chip design incorporates error detection during convolution, and overflow avoidance techniques are possible for maximum system autonomy. The chip is estimated to operate at a maximum frequency of 16 MHz.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	journal = {Conference Proceeding IEEE Pacific Rim Conference on Communications, Computers and Signal Processing},
	author = {Haule, D. and Malowany, A.},
	year = {1989},
	annote = {[TLDR] The design of a special-purpose systolic image convolution processor for use in a robot vision system and an architecture for the parallel processing of the generalized two-dimensional convolution is summarized.},
}

@inproceedings{kung_two-level_1981,
	address = {Berlin, Heidelberg},
	title = {A {Two}-{Level} {Pipelined} {Systolic} {Array} for {Convolutions}},
	isbn = {978-3-642-68404-3 978-3-642-68402-9},
	url = {http://link.springer.com/10.1007/978-3-642-68402-9_28},
	doi = {10.1007/978-3-642-68402-9_28},
	abstract = {Pipelining computations over a large array of cells has been an important feature of systolic arrays. To achieve even higher degrees of concurrency, it is desirable to have cells of a systolic array themselves be pipelined as well. The resulting two-level pipelined systolic array would enjoy in principle a k-fold increase in its throughput, where k is the ratio of the time to perform the entire cell computation over that to perform just one of its pipeline stages. This paper describes such a two-level pipelined systolic array that is capable of performing convolutions of any dimension. The designs take full advantages of the pipelining assumed to be available at each cell.},
	language = {en},
	publisher = {Springer Berlin Heidelberg},
	author = {Kung, H. T. and Ruane, Lawrence M. and Yen, David W. L.},
	editor = {Kung, H. T. and Sproull, Bob and Steele, Guy},
	year = {1981},
	note = {Book Title: VLSI Systems and Computations},
	pages = {255--264},
	annote = {[TLDR] A two-level pipelined systolic array that is capable of performing convolutions of any dimension and the designs take full advantages of the pipelining assumed to be available at each cell are described.},
}

@article{kung_why_1982,
	title = {Why systolic architectures?},
	volume = {15},
	issn = {1558-0814},
	doi = {10.1109/MC.1982.1653825},
	number = {1},
	journal = {Computer},
	author = {Kung},
	month = jan,
	year = {1982},
	note = {Conference Name: Computer},
	keywords = {Application software, Assembly systems, Computer architecture, Computer interfaces, Concurrent computing, Costs, Data flow computing, Hardware, Image processing, Signal processing},
	pages = {37--46},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/HDYM6A5V/1653825.html:text/html;Submitted Version:/home/binaryman/Zotero/storage/YCW4TXMW/Kung - 1982 - Why systolic architectures.pdf:application/pdf}
}


@article{quinton_new_1986,
	title = {A new matrix multiplication systolic array},
	year = {1986},
	author = {Quinton, Patrice and Joinnault, Brigitte and Gachet, Pierrick},
	pages = {15},
	file = {Quinton et al. - A new matrix multiplication systolic array.pdf:/home/binaryman/Zotero/storage/H4PJ84JN/Quinton et al. - A new matrix multiplication systolic array.pdf:application/pdf}
}


@article{benaini_even_1989,
	title = {An even faster systolic array for matrix multiplication},
	volume = {12},
	issn = {0167-8191},
	url = {https://www.sciencedirect.com/science/article/pii/0167819189900574},
	doi = {10.1016/0167-8191(89)90057-4},
	abstract = {We propose a two-layered mesh array for matrix multiplication which is faster than all previously published arrays in the literature. We map Winograd's algorithm onto a systolic array composed of 0.5n2 multiply-and-add cells which can compute the product of two n × n matrices within 1.5n time-steps.},
	number = {2},
	journal = {Parallel Computing},
	author = {Benaini, Abdelhamid and Robert, Yves},
	month = nov,
	year = {1989},
	keywords = {Matrix product, systolic array, two-layered mesh array},
	pages = {249--254},
}

@article{preparata_area-time_1980,
	title = {Area-time optimal {VLSI} networks for multiplying matrices},
	volume = {11},
	issn = {0020-0190},
	url = {https://www.sciencedirect.com/science/article/pii/002001908090006X},
	doi = {10.1016/0020-0190(80)90006-X},
	number = {2},
	journal = {Information Processing Letters},
	author = {Preparata, Franco P. and Vuillemin, Jean E.},
	month = oct,
	year = {1980},
	keywords = {area-time complexity, matrix multiplication, optimal networks, pipeline computation, VLSI},
	pages = {77--80},
	file = {ScienceDirect Snapshot:/home/lledoux/Zotero/storage/9YM7SZ6Y/002001908090006X.html:text/html},
}

@article{kak_two-layered_1988,
	title = {A two-layered mesh array for matrix multiplication},
	volume = {6},
	issn = {0167-8191},
	url = {https://www.sciencedirect.com/science/article/pii/0167819188900786},
	doi = {10.1016/0167-8191(88)90078-6},
	abstract = {A two-layered mesh array for matrix multiplication is presented. It computers the matrix product faster than the standard array.},
	number = {3},
	journal = {Parallel Computing},
	author = {Kak, Subhash C.},
	month = mar,
	year = {1988},
	keywords = {matrix multiplication, Mesh design, two-layered mesh array},
	pages = {383--385},
	file = {ScienceDirect Snapshot:/home/lledoux/Zotero/storage/RG8HDJBD/0167819188900786.html:text/html},
}

@article{dezan_synthesis_1991,
	title = {Synthesis of systolic arrays by equation transformations},
	url = {https://www.academia.edu/24295846/Synthesis_of_systolic_arrays_by_equation_transformations},
	abstract = {Synthesis of systolic arrays by equation transformations},
	journal = {Proceedings of the International Conference on Application Specific Array Processors},
	author = {Dezan, C. and Gautrin, E. and Verge, H. Le and Quinton, P. and Saouter, Y.},
	month = jan,
	year = {1991},
	file = {Snapshot:/home/lledoux/Zotero/storage/6QYZC8GV/Synthesis_of_systolic_arrays_by_equation_transformations.html:text/html},
}

@article{rajopadhye_synthesizing_1990,
	title = {Synthesizing systolic arrays from recurrence equations},
	volume = {14},
	issn = {0167-8191},
	url = {https://www.academia.edu/17784589/Synthesizing_systolic_arrays_from_recurrence_equations},
	abstract = {Synthesizing systolic arrays from recurrence equations},
	number = {2},
	journal = {Parallel Computing},
	author = {Rajopadhye, Sanjay V. and Fujimoto, Richard M.},
	year = {1990},
	pages = {163},
	file = {Snapshot:/home/lledoux/Zotero/storage/YV9MRDMH/Synthesizing_systolic_arrays_from_recurrence_equations.html:text/html},
}

@inproceedings{derrien_etude_2002,
	title = {Etude quantitative des techniques de partitionnement de réseaux de processeurs pour l'implantation sur circuits {FPGA}},
	url = {https://www.semanticscholar.org/paper/Etude-quantitative-des-techniques-de-de-r%C3%A9seaux-de-Derrien/f8c21aade9aa095df5ac722738280cfe2994e6ac},
	abstract = {Les outils de synthese de reseaux de processeurs sont utilises pour produire, a partir d'un nid de boucles, une architecture specialisee parallele, bien adaptee a une implantation sur FPGA. Les limites des techniques utilisees imposent cependant l'utilisation de post-transformations " de partitionnement " qui permettent de modifier les caracteristiques des circuits produits. Le probleme est que le choix optimal des parametres associes a ces transformations est tres difficile a effectuer a priori, or une exploration directe de l'espace des solutions est inenvisageable. Dans cette these, nous proposons des modeles haut-nivaux qui permettent d'estimer les caracteristiques de l'architecture (surface, performance, consommation) en fonction des parametres et du type de partitionnement utilise. Ces modeles ont donne lieu a des experimentations sur une plate-forme a base d'un FPGA Virtex, et les resultats obtenus ont permis de confirmer la validite de notre approche.},
	author = {Derrien, Steven},
	year = {2002},
	annote = {[TLDR] Nous proposons des modeles haut-nivaux qui permettent d'estimer les caracteristiques de l'architecture (surface, performance, consommation) en fonction des parametres and du type of partitionnement utilise.},
	file = {Full Text PDF:/home/lledoux/Zotero/storage/HM4JU7BK/Derrien - 2002 - Etude quantitative des techniques de partitionneme.pdf:application/pdf},
}
@inproceedings{isca_NavarroLV86,
  author    = {Juan J. Navarro and
               Jos{\'{e}} M. Llaber{\'{\i}}a and
               Mateo Valero},
  editor    = {Hideo Aiso},
  title     = {Computing Size-Independent Matrix Problems on Systolic Array Processors},
  booktitle = {Proceedings of the 13th Annual Symposium on Computer Architecture,
               Tokyo, Japan, June 1986},
  pages     = {271--278},
  publisher = {{IEEE} Computer Society},
  year      = {1986},
  url       = {https://dl.acm.org/citation.cfm?id=17388},
  timestamp = {Mon, 26 Nov 2018 15:05:58 +0100},
  biburl    = {https://dblp.org/rec/conf/isca/NavarroLV86.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{noauthor_accelerated_nodate,
	author ={Fred Lamert},
	title = {Accelerated {Mathematical} {Engine} {Tesla}},
	url = {https://www.scribd.com/document/398220774/Accelerated-Mathematical-Engine-Tesla},
	abstract = {Accelerated Mathematical Engine Tesla},
	language = {en},

	journal = {Scribd},
	file = {Snapshot:/home/binaryman/Zotero/storage/L89KURW8/Accelerated-Mathematical-Engine-Tesla.html:text/html}
}

@techreport{noauthor_system_nodate,
	author= {Google},
	title = {System {Architecture} {\textbar} {Cloud} {TPU}},
	url = {https://cloud.google.com/tpu/docs/system-architecture},
	year = {2023},
	language = {en},
	journal = {Google Cloud},
}

@misc{wang_benchmarking_2019,
	title = {Benchmarking {TPU}, {GPU}, and {CPU} {Platforms} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1907.10701},
	doi = {10.48550/arXiv.1907.10701},
	abstract = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.},
	publisher = {arXiv},
	author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
	month = oct,
	year = {2019},
	note = {arXiv:1907.10701 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@inproceedings{jouppi_-datacenter_2017,
	address = {New York, NY, USA},
	series = {{ISCA} '17},
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}},
	isbn = {978-1-4503-4892-8},
	url = {https://dl.acm.org/doi/10.1145/3079856.3080246},
	doi = {10.1145/3079856.3080246},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
	booktitle = {Proceedings of the 44th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	month = jun,
	year = {2017},
	keywords = {accelerator, CNN, deep learning, DNN, domain-specific architecture, GPU, LSTM, MLP, neural network, RNN, TensorFlow, TPU},
	pages = {1--12},
}

@article{nordhoff_mis-use_2023,
	title = {({Mis}-)use of standard {Autopilot} and {Full} {Self}-{Driving} ({FSD}) {Beta}: {Results} from interviews with users of {Tesla}'s {FSD} {Beta}},
	volume = {14},
	issn = {1664-1078},
	shorttitle = {({Mis}-)use of standard {Autopilot} and {Full} {Self}-{Driving} ({FSD}) {Beta}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1101520},
	abstract = {Tesla's Full Self-Driving Beta (FSD) program introduces technology that extends the operational design domain of standard Autopilot from highways to urban roads. This research conducted 103 in-depth semi-structured interviews with users of Tesla's FSD Beta and standard Autopilot to evaluate the impact on user behavior and perception. It was found that drivers became complacent over time with Autopilot engaged, failing to monitor the system, and engaging in safety-critical behaviors, such as hands-free driving, enabled by weights placed on the steering wheel, mind wandering, or sleeping behind the wheel. Drivers' movement of eyes, hands, and feet became more relaxed with experience with Autopilot engaged. FSD Beta required constant supervision as unfinished technology, which increased driver stress and mental and physical workload as drivers had to be constantly prepared for unsafe system behavior (doing the wrong thing at the worst time). The hands-on wheel check was not considered as being necessarily effective in driver monitoring and guaranteeing safe use. Drivers adapt to automation over time, engaging in potentially dangerous behaviors. Some behavior seems to be a knowing violation of intended use (e.g., weighting the steering wheel), and other behavior reflects a misunderstanding or lack of experience (e.g., using Autopilot on roads not designed for). As unfinished Beta technology, FSD Beta can introduce new forms of stress and can be inherently unsafe. We recommend future research to investigate to what extent these behavioral changes affect accident risk and can be alleviated through driver state monitoring and assistance.},
	journal = {Frontiers in Psychology},
	author = {Nordhoff, Sina and Lee, John D. and Calvert, Simeon C. and Berge, Siri and Hagenzieker, Marjan and Happee, Riender},
	year = {2023},
}

@article{linja_when_2022,
	title = {When {Self}-{Driving} {Fails}: {Evaluating} {Social} {Media} {Posts} {Regarding} {Problems} and {Misconceptions} about {Tesla}’s {FSD} {Mode}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2414-4088},
	shorttitle = {When {Self}-{Driving} {Fails}},
	url = {https://www.mdpi.com/2414-4088/6/10/86},
	doi = {10.3390/mti6100086},
	abstract = {With the recent deployment of the latest generation of Tesla’s Full Self-Driving (FSD) mode, consumers are using semi-autonomous vehicles in both highway and residential driving for the first time. As a result, drivers are facing complex and unanticipated situations with an unproven technology, which is a central challenge for cooperative cognition. One way to support cooperative cognition in such situations is to inform and educate the user about potential limitations. Because these limitations are not always easily discovered, users have turned to the internet and social media to document their experiences, seek answers to questions they have, provide advice on features to others, and assist other drivers with less FSD experience. In this paper, we explore a novel approach to supporting cooperative cognition: Using social media posts can help characterize the limitations of the automation in order to get information about the limitations of the system and explanations and workarounds for how to deal with these limitations. Ultimately, our goal is to determine the kinds of problems being reported via social media that might be useful in helping users anticipate and develop a better mental model of an AI system that they rely on. To do so, we examine a corpus of social media posts about FSD problems to identify (1) the typical problems reported, (2) the kinds of explanations or answers provided by users, and (3) the feasibility of using such user-generated information to provide training and assistance for new drivers. The results reveal a number of limitations of the FSD system (e.g., lane-keeping and phantom braking) that may be anticipated by drivers, enabling them to predict and avoid the problems, thus allowing better mental models of the system and supporting cooperative cognition of the human-AI system in more situations.},
	language = {en},
	number = {10},
	journal = {Multimodal Technologies and Interaction},
	author = {Linja, Anne and Mamun, Tauseef Ibne and Mueller, Shane T.},
	month = oct,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cooperative cognition, Explainable AI, Tesla FSD, user-centered AI},
	pages = {86},
}

@article{mamun_use_2023,
	title = {The {Use} of {Social} {Forums} to {Train} {Users} {About} {Shortcomings} of {Tesla} {Full} {Self}-{Driving} ({FSD})},
	issn = {2169-5067},
	url = {https://doi.org/10.1177/21695067231193644},
	doi = {10.1177/21695067231193644},
	abstract = {In the past decade, consumer adoption of commercial semi-autonomous vehicles has increased, and along with it user concerns about the shortcomings of these systems, especially regarding safety. Users often turn to social media forums to discuss these shortcomings, find workarounds, and confirm their experience is common. We suggest that these forums may provide some of the best training for users to understand the limitations of AI, as they are not controlled by the vendor who has a vested interest in hiding the limitations of their systems. In two laboratory experiments, we examined how information from Tesla FSD forums impact participants' ability to detect and predict hazardous driving situations in simulated scenarios. Drivers who received the training were better at anticipating and recognizing dangerous driving conditions, suggesting that exposure to user-generated explanations of the shortcomings of the system may in fact improve safety and acceptance of the systems.},
	language = {en},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Mamun, Tauseef Ibne and Mueller, Shane T.},
	month = oct,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
}
@article{talpes_compute_2020,
	title = {Compute {Solution} for {Tesla}'s {Full} {Self}-{Driving} {Computer}},
	volume = {40},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/abstract/document/9007413},
	doi = {10.1109/MM.2020.2975764},
	abstract = {Tesla's full self-driving (FSD) computer is the world's first purpose-built computer for the highly demanding workloads of autonomous driving. It is based on a new System on a Chip (SoC) that integrates industry standard components such as CPUs, ISP, and GPU, together with our custom neural network accelerators. The FSD computer is capable of processing up to 2300 frames per second, a 21× improvement over Tesla's previous hardware and at a lower cost, and when fully utilized, enables a new level of safety and autonomy on the road.},
	number = {2},
	journal = {IEEE Micro},
	author = {Talpes, Emil and Sarma, Debjit Das and Venkataramanan, Ganesh and Bannon, Peter and McGee, Bill and Floering, Benjamin and Jalote, Ankit and Hsiong, Christopher and Arora, Sahil and Gorti, Atchyuth and Sachdev, Gagandeep S.},
	month = mar,
	year = {2020},
	note = {Conference Name: IEEE Micro},
	pages = {25--35},
}
@article{he_using_2000,
	title = {Using {Accurate} {Arithmetics} to {Improve} {Numerical} {Reproducibility} and {Stability} in {Parallel} {Applications}},
	volume = {18},
	doi = {10.1023/A:1008153532043},
	abstract = {Numerical reproducibility and stability of large scale scientic simulations, especially climate modeling, on distributed memory parallel computers are becoming critical issues. In particular, global summation of distributed arrays is most susceptible to rounding errors, and their propagation and accumulation cause uncertainty in nal simulation results. We analyzed several accurate summation methods and found that two methods are particularly eective to improve (ensure) reproducibility and stability: Kahan's self-compensated summation and Bailey's double-double precision summation.},
	journal = {Journal of Supercomputing},
	author = {He, Yun and DING, CHRIS},
	month = sep,
	year = {2000},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/G5CDRQA2/He and DING - 2000 - Using Accurate Arithmetics to Improve Numerical Re.pdf:application/pdf}
}



@article{frolov_highly_2003,
	title = {Highly accurate evaluation of the few-body auxiliary functions and four-body integrals},
	volume = {36},
	doi = {10.1088/0953-4075/36/9/315},
	abstract = {Analytical formulae suitable for numerical calculations of the second- and third-order auxiliary functions A2 (k, m, a, b) and A3 (k, ℓ, m, a, b, c) are presented. These formulae can directly be used in highly accurate calculations of the A2 (k, m, a, b) and A3 (k, ℓ, m, a, b, c) functions. In turn, the highly accurate auxiliary functions of the second and third order are used to compute various four-body integrals, fourth-order auxiliary functions A4 (k, ℓ, m, n, a, b, c, d) and so-called general four-body integrals. The A2 (k, m, a, b) and A3 (k, ℓ, m, a, b, c) functions can be used to solve a large number of four-, five- and many-body problems from atomic, nuclear and molecular physics.},
	journal = {Journal of Physics B: Atomic, Molecular and Optical Physics},
	author = {Frolov, Alexei and Bailey, David},
	month = apr,
	year = {2003},
	pages = {1857},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/BUTK6J3B/Frolov and Bailey - 2003 - Highly accurate evaluation of the few-body auxilia.pdf:application/pdf}
}

@article{lake_sir_1996,
	title = {From {Sir} {Isaac} to the {Sloan} {Survey} {Calculating} the {Structure} and {Chaos} {Owing} to {Gravity} in the {Universe}},
	doi = {10.1145/314161.314166},
	abstract = {The orbit of any one planet depends on the combined motion of all the planets, not to mention the actions of all these on each other. To consider simultaneously all these causes of motion and to define these motions by exact laws allowing of convenient calculation exceeds, unless I am mistaken, the forces of the entire human intellect. ---Isaac Newton 1687 Epochal surveys are throwing down the gauntlet for cosmological simulation. We describe three keys to meeting the challenge of N-body simulation: adaptive potential solvers, adaptive integrators and volume renormalization. With these techniques and a dedicated Teraflop facility, simulation can stay even with observation of the Universe. We also describe some problems in the formation and stability of planetary systems. Here, the challenge is to perform accurate integrations that retain Hamiltonian properties for 10 13 timesteps. 1 The Scientific Importance of Cosmological N-body Simulation. Simulations are required to calculate ...},
	author = {Lake, George and Quinn, Thomas and Richardson, Derek},
	month = nov,
	year = {1996},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/AQ85L4P8/Lake et al. - 1996 - From Sir Isaac to the Sloan Survey Calculating the.pdf:application/pdf}
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	language = {en},

	journal = {arXiv:1603.04467 [cs]},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04467},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	annote = {Comment: Version 2 updates only the metadata, to correct the formatting of Mart{\textbackslash}'in Abadi's name},
	file = {Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:/home/binaryman/Zotero/storage/CPHRLWT3/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf}
}


@ARTICLE{Morris71,
  author={R. {Morris}},
  journal={IEEE Transactions on Computers},
  title={Tapered Floating Point: A New Floating-Point Representation},
  year={1971},
  volume={C-20},
  number={12},
  pages={1578-1579},
  doi={10.1109/T-C.1971.223174}}
}

@book{khan_algorithms_2013,
	title = {Algorithms of {Informatics}, {Volume} {III}},
	isbn = {978-963-87596-7-2},
	author = {Khan, Muhammad and Anisiu, Mira-Cristiana and Domoszali, L. and Iványi, Antal and Kasa, Zoltan and Pirzada, Shariefuddin and Szécsi, László and Szidarovszky, Ferenc and Szirmay-Kalos, László and Vizvári, Béla},
	month = sep,
	year = {2013},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/PFHL6IZ9/Khan et al. - 2013 - Algorithms of Informatics, Volume III.pdf:application/pdf}
}

@article{tambe_adaptivfloat_2020,
	title = {{AdaptivFloat}: {A} {Floating}-point based {Data} {Type} for {Resilient} {Deep} {Learning} {Inference}},
	shorttitle = {{AdaptivFloat}},
	url = {http://arxiv.org/abs/1909.13271},
	abstract = {Conventional hardware-friendly quantization methods, such as ﬁxed-point or integer, tend to perform poorly at very low word sizes as their shrinking dynamic ranges cannot adequately capture the wide data distributions commonly seen in sequence transduction models. We present AdaptivFloat, a ﬂoating-point inspired number representation format for deep learning that dynamically maximizes and optimally clips its available dynamic range, at a layer granularity, in order to create faithful encoding of neural network parameters. AdaptivFloat consistently produces higher inference accuracies compared to block ﬂoating-point, uniform, IEEE-like ﬂoat or posit encodings at very low precision (≤ 8-bit) across a diverse set of state-of-the-art neural network topologies. And notably, AdaptivFloat is seen surpassing baseline FP32 performance by up to +0.3 in BLEU score and -0.75 in word error rate at weight bit widths that are ≤ 8-bit. Experimental results on a deep neural network (DNN) hardware accelerator, exploiting AdaptivFloat logic in its computational datapath, demonstrate per-operation energy and area that is 0.9× and 1.14×, respectively, that of equivalent bit width integer-based accelerator variants.},
	language = {en},

	journal = {arXiv:1909.13271 [cs, stat]},
	author = {Tambe, Thierry and Yang, En-Yu and Wan, Zishen and Deng, Yuntian and Reddi, Vijay Janapa and Rush, Alexander and Brooks, David and Wei, Gu-Yeon},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.13271},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages},
	file = {Tambe et al. - 2020 - AdaptivFloat A Floating-point based Data Type for.pdf:/home/binaryman/Zotero/storage/L8YZ7NYH/Tambe et al. - 2020 - AdaptivFloat A Floating-point based Data Type for.pdf:application/pdf}
}

@misc{munafo_survey,
	author = {Munafo, Robert},
	title = {Survey of {Floating}-{Point} {Formats} at {MROB}},
	year = {2016},
	month = may,
	url = {http://www.mrob.com/pub/math/floatformats.html},
}

@misc{shibo_tpu_2019,
	author= {Shibo Wang},
	year = {2019},
	title = {{BFloat16}: {The} secret to high performance on {Cloud} {TPUs}},
	shorttitle = {{BFloat16}},
	url = {https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus/},
	abstract = {How the high performance of Google Cloud TPUs is driven by Brain Floating Point Format, or bfloat16},
	language = {en},

	journal = {Google Cloud Blog},
}

@inproceedings{chromczak_architectural_2020,
	address = {New York, NY, USA},
	series = {{FPGA} '20},
	title = {Architectural {Enhancements} {in} {Intel} {Agilex} {FPGAs}},
	isbn = {978-1-4503-7099-8},
	url = {https://doi.org/10.1145/3373087.3375308},
	doi = {10.1145/3373087.3375308},
	abstract = {This paper describes architectural enhancements in Intel® Agilex™ FPGAs and SoCs. Agilex devices are built on Intel's 10nm process and feature next-generation programmable fabric, tightly coupled with a quad-core ARM processor subsystem, a secure device manager, IO and memory interfaces, and multiple companion transceiver tile choices. The Agilex fabric features multiple logic block enhancements that significantly improve propagation delays and integrate more effectively with the second-generation HyperFlexAgilex™ pipelined routing architecture. Routing connections are re-designed to be point-to-point, dropping intermediate connections featured in prior FPGA generations and replacing them with a wider variety of shorter wire types. Fine-grain programmable clock skew and time-borrowing were introduced throughout the fabric to augment the slack-balancing capabilities of HyperFlex registers. DSP capabilities are also extended to natively support new INT9/BFLOAT16/FP16 formats. Together, along with process and circuit enhancements, these changes support more than 40\% performance improvement over the Stratix® 10 family of FPGAs.},

	booktitle = {Proceedings of the 2020 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Chromczak, Jeffrey and Wheeler, Mark and Chiasson, Charles and How, Dana and Langhammer, Martin and Vanderhoek, Tim and Zgheib, Grace and Ganusov, Ilya},
	month = feb,
	year = {2020},
	keywords = {clock skew scheduling, dsp, fpga, logic module, routing},
	pages = {140--149}
}

@inproceedings{ganusov_agilex_2020,
	address = {Palo Alto, CA, USA},
	title = {Agilex™ {Generation} of {Intel}® {FPGAs}},
	isbn = {978-1-72817-129-6},
	url = {https://ieeexplore.ieee.org/document/9220557/},
	doi = {10.1109/HCS49909.2020.9220557},
	language = {en},

	booktitle = {2020 {IEEE} {Hot} {Chips} 32 {Symposium} ({HCS})},
	publisher = {IEEE},
	author = {Ganusov, Ilya K. and Iyer, Mahesh A. and Cheng, Ning and Meisler, Alon},
	month = aug,
	year = {2020},
	pages = {1--26},
}

@misc{nigel_bfloat16_2019,
	author= {Nigel Stephens},
	year = {2019},
	title = {{BFloat16} extensions for {Armv8}-{A}},
	url = {https://community.arm.com/developer/ip-products/processors/b/ml-ip-blog/posts/bfloat16-processing-for-neural-networks-on-armv8_2d00_a},
	abstract = {The next revision of the Armv8-A architecture will introduce Neon and SVE vector instruction designed to accelerate Neural Networks using the BFloat16 format.},
	language = {en},

}

@inproceedings{wang_distributed_2021,
	address = {New York, NY, USA},
	series = {{HPC} {Asia} 2021},
	title = {Distributed {MLPerf} {ResNet50} {Training} on {Intel} {Xeon} {Architectures} with {TensorFlow}},
	isbn = {978-1-4503-8303-5},
	url = {https://doi.org/10.1145/3440722.3440880},
	doi = {10.1145/3440722.3440880},
	abstract = {MLPerf benchmarks, which measure training and inference performance of ML hardware and software, have published three sets of ML training results so far. In all sets of results, ResNet50v1.5 was used as a standard benchmark to showcase the latest developments on image recognition tasks. The latest MLPerf training round (v0.7) featured Intel’s submission with TensorFlow. In this paper, we describe the recent optimization work that enabled this submission. In particular, we enabled BFloat16 data type in ResNet50v1.5 model as well as in Intel-optimized TensorFlow to exploit full potential of 3rd generation Intel Xeon scalable processors that have built-in BFloat16 support. We also describe the performance optimizations as well as the state-of-the-art accuracy/convergence results of ResNet50v1.5 model, achieved with large-scale distributed training (with upto 256 MPI workers) with Horovod. These results lay great foundation to support future MLPerf training submissions with large scale Intel Xeon clusters.},

	booktitle = {The {International} {Conference} on {High} {Performance} {Computing} in {Asia}-{Pacific} {Region} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wei and Hasabnis, Niranjan},
	month = jan,
	year = {2021},
	keywords = {Bfloat16, Convergence, Intel Xeon Architectures, LARS, Low-Precision Training, MLPerf, ResNet50},
	pages = {29--35}
}

@article{collange_full-speed_2014,
	title = {Full-{Speed} {Deterministic} {Bit}-{Accurate} {Parallel} {Floating}-{Point} {Summation} on {Multi}- and {Many}-{Core} {Architectures}},
	doi = {10.13140/2.1.5097.1843},
	abstract = {On modern multi-core, many-core, and heterogeneous architectures, floating-point computations, especially reductions, may become non-deterministic and thus non-reproducible mainly due to non-associativity of floating-point operations. We introduce a solution to compute deterministic sums of floating-point numbers efficiently and with the best possible accuracy. Our multi-level algorithm consists of two main stages: a filtering stage that uses fast vectorized floating-point expansions; an accumulation stage based on superaccumulators in a high-radix carry-save representation. We present implementations on recent Intel desktop and server processors, on Intel Xeon Phi accelerator, and on AMD and NVIDIA GPUs. We show that the numerical reproducibility and bit-perfect accuracy can be achieved at no additional cost for large sums that have dynamic ranges of up to 90 orders of magnitude by leveraging arithmetic units that are left underused by standard reduction algorithms.},
	author = {Collange, Sylvain and Defour, David and Graillat, Stef and Iakymchuk, Roman},
	month = feb,
	year = {2014},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/VICQSXWB/Collange et al. - 2014 - Full-Speed Deterministic Bit-Accurate Parallel Flo.pdf:application/pdf}
}

@misc{iakymchuk_reproducible_2015,
	title = {Reproducible and {Accurate} {Matrix} {Multiplication} for {GPU} {Accelerators}},
	url = {https://hal.archives-ouvertes.fr/hal-01102877},
	abstract = {Due to non-associativity of floating-point operations and dynamic scheduling on parallel architectures, getting a bitwise reproducible floating-point result for multiple executions of the same code on different or even similar parallel architectures is challenging. In this paper, we address the problem of reproducibility in the context of matrix multiplication and propose an algorithm that yields both reproducible and accurate results. This algorithm is composed of two main stages: a filtering stage that uses fast vectorized floating-point expansions in con-junction with error-free transformations; an accumulation stage based on Kulisch long accumulators in a high-radix carry-save representation. Finally, we provide implementations and performance results in parallel environments like GPUs.},
	language = {en},

	author = {Iakymchuk, Roman and Defour, David and Collange, Sylvain and Graillat, Stef},
	month = jan,
	year = {2015},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/LZLAXL4X/Iakymchuk et al. - 2015 - Reproducible and Accurate Matrix Multiplication fo.pdf:application/pdf;Snapshot:/home/binaryman/Zotero/storage/5YM83B4V/hal-01102877.html:text/html}
}

@article{jain_clarinet_2020,
	title = {{CLARINET}: {A} {RISC}-{V} {Based} {Framework} for {Posit} {Arithmetic} {Empiricism} V1},
	shorttitle = {{CLARINET}},
	url = {http://arxiv.org/abs/2006.00364},
	abstract = {Many engineering and scientific applications require high precision arithmetic. IEEE 754-2008 compliant (floating-point) arithmetic is the de facto standard for performing these computations. Recently, posit arithmetic has been proposed as a drop-in replacement for floating-point arithmetic. The posit data representation and arithmetic offer several absolute advantages over the floating-point format and arithmetic including higher dynamic range, better accuracy, and superior performance-area trade-offs. In this paper, we present a consolidated general-purpose processor-based framework to support posit arithmetic empiricism. The end-users of the framework have the liberty to seamlessly experiment with their applications using posit and floating-point arithmetic since the framework is designed for the two number systems to coexist. The framework consists of Melodica and Clarinet. Melodica is a posit arithmetic core that implements parametric fused-multiply-accumulate and, more importantly, supports the quire data type. Clarinet is a Melodica-enabled processor based on the RISC-V ISA. To the best of our knowledge, this is the first-ever integration of quire to a RISC-V core. To show the effectiveness of the Clarinet platform, we perform an extensive application study and benchmarking on some of the common linear algebra and computer vision kernels. We perform ASIC synthesis of Clarinet and Melodica on a 90 nm-CMOS Faraday process. Finally, based on our analysis and synthesis results, we define a quality metric for the different instances of Clarinet that gives us initial recommendations on the goodness of the instances. Clarinet-Melodica is an easy-to-experiment platform that will be made available in open-source for posit arithmetic empiricism.},

	journal = {arXiv:2006.00364 [cs]},
	author = {Jain, Riya and Sharma, Niraj and Merchant, Farhad and Patkar, Sachin and Leupers, Rainer},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.00364},
	keywords = {Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:/home/binaryman/Zotero/storage/LCBQ2QAU/Jain et al. - 2020 - CLARINET A RISC-V Based Framework for Posit Arith.pdf:application/pdf;arXiv.org Snapshot:/home/binaryman/Zotero/storage/JJAK6UNT/2006.html:text/html}
}

@misc{sharma2021clarinet,
      title={CLARINET: A RISC-V Based Framework for Posit Arithmetic Empiricism V4},
      author={Niraj Sharma and Riya Jain and Madhumita Mohan and Sachin Patkar and Rainer Leupers and Nikhil Rishiyur and Farhad Merchant},
      year={2021},
      eprint={2006.00364},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@inproceedings{mallasen_customizing_2022,
	title = {Customizing the {CVA6} {RISC}-{V} {Core} to {Integrate} {Posit} and {Quire} {Instructions}},
	url = {https://ieeexplore.ieee.org/document/9970026},
	doi = {10.1109/DCIS55711.2022.9970026},
	abstract = {The posit representation for real numbers, aka Unum-v3, is an alternative to substitute the IEEE 754 standard and thus mitigate the inherent problems to the construction of floating-point numbers. Nonetheless, posits are not standard yet, and previously there was no approach, neither academically nor industrially, which implemented a fully compliant core for deploying this novel format. Recently, the open-source PERCIVAL posit RISC-V core was presented as the first work that fully integrates posit arithmetic and quire capabilities into hardware. In addition, Xposit, a RISC-V extension for posit operations allows for the compilation of C programs with inline assembly posit and quire instructions. As a study platform, PERCIVAL is based on the CVA6 core and has support for both posit and IEEE 754 formats, further permitting the comparison of these representations. This paper details the microarchitecture of the Posit Arithmetic Unit with quire added to this core. It also describes how to perform the necessary additions and modifications to the CVA6 core to add support for the Xposit RISC-V custom extension. Furthermore, FPGA synthesis results highlight the cost of including support for both posits with quire and IEEE 754 formats. This is done by breaking down the area resources needed for every arithmetic configuration.},
	booktitle = {2022 37th {Conference} on {Design} of {Circuits} and {Integrated} {Circuits} ({DCIS})},
	author = {Mallasén, David and Murillo, Raul and Del Barrio, Alberto A. and Botella, Guillermo and Piñuel, Luis and Prieto–Matias, Manuel},
	month = nov,
	year = {2022},
	note = {ISSN: 2640-5563},
	pages = {01--06},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/M3Q2LQMG/9970026.html:text/html},
}

@misc{arunkumarm_perc_2020,
	title = {{PERC}: {Posit} {Enhanced} {Rocket} {Chip}},
	shorttitle = {{PERC}},
	abstract = {Balancing between precision and performance is a known trade-off in system design. Universal Number system attempts to dissolve this trade-off with its flexible arbitrary precision bound within fixed bit length. Its Type-III class, also known as Posits, has been especially introduced to be hardware implementation friendly. Posit is a dynamic floating-point representation that ensures better accuracy and precision using a system that minimizes the number of unusable representations and introduces a higher dynamic range which can serve as a substitute for the IEEE-754 2008 floating-point standard. In this paper, we share our experience with implementation and integration of a Posit Processing Unit (PPU) into the Rocket Chip SoC generator. This PPU replaces the IEEE-754 2008 FPU inside the chip, and supports both of RISC-V ISA floating-point extensions namely ‘F’ for single precision and ‘D’ for double precision using 32-bit and 64-bit posits respectively. We discuss various design choices that were available to us and the decisions made in this work. We elaborate our use of Chisel, a Scala embedded hardware construction DSL, to describe our design. Later we observe how various constructs in Chisel help not only to describe a product but also aids the description process in a robust, flexible and efficient manner. We further delve into how the design has been tested using a version of the RISC-V ISA test suite which has been modified for Posit arithmetic numbers. The paper also discusses the scope for future work that can be done, including a posit arithmetic accelerator and higher-level toolchain support for posits. ACM Reference Format: Arunkumar M. V., Sai Ganesh Bhairathi, and Harshal G. Hayatnagarkar. 2020. PERC: Posit Enhanced Rocket Chip. In Proceedings of Fourth Workshop on Computer Architecture Research with RISC-V (CARRV 2020). ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CARRV 2020, May 30, 2020, Valencia, Spain © 2020 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . \$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn},
	language = {en},

	author = {ArunkumarM, V. and Bhairathi, Sai Ganesh and Hayatnagarkar, Harshal},
	year = {2020},
	file = {Snapshot:/home/binaryman/Zotero/storage/7Y38Q86D/555f2633f65af768cd15c5fddeedf410f9911818.html:text/html}
}

@article{zaruba2019cost,
   author={F. {Zaruba} and L. {Benini}},
   journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
   title={The Cost of Application-Class Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V Core in 22-nm FDSOI Technology},
   year={2019},
   volume={27},
   number={11},
   pages={2629-2640},
   doi={10.1109/TVLSI.2019.2926114},
   ISSN={1557-9999},
   month=Nov,
}

@article{tiwari_peri_2021,
	title = {{PERI}: {A} {Configurable} {Posit} {Enabled} {RISC}-{V} {Core}},
	volume = {18},
	issn = {1544-3566},
	shorttitle = {{PERI}},
	url = {https://doi.org/10.1145/3446210},
	doi = {10.1145/3446210},
	abstract = {Owing to the failure of Dennard’s scaling, the past decade has seen a steep growth of prominent new paradigms leveraging opportunities in computer architecture. Two technologies of interest are Posit and RISC-V. Posit was introduced in mid-2017 as a viable alternative to IEEE-754, and RISC-V provides a commercial-grade open source Instruction Set Architecture (ISA). In this article, we bring these two technologies together and propose a Configurable Posit Enabled RISC-V Core called PERI. The article provides insights on how the Single-Precision Floating Point (“F”) extension of RISC-V can be leveraged to support posit arithmetic. We also present the implementation details of a parameterized and feature-complete posit Floating Point Unit (FPU). The configurability and the parameterization features of this unit generate optimal hardware, which caters to the accuracy and energy/area tradeoffs imposed by the applications, a feature not possible with IEEE-754 implementation. The posit FPU has been integrated with the RISC-V compliant SHAKTI C-class core as an execution unit. To further leverage the potential of posit, we enhance our posit FPU to support two different exponent sizes (with posit-size being 32-bits), thereby enabling multiple-precision at runtime. To enable the compilation and execution of C programs on PERI, we have made minimal modifications to the GNU C Compiler (GCC), targeting the “F” extension of the RISC-V. We compare posit with IEEE-754 in terms of hardware area, application accuracy, and runtime. We also present an alternate methodology of integrating the posit FPU with the RISC-V core as an accelerator using the custom opcode space of RISC-V.},
	number = {3},

	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Tiwari, Sugandha and Gala, Neel and Rebeiro, Chester and Kamakoti, V.},
	month = apr,
	year = {2021},
	keywords = {floating-point, IEEE-754, Posit, processor, RISC-V},
	pages = {25:1--25:26},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/8TXYECZ2/Tiwari et al. - 2021 - PERI A Configurable Posit Enabled RISC-V Core.pdf:application/pdf}
}


@inproceedings{gala_shakti_2016,
	title = {{SHAKTI} {Processors}: {An} {Open}-{Source} {Hardware} {Initiative}},
	shorttitle = {{SHAKTI} {Processors}},
	url = {https://ieeexplore.ieee.org/document/7434907},
	doi = {10.1109/VLSID.2016.130},
	abstract = {Summary form only given. State-of-the-Art Computer Architecture research at Indian Academic Institutions is majorly restricted due to unavailability of processor design models that are close to current commercially available cores. The SHAKTI processor initiative aims at breaking this barrier between Academia and Industry by providing open-source Processor and SoC designs. With the advent of RISC-V ISA by UC Berkeley, we have a simple, clean and most importantly an open source ISA that can be used to design processors which have the potential to match the current day processors in the market. The initiative is also driven to provide substantial information about design decisions and promote more competitive learning environment in academia. The processors from SHAKTI will help in aiding research related to architecture, where one can run simulations on the actual hardware and obtain much accurate results, rather than settling with a lesser accurate software simulation. Since these processors and designs are targeted for real-world use, they can also be freely adopted by industries, thereby supporting the initiative further in terms of product-driven research.In this workshop we plan to talk about the various designs that we are working on and how they can be used. The proposed tutorial consists of four parts. The first part emphasizes on the need for open-source hardware design and the rationale behind our choices of ISA and HDL. The second part covers about our microcontroller class (C-class) core and the verification and debug environment. The third part presents our flagship processor which is the industrial class (I-class) core; the various design decisions involved and some performance metrics. The final part concludes on the note of future work and upcoming releases under SHAKTI.},
	booktitle = {2016 29th {International} {Conference} on {VLSI} {Design} and 2016 15th {International} {Conference} on {Embedded} {Systems} ({VLSID})},
	author = {Gala, Neel and Menon, Arjun and Bodduna, Rahul and Madhusudan, G. S. and Kamakoti, V.},
	month = jan,
	year = {2016},
	note = {ISSN: 2380-6923},
	pages = {7--8},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/ABVN7NHK/7434907.html:text/html},
}

@article{veezhinathan_building_2022,
	title = {Building the {SHAKTI} microprocessor},
	volume = {65},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3556632},
	doi = {10.1145/3556632},
	number = {11},
	journal = {Commun. ACM},
	author = {Veezhinathan, Kamakoti},
	month = oct,
	year = {2022},
	pages = {48--51},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/Y8E7KFQ9/Veezhinathan - 2022 - Building the SHAKTI microprocessor.pdf:application/pdf},
}

@article{zhang_design_2020,
	title = {Design of {Power} {Efficient} {Posit} {Multiplier}},
	volume = {67},
	issn = {1558-3791},
	doi = {10.1109/TCSII.2020.2980531},
	abstract = {Posit number system has been used as an alternative to IEEE floating-point number system in many applications, especially the recent popular deep learning. Its non-uniformed number distribution fits well with the data distribution of deep learning and thus can speedup the training process of deep learning. Among all the related arithmetic operations, multiplication is one of the most frequent operations used in applications. However, due to the bit-width flexibility nature of posit numbers, the hardware multiplier is usually designed with the maximum possible mantissa bit-width. As the mantissa bit-width is not always the maximum value, such multiplier design leads to a high power consumption especially when the mantissa bit-width is small. In this brief, a power efficient posit multiplier architecture is proposed. The mantissa multiplier is still designed for the maximum possible bit-width, however, the whole multiplier is divided into multiple smaller multipliers. Only the required small multipliers are enabled at run-time. Those smaller multipliers are controlled by the regime bit-width which can be used to determine the mantissa bit-width. This design technique is applied to 8-bit, 16-bit, and 32-bit posit formats in this brief and an average of 16\% power reduction can be achieved with negligible area and timing overhead.},
	number = {5},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {Zhang, Hao and Ko, Seok-Bum},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {Adders, Computer architecture, computer arithmetic, Deep learning, Generators, Hardware, low-power arithmetic circuit, posit multiplier, Posit number system, Power demand, Standards},
	pages = {861--865},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/Y5BH8Z3Y/9035440.html:text/html}
}

@inproceedings{chaurasiya_parameterized_2018,
	address = {Orlando, FL, USA},
	title = {Parameterized {Posit} {Arithmetic} {Hardware} {Generator}},
	isbn = {978-1-5386-8477-1},
	url = {https://ieeexplore.ieee.org/document/8615707/},
	doi = {10.1109/ICCD.2018.00057},
	abstract = {Hardware implementation of Floating Point Units (FPUs) has been a key area of research due to their massive area and energy footprints. Recently, a proposal was made to replace IEEE 754-2008 technical standard compliant FPUs with Posit Arithmetic Units (PAUs) due to the greater accuracy, speed, and simpler hardware design. In this paper, we present the architecture of a parameterized PAU generator that can generate PAU adders and PAU multipliers of any bit-width pre-synthesis. We synthesize generated arithmetic units using the parameterized PAU generator for 8-bit, 16-bit, and 32-bit adders and multipliers and compare them with IEEE 754-2008 compliant adders and multipliers. Both, synthesis for Field Programmable Gate Array (FPGA) and Application Speciﬁc Integrated Circuit (ASIC) are performed. In our comparison of m-bit PAU units with n-bit IEEE 754-2008 compliant units, it is observed that the area and energy of a PAU adder and multiplier are comparable to their IEEE 754-2008 compliant counterparts where m = n. We argue that an n-bit IEEE 754-2008 adder and multiplier can be safely replaced with an m-bit PAU adder and multiplier where m {\textless} n, due to superior numerical accuracy of the PAU; we also compare m-bit PAU adders and multipliers with n-bit IEEE 754-2008 compliant adders and multipliers. As an application example, we examine performance in the domain of signal processing with and without PAU adders and multipliers, and show the advantage of our approach.},
	language = {en},
	booktitle = {2018 {IEEE} 36th {International} {Conference} on {Computer} {Design} ({ICCD})},
	publisher = {IEEE},
	author = {Chaurasiya, Rohit and Gustafson, John and Shrestha, Rahul and Neudorfer, Jonathan and Nambiar, Sangeeth and Niyogi, Kaustav and Merchant, Farhad and Leupers, Rainer},
	month = oct,
	year = {2018},
	pages = {334--341},
}

@inproceedings{moss_customizable_2018,
	address = {New York, NY, USA},
	series = {{FPGA} '18},
	title = {A {Customizable} {Matrix} {Multiplication} {Framework} for the {Intel} {HARPv2} {Xeon}+{FPGA} {Platform}: {A} {Deep} {Learning} {Case} {Study}},
	isbn = {978-1-4503-5614-5},
	shorttitle = {A {Customizable} {Matrix} {Multiplication} {Framework} for the {Intel} {HARPv2} {Xeon}+{FPGA} {Platform}},
	url = {https://doi.org/10.1145/3174243.3174258},
	doi = {10.1145/3174243.3174258},
	abstract = {General Matrix to Matrix multiplication (GEMM) is the cornerstone for a wide gamut of applications in high performance computing (HPC), scientific computing (SC) and more recently, deep learning. In this work, we present a customizable matrix multiplication framework for the Intel HARPv2 CPU+FPGA platform that includes support for both traditional single precision floating point and reduced precision workloads. Our framework supports arbitrary size GEMMs and consists of two parts: (1) a simple application programming interface (API) for easy configuration and integration into existing software and (2) a highly customizable hardware template. The API provides both compile and runtime options for controlling key aspects of the hardware template including dynamic precision switching; interleaving and block size control; and fused deep learning specific operations. The framework currently supports single precision floating point (FP32), 16, 8, 4 and 2 bit Integer and Fixed Point (INT16, INT8, INT4, INT2) and more exotic data types for deep learning workloads: INT16xTernary, INT8xTernary, BinaryxBinary. We compare our implementation to the latest NVIDIA Pascal GPU and evaluate the performance benefits provided by optimizations built into the hardware template. Using three neural networks (AlexNet, VGGNet and ResNet) we illustrate that reduced precision representations such as binary achieve the best performance, and that the HARPv2 enables fine-grained partitioning of computations over both the Xeon and FPGA. We observe up to 50x improvement in execution time compared to single precision floating point, and that runtime configuration options can improve the efficiency of certain layers in AlexNet up to 4x, achieving an overall 1.3x improvement over the entire network.},

	booktitle = {Proceedings of the 2018 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Moss, Duncan J.M and Krishnan, Srivatsan and Nurvitadhi, Eriko and Ratuszniak, Piotr and Johnson, Chris and Sim, Jaewoong and Mishra, Asit and Marr, Debbie and Subhaschandra, Suchit and Leong, Philip H.W.},
	month = feb,
	year = {2018},
	keywords = {deep learning, fpga, heterogeneous architectures, neural networks, reduced precision},
	pages = {107--116}
}

@inproceedings{zhuo_sparse_2005,
	address = {New York, NY, USA},
	series = {{FPGA} '05},
	title = {Sparse {Matrix}-{Vector} multiplication on {FPGAs}},
	isbn = {978-1-59593-029-3},
	url = {https://doi.org/10.1145/1046192.1046202},
	doi = {10.1145/1046192.1046202},
	abstract = {Floating-point Sparse Matrix-Vector Multiplication (SpMXV) is a key computational kernel in scientific and engineering applications. The poor data locality of sparse matrices significantly reduces the performance of SpMXV on general-purpose processors, which rely heavily on the cache hierarchy to achieve high performance. The abundant hardware resources on current FPGAs provide new opportunities to improve the performance of SpMXV. In this paper, we propose an FPGA-based design for SpMXV. Our design accepts sparse matrices in Compressed Row Storage format, and makes no assumptions about the sparsity structure of the input matrix. The design employs IEEE-754 format double-precision floating-point multipliers/adders, and performs multiple floating-point operations as well as I/O operations in parallel. The performance of our design for SpMXV is evaluated using various sparse matrices from the scientific computing community, with the Xilinx Virtex-II Pro XC2VP70 as the target device. The MFLOPS performance increases with the hardware resources on the device as well as the available memory bandwidth. For example, when the memory bandwidth is 8 GB/s, our design achieves over 350 MFLOPS for all the test matrices. It demonstrates significant speedup over general-purpose processors particularly for matrices with very irregular sparsity structure. Besides solving SpMXV problem, our design provides a parameterized and flexible tree-based design for floating-point applications on FPGAs.},

	booktitle = {Proceedings of the 2005 {ACM}/{SIGDA} 13th international symposium on {Field}-programmable gate arrays},
	publisher = {Association for Computing Machinery},
	author = {Zhuo, Ling and Prasanna, Viktor K.},
	month = feb,
	year = {2005},
	keywords = {floating-point, FPGA, high performance, reconfigurable architecture, sparse matrix},
	pages = {63--74}
}

@inproceedings{dou_64-bit_2005,
	address = {New York, NY, USA},
	series = {{FPGA} '05},
	title = {64-bit floating-point {FPGA} matrix multiplication},
	isbn = {978-1-59593-029-3},
	url = {https://doi.org/10.1145/1046192.1046204},
	doi = {10.1145/1046192.1046204},
	abstract = {We introduce a 64-bit ANSI/IEEE Std 754-1985 floating point design of a hardware matrix multiplier optimized for FPGA implementations. A general block matrix multiplication algorithm, applicable for an arbitrary matrix size is proposed. The algorithm potentially enables optimum performance by exploiting the data locality and reusability incurred by the general matrix multiplication scheme and considering the limitations of the I/O bandwidth and the local storage volume. We implement a scalable linear array of processing elements (PE) supporting the proposed algorithm in the Xilinx Virtex II Pro technology. Synthesis results confirm a superior performance-area ratio compared to related recent works. Assuming the same FPGA chip, the same amount of local memory, and the same I/O bandwidth, our design outperforms related proposals by at least 1.7X and up to 18X consuming the least reconfigurable resources. A total of 39 PEs can be integrated into the xc2vp125-7 FPGA, reaching performance of, e.g., 15.6 GFLOPS with 1600 KB local memory and 400 MB/s external memory bandwidth.},

	booktitle = {Proceedings of the 2005 {ACM}/{SIGDA} 13th international symposium on {Field}-programmable gate arrays},
	publisher = {Association for Computing Machinery},
	author = {Dou, Yong and Vassiliadis, S. and Kuzmanov, G. K. and Gaydadjiev, G. N.},
	month = feb,
	year = {2005},
	keywords = {floating-point, FPGA, matrix multiplication},
	pages = {86--95}
}

@article{dhollander_high-level_2017,
	title = {High-{Level} {Synthesis} {Optimization} for {Blocked} {Floating}-{Point} {Matrix} {Multiplication}},
	volume = {44},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/3039902.3039916},
	doi = {10.1145/3039902.3039916},
	abstract = {In the last decade floating-point matrix multiplication on FPGAs has been studied extensively and efficient architectures as well as detailed performance models have been developed. By design these IP cores take a fixed footprint which not necessarily optimizes the use of all available resources. Moreover, the low-level architectures are not easily amenable to a parameterized synthesis. In this paper high-level synthesis is used to fine-tune the configuration parameters in order to achieve the highest performance with maximal resource utilization. An{\textbackslash} exploration strategy is presented to optimize the use of critical resources (DSPs, memory) for any given FPGA. To account for the limited memory size on the FPGA, a blockoriented matrix multiplication is organized such that the block summation is done on the CPU while the block multiplication occurs on the logic fabric simultaneously. The communication overhead between the CPU and the FPGA is minimized by streaming the blocks in a Gray code ordering scheme which maximizes the data reuse for consecutive block matrix product calculations. Using highlevel synthesis optimization, the programmable logic operates at 93\% of the theoretical peak performance and the combined CPU-FPGA design achieves 76\% of the available hardware processing speed for the floating-point multiplication of 2K by 2K matrices.},
	number = {4},

	journal = {ACM SIGARCH Computer Architecture News},
	author = {D'Hollander, Erik H.},
	month = jan,
	year = {2017},
	pages = {74--79},
	file = {Full Text:/home/binaryman/Zotero/storage/MNS5IXZA/D'Hollander - 2017 - High-Level Synthesis Optimization for Blocked Floa.pdf:application/pdf}
}

@inproceedings{guan_fp-dnn_2017,
	title = {{FP}-{DNN}: {An} {Automated} {Framework} for {Mapping} {Deep} {Neural} {Networks} onto {FPGAs} with {RTL}-{HLS} {Hybrid} {Templates}},
	shorttitle = {{FP}-{DNN}},
	doi = {10.1109/FCCM.2017.25},
	abstract = {DNNs (Deep Neural Networks) have demonstrated great success in numerous applications such as image classification, speech recognition, video analysis, etc. However, DNNs are much more computation-intensive and memory-intensive than previous shallow models. Thus, it is challenging to deploy DNNs in both large-scale data centers and real-time embedded systems. Considering performance, flexibility, and energy efficiency, FPGA-based accelerator for DNNs is a promising solution. Unfortunately, conventional accelerator design flows make it difficult for FPGA developers to keep up with the fast pace of innovations in DNNs. To overcome this problem, we propose FP-DNN (Field Programmable DNN), an end-to-end framework that takes TensorFlow-described DNNs as input, and automatically generates the hardware implementations on FPGA boards with RTL-HLS hybrid templates. FP-DNN performs model inference of DNNs with our high-performance computation engine and carefully-designed communication optimization strategies. We implement CNNs, LSTM-RNNs, and Residual Nets with FPDNN, and experimental results show the great performance and flexibility provided by our proposed FP-DNN framework.},
	booktitle = {2017 {IEEE} 25th {Annual} {International} {Symposium} on {Field}-{Programmable} {Custom} {Computing} {Machines} ({FCCM})},
	author = {Guan, Yijin and Liang, Hao and Xu, Ningyi and Wang, Wenqiang and Shi, Shaoshuai and Chen, Xi and Sun, Guangyu and Zhang, Wei and Cong, Jason},
	month = apr,
	year = {2017},
	keywords = {Analytical models, Automation, Computational modeling, Data models, Deep Neural Networks, Field programmable gate arrays, FPGA, Generators, Hardware, Kernel, RTL-HLS},
	pages = {152--159},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/H2EN9BJ5/7966671.html:text/html}
}

@misc{qpi,
	author = {{Intel}®},
	year = {2019},
	title = {An {Introduction} to the {Intel}® {QuickPath} {Interconnect}},
	url = {https://www.intel.com/content/www/us/en/io/quickpath-technology/quick-path-interconnect-introduction-paper.html},
	abstract = {White Paper:  the Intel® QuickPath Interconnect links stitch together processors in distributed shared memory platform architecture.},
	language = {en},

	journal = {Intel},
	file = {Snapshot:/home/binaryman/Zotero/storage/DR4EXIAM/quick-path-interconnect-introduction-paper.html:text/html}
}

@article{faict_mapping_2019,
	title = {Mapping a {Guided} {Image} {Filter} on the {HARP} {Reconfigurable} {Architecture} {Using} {OpenCL}},
	volume = {12},
	doi = {10.3390/a12080149},
	abstract = {Intel recently introduced the Heterogeneous Architecture Research Platform, HARP. In this platform, the Central Processing Unit and a Field-Programmable Gate Array are connected through a high-bandwidth, low-latency interconnect and both share DRAM memory. For this platform, Open Computing Language (OpenCL), a High-Level Synthesis (HLS) language, is made available. By making use of HLS, a faster design cycle can be achieved compared to programming in a traditional hardware description language. This, however, comes at the cost of having less control over the hardware implementation. We will investigate how OpenCL can be applied to implement a real-time guided image filter on the HARP platform. In the first phase, the performance-critical parameters of the OpenCL programming model are defined using several specialized benchmarks. In a second phase, the guided image filter algorithm is implemented using the insights gained in the first phase. Both a floating-point and a fixed-point implementation were developed for this algorithm, based on a sliding window implementation. This resulted in a maximum floating-point performance of 135 GFLOPS, a maximum fixed-point performance of 430 GOPS and a throughput of HD color images at 74 frames per second.},
	journal = {Algorithms},
	author = {Faict, Thomas and D’Hollander, Erik and Goossens, Bart},
	month = jul,
	year = {2019},
	pages = {149},
	file = {Full Text:/home/binaryman/Zotero/storage/YMCSVS89/Faict et al. - 2019 - Mapping a Guided Image Filter on the HARP Reconfig.pdf:application/pdf}
}

@article{stratixV,
	title = {Stratix {V} {Device} {Overview}},
	author = {Altera Corporation},
	year = {2020},
	language = {en},
	pages = {24},
	file = {Stratix V Device Overview.pdf:/home/binaryman/Zotero/storage/X4LIAERW/Stratix V Device Overview.pdf:application/pdf}
}

@misc{virtex-7,
	author = {Xilinx},
	year = {2010},
	title = {Virtex-7 {FPGA} {Family}},
	url = {https://www.xilinx.com/products/silicon-devices/fpga/virtex-7.html},
	abstract = {Virtex®-7 FPGAs are optimized for system performance and integration at 28nm and bring best-in-class performance/watt fabric, DSP performance, and I/O bandwidth to your designs.},
	language = {en},

	journal = {Xilinx},
	file = {Snapshot:/home/binaryman/Zotero/storage/B7ZM95JF/virtex-7.html:text/html}
}

@inproceedings{de_fine_licht_flexible_2020,
	address = {Seaside CA USA},
	title = {Flexible {Communication} {Avoiding} {Matrix} {Multiplication} on {FPGA} with {High}-{Level} {Synthesis}},
	isbn = {978-1-4503-7099-8},
	url = {https://dl.acm.org/doi/10.1145/3373087.3375296},
	doi = {10.1145/3373087.3375296},
	abstract = {Data movement is the dominating factor affecting performance and energy in modern computing systems. Consequently, many algorithms have been developed to minimize the number of I/O operations for common computing patterns. Matrix multiplication is no exception, and lower bounds have been proven and implemented both for shared and distributed memory systems. Reconfigurable hardware platforms are a lucrative target for I/O minimizing algorithms, as they offer full control of memory accesses to the programmer. While bounds developed in the context of fixed architectures still apply to these platforms, the spatially distributed nature of their computational and memory resources requires a decentralized approach to optimize algorithms for maximum hardware utilization. We present a model to optimize matrix multiplication for FPGA platforms, simultaneously targeting maximum performance and minimum off-chip data movement, within constraints set by the hardware. We map the model to a concrete architecture using a high-level synthesis tool, maintaining a high level of abstraction, allowing us to support arbitrary data types, and enables maintainability and portability across FPGA devices. Kernels generated from our architecture are shown to offer competitive performance in practice, scaling with both compute and memory resources. We offer our design as an open source project1 to encourage the open development of linear algebra and I/O minimizing algorithms on reconfigurable hardware platforms.},
	language = {en},

	booktitle = {Proceedings of the 2020 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {ACM},
	author = {de Fine Licht, Johannes and Kwasniewski, Grzegorz and Hoefler, Torsten},
	month = feb,
	year = {2020},
	pages = {244--254},
	file = {de Fine Licht et al. - 2020 - Flexible Communication Avoiding Matrix Multiplicat.pdf:/home/binaryman/Zotero/storage/MV6A8X8W/de Fine Licht et al. - 2020 - Flexible Communication Avoiding Matrix Multiplicat.pdf:application/pdf}
}

@techreport{rocket_generator,
    Author = {Asanović, Krste and Avizienis, Rimas and Bachrach, Jonathan and Beamer, Scott and Biancolin, David and Celio, Christopher and Cook, Henry and Dabbelt, Daniel and Hauser, John and Izraelevitz, Adam and Karandikar, Sagar and Keller, Ben and Kim, Donggyu and Koenig, John and Lee, Yunsup and Love, Eric and Maas, Martin and Magyar, Albert and Mao, Howard and Moreto, Miquel and Ou, Albert and Patterson, David A. and Richards, Brian and Schmidt, Colin and Twigg, Stephen and Vo, Huy and Waterman, Andrew},
    Title = {The Rocket Chip Generator},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2016},
    Month = apr,
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html},
    Number = {UCB/EECS-2016-17},
    Abstract = {Rocket Chip is an open-source Sysem-on-Chip design generator that emits synthesizable RTL. It leverages the Chisel hardware construction language to compose a library of sophisticated generators for cores, caches, and interconnects into an integrated SoC. Rocket Chip generates general-purpose processor cores that use the open RISC-V ISA, and provides both an in-order core generator (Rocket) and an out-of-order core generator (BOOM). For SoC designers interested in utilizing heterogeneous specialization for added efficiency gains, Rocket Chip supports the integration of custom accelerators in the form of instruction set extensions, coprocessors, or fully independent novel cores. Rocket Chip has been taped out (manufactured) eleven times, and yielded functional silicon prototypes capable of booting Linux.}
}

@techreport{boom_ooo,
    Author = {Celio, Christopher and Chiu, Pi-Feng and Nikolic, Borivoje and Patterson, David A. and Asanović, Krste},
    Title = {BOOM v2: an open-source out-of-order RISC-V core},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2017},
    Month = Sep,
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-157.html},
    Number = {UCB/EECS-2017-157},
    Abstract = {This paper presents BOOM version 2, an updated version of the Berkeley Out-of-Order Machine. The design exploration was performed through synthesis, place and route using the foundry-provided standard-cell library and the memory compiler in the TSMC 28 nm HPM process (high performance mobile).

BOOM is an open-source processor that implements the RV64G RISC-V Instruction Set Architecture (ISA). Like most contemporary high-performance cores, BOOM is superscalar (able to execute multiple instructions per cycle) and out-of-order (able to execute instructions as their dependencies are resolved and not restricted to their program order).  BOOM is implemented as a parameterizable generator written using the Chisel hardware construction language that can used to generate synthesizable implementations targeting both FPGAs and ASICs.

BOOMv2 is an update in which the design effort has been informed by analysis of synthesized, placed and routed data provided by a contemporary industrial tool flow. We also had access to standard single- and dual-ported memory compilers provided by the foundry, allowing us to explore design trade-offs using different SRAM memories and comparing against synthesized flip-flop arrays. The main distinguishing features of BOOMv2 include an updated 3-stage front-end design with a bigger set-associative Branch Target Buffer (BTB); a pipelined register rename stage; split floating point and integer register files; a dedicated floating point pipeline; separate issue windows for floating point, integer, and memory micro-operations; and separate stages for issue-select and register read.

Managing the complexity of the register file was the largest obstacle to improving BOOM's clock frequency. We spent considerable effort on placing-and-routing a semi-custom 9-port register file to explore the potential improvements over a fully synthesized design, in conjunction with microarchitectural techniques to reduce the size and port count of the register file. BOOMv2 has a 37 fanout-of-four (FO4) inverter delay after synthesis and 50 FO4 after place-and-route, a 24% reduction from BOOMv1's 65 FO4 after place-and-route. Unfortunately, instruction per cycle (IPC) performance drops up to 20%, mostly due to the extra latency between load instructions and dependent instructions. However, the new BOOMv2 physical design paves the way for IPC recovery later.}
}

@phdthesis{waterman_design_2016,
	title = {Design of the {RISC}-{V} {Instruction} {Set} {Architecture}},
	url = {https://escholarship.org/uc/item/7zj0b3m7},
	abstract = {The hardware-software interface, embodied in the instruction set architecture (ISA), is arguably the most important interface in a computer system. Yet, in contrast to nearly all other interfaces in a modern computer system, all commercially popular ISAs are proprietary. A free and open ISA standard has the potential to increase innovation in microprocessor design, reduce computer system cost, and, as Moore’s law wanes, ease the transition to more specialized computational devices.In this dissertation, I present the RISC-V instruction set architecture. RISC-V is a free and open ISA that, with three decades of hindsight, builds and improves upon the original Reduced Instruction Set Computer (RISC) architectures. It is structured as a small base ISA with a variety of optional extensions. The base ISA is very simple, making RISC-V suitable for research and education, but complete enough to be a suitable ISA for inexpensive, low-power embedded devices. The optional extensions form a more powerful ISA for general-purpose and high-performance computing. I also present and evaluate a new RISC-V ISA extension for reduced code size, which makes RISC-V more compact than all popular 64-bit ISAs.},
	language = {en},

	school = {UC Berkeley},
	author = {Waterman, Andrew Shell},
	year = {2016},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/M6CFL7VX/Waterman - 2016 - Design of the RISC-V Instruction Set Architecture.pdf:application/pdf;Snapshot:/home/binaryman/Zotero/storage/9YNJCBW6/7zj0b3m7.html:text/html}
}

@misc{intel_xeon_E5,
	author = {Intel®},
	year = {2021},
	title = {Intel® {Xeon}® {Processor} {E5} {Family} {Product} {Specifications}},
	url = {https://ark.intel.com/content/www/us/en/ark/products/series/59138/intel-xeon-processor-e5-family.html},
	abstract = {Intel® Xeon® Processor E5 Family product listing with links to detailed product features and specifications.},
	language = {en},

	file = {Snapshot:/home/binaryman/Zotero/storage/YWK9HGMA/intel-xeon-processor-e5-family.html:text/html}
}
@INPROCEEDINGS{POWER8,
  author={Fluhr, Eric J. and Friedrich, Joshua and Dreps, Daniel and Zyuban, Victor and Still, Gregory and Gonzalez, Christopher and Hall, Allen and Hogenmiller, David and Malgioglio, Frank and Nett, Ryan and Paredes, Jose and Pille, Juergen and Plass, Donald and Puri, Ruchir and Restle, Phillip and Shan, David and Stawiasz, Kevin and Deniz, Zeynep Toprak and Wendel, Dieter and Ziegler, Matt},
  booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  title={5.1 POWER8TM: A 12-core server-class processor in 22nm SOI with 7.6Tb/s off-chip bandwidth},
  year={2014},
  volume={},
  number={},
  pages={96-97},
  doi={10.1109/ISSCC.2014.6757353}}

@misc{morgan_power9_2017,
	title = {Power9 {To} {The} {People}},
	url = {https://www.nextplatform.com/2017/12/05/power9-to-the-people/},
	abstract = {The server race is really afoot now that IBM has finally gotten off the starting blocks with its first Power9 system, based on its “Nimbus” variant of},
	language = {en-US},

	journal = {The Next Platform},
	author = {Morgan, Timothy Prickett},
	month = dec,
	year = {2017},
	file = {Snapshot:/home/binaryman/Zotero/storage/PLX5Y74B/power9-to-the-people.html:text/html}
}

@article{stuecheli_capi_2015,
	title = {{CAPI}: {A} {Coherent} {Accelerator} {Processor} {Interface}},
	volume = {59},
	issn = {0018-8646},
	shorttitle = {{CAPI}},
	doi = {10.1147/JRD.2014.2380198},
	abstract = {Heterogeneous computing systems combine different types of compute elements that share memory. A specific class of heterogeneous systems discussed in this paper pairs traditional general-purpose processing cores and accelerator units. While this arrangement enables significant gains in application performance, device driver overheads and operating system code path overheads can become prohibitive. The I/O interface of a processor chip is a well-suited attachment point from a system design perspective, in that standard server models can be augmented with application-specific accelerators. However, traditional I/O attachment protocols introduce significant device driver and operating system software latencies. With the Coherent Accelerator Processor Interface (CAPI), we enable attaching an accelerator as a coherent CPU peer over the I/O physical interface. The CPU peer features consist of a homogeneous virtual address space across the CPU and accelerator, and hardware-managed caching of this shared data on the I/O device. This attachment method greatly increases the opportunities for acceleration due to the much shorter software path length required to enable its use compared to a traditional I/O model.},
	number = {1},
	journal = {IBM Journal of Research and Development},
	author = {Stuecheli, J. and Blaner, B. and Johns, C. R. and Siegel, M. S.},
	month = jan,
	year = {2015},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Field programmable gate arrays, Harmonic analysis, Heterogeneous computing systems, High performance computing, Memory management, Operating systems, Performance evaluation, System analysis and design},
	pages = {7:1--7:7},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/682KRW33/7029171.html:text/html}
}

@misc{morgan_opening_2016,
	title = {Opening {Up} {The} {Server} {Bus} {For} {Coherent} {Acceleration}},
	url = {https://www.nextplatform.com/2016/10/17/opening-server-bus-coherent-acceleration/},
	abstract = {When IBM started to use the word "open" in conjunction with its Power architecture more than three years with the formation of the OpenPower Foundation},
	language = {en-US},

	journal = {The Next Platform},
	author = {Morgan, Timothy Prickett},
	month = oct,
	year = {2016},
	file = {Snapshot:/home/binaryman/Zotero/storage/P7W97URB/opening-server-bus-coherent-acceleration.html:text/html}
}

@article{stuecheli_ibm_2018,
	title = {{IBM} {POWER9} opens up a new era of acceleration enablement: {OpenCAPI}},
	volume = {62},
	issn = {0018-8646},
	shorttitle = {{IBM} {POWER9} opens up a new era of acceleration enablement},
	doi = {10.1147/JRD.2018.2856978},
	abstract = {Open Coherent Accelerator Processor Interface (OpenCAPI) is a new industry-standard device interface that enables the development of host-agnostic devices that can coherently connect to any host platform that supports the OpenCAPI standard. This in turn allows such devices to coherently cache host memory to facilitate accelerator execution, perform direct memory access and atomics to host memory, send messages and interrupts to the host, and act as a host memory home agent. OpenCAPI utilizes high-frequency differential signaling technology while providing the high bandwidth and low latency needed by advanced accelerators. OpenCAPI encapsulates the serializing cache access and address translation constructs in high-speed host silicon technology to minimize overhead and design complexity in attached silicon such as field-programmable gate arrays and application-specific integrated circuits. Finally, OpenCAPI architecturally ties together transaction layer, link layer, and physical layer attributes to optimally align to high serializer/deserializer (SerDes) ratios and enable high-bandwidth, highly parallel exploitation of attached silicon.},
	number = {4/5},
	journal = {IBM Journal of Research and Development},
	author = {Stuecheli, J. and Starke, W. J. and Irish, J. D. and Arimilli, L. B. and Dreps, D. and Blaner, B. and Wollbrink, C. and Allison, B.},
	month = jul,
	year = {2018},
	note = {Conference Name: IBM Journal of Research and Development},
	keywords = {Central Processing Unit, Coherence, Field programmable gate arrays, Performance evaluation, Protocols, Random access memory, Standards},
	pages = {8:1--8:8},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/QKPIRKPF/8413085.html:text/html}
}

@misc{adm_pcie_9h7,
	author = {AlphaData},
	year = {2021},
	title = {{ADM}{PCIE}-{9H7} {\textbar} {Alpha} {Data}},
	url = {https://www.alpha-data.com/product/},
	language = {en-GB},

	file = {Snapshot:/home/binaryman/Zotero/storage/TLVSMCRC/adm-pcie-9h7.html:text/html}
}

@article{hrica_floating-point_2012,
	title = {Floating-{Point} {Design} with {Vivado} {HLS}},
	language = {en},
	author = {Hrica, James},
	year = {2012},
	pages = {13},
	file = {Hrica - 2012 - Floating-Point Design with Vivado HLS.pdf:/home/binaryman/Zotero/storage/DZ3R66XJ/Hrica - 2012 - Floating-Point Design with Vivado HLS.pdf:application/pdf}
}

@inproceedings{bachrach_chisel_2012,
	address = {New York, NY, USA},
	series = {{DAC} '12},
	title = {Chisel: constructing hardware in a {Scala} embedded language},
	isbn = {978-1-4503-1199-1},
	shorttitle = {Chisel},
	url = {https://doi.org/10.1145/2228360.2228584},
	doi = {10.1145/2228360.2228584},
	abstract = {In this paper we introduce Chisel, a new hardware construction language that supports advanced hardware design using highly parameterized generators and layered domain-specific hardware languages. By embedding Chisel in the Scala programming language, we raise the level of hardware design abstraction by providing concepts including object orientation, functional programming, parameterized types, and type inference. Chisel can generate a high-speed C++-based cycle-accurate software simulator, or low-level Verilog designed to map to either FPGAs or to a standard ASIC flow for synthesis. This paper presents Chisel, its embedding in Scala, hardware examples, and results for C++ simulation, Verilog emulation and ASIC synthesis.},

	booktitle = {Proceedings of the 49th {Annual} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Bachrach, Jonathan and Vo, Huy and Richards, Brian and Lee, Yunsup and Waterman, Andrew and Avižienis, Rimas and Wawrzynek, John and Asanović, Krste},
	month = jun,
	year = {2012},
	keywords = {CAD},
	pages = {1216--1225}
}

@article{gupta_accelerating_2016,
	title = {Accelerating {Datacenter} {Workloads}},
	year = {2016},
	language = {en},
	author = {Gupta, PK},
	pages = {27},
	file = {Gupta - Accelerating Datacenter Workloads.pdf:/home/binaryman/Zotero/storage/CPZAMBPM/Gupta - Accelerating Datacenter Workloads.pdf:application/pdf}
}
@unpublished{uguen_hls_2017,
  TITLE = {{High-Level Synthesis Using Application-Specific Arithmetic: A Case Study}},
  AUTHOR = {Uguen, Yohann and de Dinechin, Florent and Derrien, Steven},
  URL = {https://hal.archives-ouvertes.fr/hal-01502644},
  NOTE = {working paper or preprint},
  YEAR = {2017},
  MONTH = Apr,
  PDF = {https://hal.archives-ouvertes.fr/hal-01502644/file/HLS-Using-App-Specific-Arith_A-Case-Study.pdf},
}
@phdthesis{uguen:tel-02420901,
  TITLE = {{High-level synthesis and arithmetic optimizations}},
  AUTHOR = {Uguen, Yohann},
  URL = {https://hal.archives-ouvertes.fr/tel-02420901},
  SCHOOL = {{INSA Lyon}},
  YEAR = {2019},
  MONTH = Nov,
  KEYWORDS = {Computer arithmetics ; Arithm{\'e}tique des ordinateurs},
  TYPE = {Theses},
  PDF = {https://hal.archives-ouvertes.fr/tel-02420901v1/file/manuscript.pdf},
  HAL_ID = {tel-02420901},
  HAL_VERSION = {v1},
}

@article{choquette_nvidia_2021,
	title = {{NVIDIA} {A100} {Tensor} {Core} {GPU}: {Performance} and {Innovation}},
	volume = {41},
	issn = {1937-4143},
	shorttitle = {{NVIDIA} {A100} {Tensor} {Core} {GPU}},
	doi = {10.1109/MM.2021.3061394},
	abstract = {NVIDIA A100 Tensor Core GPU is NVIDIA's latest flagship GPU. It has been designed with many new innovative features to provide performance and capabilities for HPC, AI, and data analytics workloads. Feature enhancements include a Third-Generation Tensor Core, new asynchronous data movement and programming model, enhanced L2 cache, HBM2 DRAM, and third-generation NVIDIA NVLink I/O.},
	number = {2},
	journal = {IEEE Micro},
	author = {Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
	month = mar,
	year = {2021},
	note = {Conference Name: IEEE Micro},
	keywords = {A100, Artificial intelligence, Bandwidth, Benchmark testing, C++20, CUDA, Deep Learning, GPU, Graphics processing units, NVLink, Parallel processing, Tensor Core, Tensors, Throughput},
	pages = {29--35},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/G92XZZSJ/9361255.html:text/html}
}

@article{hida_library_2007,
	title = {Library for {Double}-{Double} and {Quad}-{Double} {Arithmetic}},
	year = {2007},
	abstract = {A double-double number is an unevaluated sum of two IEEE double precision numbers, capable of representing at least 106 bits of signiﬁcand. Similarly, a quad-double number is an unevaluated sum of four IEEE double precision numbers, capable of representing at least 212 bits of signiﬁcand. Algorithms for various arithmetic operations (including the four basic operations and various algebraic and transcendental operations) are presented. A C++ implementation of these algorithms is also described, along with its C and Fortran interfaces. Performance of the library is also discussed.},
	language = {en},
	author = {Hida, Yozo and Li, Xiaoye S and Bailey, David H},
	pages = {24},
	file = {Hida et al. - Library for Double-Double and Quad-Double Arithmet.pdf:/home/binaryman/Zotero/storage/PIZDR6LS/Hida et al. - Library for Double-Double and Quad-Double Arithmet.pdf:application/pdf}
}

@inproceedings{bailey_high-precision_2009,
	address = {Erice, Italy},
	title = {High-{Precision} {Computation} and {Mathematical} {Physics}},
	url = {https://pos.sissa.it/070/014},
	doi = {10.22323/1.070.0014},
	language = {en},

	booktitle = {Proceedings of {XII} {Advanced} {Computing} and {Analysis} {Techniques} in {Physics} {Research} — {PoS}({ACAT08})},
	publisher = {Sissa Medialab},
	author = {Bailey, David and Borwein, Jonathan M},
	month = oct,
	year = {2009},
	pages = {014},
	file = {Bailey and Borwein - 2009 - High-Precision Computation and Mathematical Physic.pdf:/home/binaryman/Zotero/storage/HM87BXGM/Bailey and Borwein - 2009 - High-Precision Computation and Mathematical Physic.pdf:application/pdf}
}

@article{bailey_integrals_2006,
	title = {Integrals of the {Ising} class},
	volume = {39},
	issn = {0305-4470, 1361-6447},
	url = {https://iopscience.iop.org/article/10.1088/0305-4470/39/40/001},
	doi = {10.1088/0305-4470/39/40/001},
	language = {en},
	number = {40},

	journal = {Journal of Physics A: Mathematical and General},
	author = {Bailey, D H and Borwein, J M and Crandall, R E},
	month = oct,
	year = {2006},
	pages = {12271--12302},
	file = {Bailey et al. - 2006 - Integrals of the Ising class.pdf:/home/binaryman/Zotero/storage/5FF6IL2T/Bailey et al. - 2006 - Integrals of the Ising class.pdf:application/pdf}
}

@article{oloughlin_xilinx_2014,
	title = {Xilinx {Vivado} {High} {Level} {Synthesis}: {Case} studies},
	shorttitle = {Xilinx {Vivado} {High} {Level} {Synthesis}},
	url = {https://digital-library.theiet.org/content/conferences/10.1049/cp.2014.0713},
	doi = {10.1049/cp.2014.0713},
	abstract = {This paper presents case studies on the application of the Xilinx Vivado High Level Synthesis (HLS) tool-suite for C++-based design capture, simulation and synthesis to Hardware Description Language (HDL) format, and further to FPGA hardware implementation. HLS reduces the effort of HDL design capture and debug while allowing flexibility in the final hardware implementation in order to meet design constraints. HLS is not yet widely used. This paper demonstrates the practical steps in using HLS and the resulting hardware implementation. Case studies illustrate the effectiveness of HLS as a developing efficient and flexible design capture to FPGA implementation approach. The paper presents four HLS design examples, including a multiplexer, counter, register block and a skin detection image processing algorithm. Xilinx PlanAhead EDA tool-suite is used to generate a Xilinx Spartan-6 FPGA bitstream from the Xilinx Vivado HLS-synthesised HDL model. Each design has been implemented and tested in FPGA hardware using the Vicilogic automation and proto-typing tools developed by the authors. These tools automate the integration of designs with an FPGA IP core, which supports Ethernet I/O, SDRAM interface and a register-based I/O system. The Vicilogic Python client application environment enables GUI-based development and testing of the hardware implementation.},
	language = {en},
	author = {O'Loughlin, D. and Coffey, A. and Callaly, F. and Lyons, D. and Morgan, F.},
	month = jan,
	year = {2014},
	note = {Publisher: IET Digital Library},
	pages = {352--356},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{zaremba_recurrent_2015,
	title = {Recurrent {Neural} {Network} {Regularization}},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},

	journal = {arXiv:1409.2329 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = feb,
	year = {2015},
	note = {arXiv: 1409.2329},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/binaryman/Zotero/storage/LFNHXI9X/Zaremba et al. - 2015 - Recurrent Neural Network Regularization.pdf:application/pdf;arXiv.org Snapshot:/home/binaryman/Zotero/storage/FJ2G8SNL/1409.html:text/html}
}

@inproceedings{bkk20,
  author = {B{\"o}ttcher, Andreas and Kullmann, Keanu and Kumm, Martin},
  title = {{Heuristics for the Design of Large Multipliers for FPGAs}},
  booktitle = {IEEE Symposium on Computer Arithmetic (ARITH)},
  year = {2020}
}

@article{fousse_mpfr_2007,
	title = {{MPFR}: {A} multiple-precision binary floating-point library with correct rounding},
	volume = {33},
	issn = {0098-3500},
	shorttitle = {{MPFR}},
	url = {https://doi.org/10.1145/1236463.1236468},
	doi = {10.1145/1236463.1236468},
	abstract = {This article presents a multiple-precision binary floating-point library, written in the ISO C language, and based on the GNU MP library. Its particularity is to extend to arbitrary-precision, ideas from the IEEE 754 standard, by providing correct rounding and exceptions. We demonstrate how these strong semantics are achieved---with no significant slowdown with respect to other arbitrary-precision tools---and discuss a few applications where such a library can be useful.},
	number = {2},

	journal = {ACM Transactions on Mathematical Software},
	author = {Fousse, Laurent and Hanrot, Guillaume and Lefèvre, Vincent and Pélissier, Patrick and Zimmermann, Paul},
	month = jun,
	year = {2007},
	keywords = {correct rounding, elementary function, floating-point arithmetic, IEEE 754 standard, Multiple-precision arithmetic, portable software},
	pages = {13--es},
}

@inproceedings{coding_lindstrom_2018,
author = {Lindstrom, Peter and Lloyd, Scott and Hittinger, Jeffrey},
	title = {Universal Coding of the Reals: Alternatives to IEEE Floating Point},
	year = {2018},
	isbn = {9781450364140},
	publisher = {Association for Computing Machinery},
	url = {https://doi.org/10.1145/3190339.3190344},
	doi = {10.1145/3190339.3190344},
	abstract = {We propose a modular framework for representing the real numbers that generalizes ieee, posits, and related floating-point number systems, and which has its roots in universal codes for the positive integers such as the Elias codes. This framework unifies several known but seemingly unrelated representations within a single schema while also introducing new representations. We particularly focus on variable-length encoding of the binary exponent and on the manner in which fraction bits are mapped to values. Our framework builds upon and shares many of the attractive properties of posits but allows for independent experimentation with exponent codes, fraction mappings, reciprocal closure, rounding modes, handling of under- and overflow, and underlying precision.},
	booktitle = {Proceedings of the Conference for Next Generation Arithmetic},
	articleno = {5},
	numpages = {14},
	keywords = {posits, floating point, numerical algorithms, number representations, universal coding, tapered precision, roundoff error},
	location = {Singapore, Singapore},
	series = {CoNGA '18}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{newton_method_1736,
	title = {The {Method} of {Fluxions} and {Infinite} {Series}: {With} {Its} {Application} to the {Geometry} of {Curve}-lines. {By} ... {Sir} {Isaac} {Newton}, ... {Translated} from the {Author}'s {Latin} {Original} {Not} {Yet} {Made} {Publick}. {To} which is {Subjoin}'d, a {Perpetual} {Comment} {Upon} the {Whole} {Work}, ... {By} {John} {Colson}, ...},
	shorttitle = {The {Method} of {Fluxions} and {Infinite} {Series}},
	language = {en},
	publisher = {Henry Woodfall; and sold by John Nourse},
	author = {Newton, Isaac},
	year = {1736},
	note = {Google-Books-ID: WyQOAAAAQAAJ},
}

@inproceedings{lutz_arm_2019,
	title = {{ARM} {Floating} {Point} 2019: {Latency}, {Area}, {Power}},
	shorttitle = {{ARM} {Floating} {Point} 2019},
	doi = {10.1109/ARITH.2019.00025},
	abstract = {We have had little or no speed increase from process in the past few years, but latency continues to decrease due to algorithmic improvements [1] and a decision to spend more area on CPU datapaths [2]. A binary64 floating-point (FP) add now takes two cycles when done as part of a 2+2-cycle FMA, and even one cycle when done as part of an in-order vector reduction. Smaller and more specialized FP operations (bfloat16) are even faster. Finally, the decision to spend more area on datapath logic took a new twist this year when we applied it to GPUs, cutting dynamic power there by a third.},
	booktitle = {2019 {IEEE} 26th {Symposium} on {Computer} {Arithmetic} ({ARITH})},
	author = {Lutz, David},
	month = jun,
	year = {2019},
	note = {ISSN: 2576-2265},
	keywords = {Adders, Clocks, Digital arithmetic, Microarchitecture, Pipelines, Security, Standards},
	pages = {97--98}
}

@inproceedings{galal_latency_2011,
	title = {Latency {Sensitive} {FMA} {Design}},
	doi = {10.1109/ARITH.2011.26},
	abstract = {The implementation of merged floating-point multiply-add operations can be optimized in many ways. For latency sensitive applications, our cascade design reduces the accumulation dependent latency by 2x over a fused design, at a cost of a 13\% increase in non-accumulation dependent latency. A simple in-order execution model shows this design is superior in most applications, providing 12\% average reduction in FP stalls, and improves performance by up to 6\%. Simulations of superscalar out-of-order machines show 4\% average improvement in CPI in 2-way machines and 4.6\% in 4-way machines. The cascade design has the same area and energy budget as a traditional fused multiple-add FMA.},
	booktitle = {2011 {IEEE} 20th {Symposium} on {Computer} {Arithmetic}},
	author = {Galal, Sameh and Horowitz, Mark},
	month = jul,
	year = {2011},
	note = {ISSN: 1063-6889},
	keywords = {Adders, Benchmark testing, Clocks, Fused Multiply Add, Graphics processing unit, Out of order, Parallel processing, Pipelines},
	pages = {129--138},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/N7NJ7QYL/5992118.html:text/html},
}

@book{muller_handbook_2018,
	address = {Cham},
	title = {Handbook of {Floating}-{Point} {Arithmetic}},
	isbn = {978-3-319-76525-9 978-3-319-76526-6},
	url = {http://link.springer.com/10.1007/978-3-319-76526-6},
	language = {en},

	publisher = {Springer International Publishing},
	author = {Muller, Jean-Michel and Brunie, Nicolas and De Dinechin, Florent and Jeannerod, Claude-Pierre and Joldes, Mioara and Lefèvre, Vincent and Melquiond, Guillaume and Revol, Nathalie and Torres, Serge},
	year = {2018},
	doi = {10.1007/978-3-319-76526-6},
	keywords = {algorithm analysis and problem complexity, Elementary Functions, Fast2Sum and 2Sum Algorithms, Floating-Point Arithmetic, Floating-Point Operators, Fused Multiply-Add Instruction, IEEE 754-2008 Standard},
	file = {Full Text:/home/binaryman/Zotero/storage/74C25AVJ/Muller et al. - 2018 - Handbook of Floating-Point Arithmetic.pdf:application/pdf},
}

@inproceedings{lutz_optimized_2017,
	title = {Optimized leading zero anticipators for faster fused multiply-adds},
	doi = {10.1109/ACSSC.2017.8335443},
	abstract = {Leading zero anticipators (LZAs) predict the number of leading zeros in a difference, and have long been used to speed up floating-point add and fused-multiply add (FMA) operations. Typically the LZA prediction must be corrected for small exponents and possible carries, and only the corrected value is useful for normalizing the difference and computing rounding. We present new techniques to speed up LZA correction and rounding, allowing differences to be computed in only 60\% of their previous best latency.},
	booktitle = {2017 51st {Asilomar} {Conference} on {Signals}, {Systems}, and {Computers}},
	author = {Lutz, David R.},
	month = oct,
	year = {2017},
	note = {ISSN: 2576-2303},
	keywords = {Adders, Delays, Digital arithmetic, Inverters, Limiting, Logic gates},
	pages = {741--744},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/2PQBF9IM/8335443.html:text/html},
}

@inproceedings{huang_multiply_2007,
  author={Huang, Libo and Shen, Li and Dai, Kui and Wang, Zhiying},
  booktitle={18th IEEE Symposium on Computer Arithmetic (ARITH '07)},
  title={A New Architecture For Multiple-Precision Floating-Point Multiply-Add Fused Unit Design},
  year={2007},
  pages={69-76},
  doi={10.1109/ARITH.2007.5}
}

@article{even_dual_2000,
	title = {A dual precision {IEEE} floating-point multiplier},
	volume = {29},
	issn = {0167-9260},
	url = {https://doi.org/10.1016/S0167-9260(00)00006-7},
	doi = {10.1016/S0167-9260(00)00006-7},
	number = {2},

	journal = {Integr. VLSI J.},
	author = {Even, Guy and Mueller, Silvia M. and Seidel, Peter-Michael},
	month = sep,
	year = {2000},
	keywords = {floating point multiplier, IEEE floating point arithmetic, rounding},
	pages = {167--180},
}

@inproceedings{beaumont-smith_reduced_1999,
	address = {USA},
	series = {{ARITH} '99},
	title = {Reduced {Latency} {IEEE} {Floating}-{Point} {Standard} {Adder} {Architectures}},
	isbn = {978-0-7695-0116-1},
	abstract = {The design and implementation of a double precision floating-point IEEE-754 standard adder is described which uses "flagged prefix addition" to merge rounding with the significand addition. The floating-point adder is implemented in 0.5um CMOS, measures 1.8mm{\textasciicircum}2, has a 3-cycle latency and implements all rounding modes. A modified version of this floating-point adder can perform accumulation in 2-cycles with a small amount of extra hardware for use in a parallel processor node. This is achieved by feeding back the previous un-normalised but correctly rounded result together with the normalisation distance. A 2-cycle latency floating-point adder architecture with potentially the same cycle time that also employs flagged prefix addition is described. It also incorporates a fast prediction scheme for the true subtraction of significands with an exponent difference of 1, with one less adder.},

	booktitle = {Proceedings of the 14th {IEEE} {Symposium} on {Computer} {Arithmetic}},
	publisher = {IEEE Computer Society},
	author = {Beaumont-Smith, A. and Burgess, N. and Lefrere, S. and Lim, C. C.},
	month = apr,
	year = {1999},
	keywords = {adder, arithmetic, floating-point, VLSI.},
	pages = {35},
}

@inproceedings{shafique_eda_2014,
	address = {New York, NY, USA},
	series = {{DAC} '14},
	title = {The {EDA} {Challenges} in the {Dark} {Silicon} {Era}: {Temperature}, {Reliability}, and {Variability} {Perspectives}},
	isbn = {978-1-4503-2730-5},
	shorttitle = {The {EDA} {Challenges} in the {Dark} {Silicon} {Era}},
	url = {https://doi.org/10.1145/2593069.2593229},
	doi = {10.1145/2593069.2593229},
	abstract = {Technology scaling has resulted in smaller and faster transistors in successive technology generations. However, transistor power consumption no longer scales commensurately with integration density and, consequently, it is projected that in future technology nodes it will only be possible to simultaneously power on a fraction of cores on a multi-core chip in order to stay within the power budget. The part of the chip that is powered off is referred to as dark silicon and brings new challenges as well as opportunities for the design community, particularly in the context of the interaction of dark silicon with thermal, reliability and variability concerns. In this perspectives paper we describe these new challenges and opportunities, and provide preliminary experimental evidence in their support.},

	booktitle = {Proceedings of the 51st {Annual} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Shafique, Muhammad and Garg, Siddharth and Henkel, Jörg and Marculescu, Diana},
	month = jun,
	year = {2014},
	pages = {1--6},
}

@article{dennard_design_1999,
	title = {Design {Of} {Ion}-implanted {MOSFET}'s with {Very} {Small} {Physical} {Dimensions}},
	volume = {87},
	issn = {1558-2256},
	doi = {10.1109/JPROC.1999.752522},
	journal = {Proceedings of the IEEE},
	author = {Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and Leblanc, A.R.},
	year = {1999},
	note = {Conference Name: Proceedings of the IEEE},
	pages = {668--678}
}

@article{dennard_design_1974,
	title = {Design of ion-implanted {MOSFET}'s with very small physical dimensions},
	volume = {9},
	issn = {1558-173X},
	doi = {10.1109/JSSC.1974.1050511},
	abstract = {This paper considers the design, fabrication, and characterization of very small Mosfet switching devices suitable for digital integrated circuits, using dimensions of the order of 1 /spl mu/. Scaling relationships are presented which show how a conventional MOSFET can be reduced in size. An improved small device structure is presented that uses ion implantation, to provide shallow source and drain regions and a nonuniform substrate doping profile. One-dimensional models are used to predict the substrate doping profile and the corresponding threshold voltage versus source voltage characteristic. A two-dimensional current transport model is used to predict the relative degree of short-channel effects for different device parameter combinations. Polysilicon-gate MOSFET's with channel lengths as short as 0.5 /spl mu/ were fabricated, and the device characteristics measured and compared with predicted values. The performance improvement expected from using these very small devices in highly miniaturized integrated circuits is projected.},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and LeBlanc, A.R.},
	year = {1974},
	note = {Conference Name: IEEE Journal of Solid-State Circuits},
	pages = {256--268}
}

@article{moore_cramming_1998,
	title = {Cramming {More} {Components} {Onto} {Integrated} {Circuits}},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/JPROC.1998.658762},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Moore, G.E.},
	month = jan,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Aerospace electronics, Costs, Home computing, Integrated circuit reliability, Integrated circuit technology, Portable computers, Semiconductor films, Space technology, Switches, Telephony},
	pages = {82--85},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/KB2ZFX7K/658762.html:text/html},
}

@misc{peckham_marenostrum_2023,
	title = {{MareNostrum} 5 {Installation} {Begins}},
	url = {https://www.hpcwire.com/2023/02/27/marenostrum-5-installation-begins/},
	abstract = {Nearly four years ago, EuroHPC announced sites for eight new supercomputers as the nascent initiative debuted. MareNostrum 5, the third of the so-called “pre-exascale” EuroHPC systems, is set to round […]},
	language = {en-US},

	journal = {HPCwire},
	author = {Peckham, Oliver},
	month = feb,
	year = {2023},
	file = {Snapshot:/home/binaryman/Zotero/storage/6MYB8FTA/marenostrum-5-installation-begins.html:text/html},
}

@article{gustafson_beating_2017,
	title = {Beating {Floating} {Point} at its {Own} {Game}: {Posit} {Arithmetic}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {2313-8734},
	shorttitle = {Beating {Floating} {Point} at its {Own} {Game}},
	url = {https://superfri.org/index.php/superfri/article/view/137},
	doi = {10.14529/jsfi170206},
	abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and “Not-a-Number” (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than “approximate computing” methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.},
	language = {en},
	number = {2},

	journal = {Supercomputing Frontiers and Innovations},
	author = {Gustafson, John L. and Yonemoto, Isaac T.},
	month = apr,
	year = {2017},
	note = {Number: 2},
	pages = {71--86},
}

@inproceedings{ledoux_generator_2022,
	title = {A {Generator} of {Numerically}-{Tailored} and {High}-{Throughput} {Accelerators} for {Batched} {GEMMs}},
	doi = {10.1109/FCCM53951.2022.9786164},
	abstract = {We propose a hardware generator of GEMM accelerators. Our generator produces vendor-agnostic HDL describing highly customizable systolic arrays guided by accuracy and energy efficiency goals. The generated arrays have three main novel aspects. First, the accelerators handle a large variety of computer number formats using intermediate representations based on our Sign Scale Significand (S3) format. Second, the processing elements perform all intermediate dot-product arithmetic operations required by the GEMM kernel without any intermediate rounding, which makes it possible to deliver better energy efficiency than state-of-the-art approaches while offering more accuracy and reproducible results. Third, our accelerators feature the Half-Speed Sink Down (HSSD) mechanism, which maximizes the overlap of host-accelerator data transfers with GEMM computations.We evaluate our automatically generated designs in a cutting-edge setup composed of a POWER9 host, CAPI (Coherent Accelerator Processor Interface) link, and a Virtex Ultrascale Plus FPGA. Arrays can operate at the speed of the link and saturate it to reach a 13GB/s throughput. Our fine-grain customization approach allows to cover a wide range of accuracy versus efficiency scenarios and can reach 0.65GOps/s/W while producing 1024 accurate bits or 148.7GOps/s/W with 6 accurate bits. Our configurations achieve up to 1613GOps/s system performance and power efficiencies of up to 240GOps/s/W for the FPGA. This automatic generator is the first being able to produce such a variety of designs. We improve the single-precision energy efficiency of state-of-the-art FPGA GEMM accelerators by 1.86×.},
	booktitle = {2022 {IEEE} 30th {Annual} {International} {Symposium} on {Field}-{Programmable} {Custom} {Computing} {Machines} ({FCCM})},
	author = {Ledoux, Louis and Casas, Marc},
	month = may,
	year = {2022},
	note = {ISSN: 2576-2621},
	keywords = {Energy efficiency, Generators, Hardware, Space exploration, System performance, Systolic arrays, Throughput},
	pages = {1--10},
}


@inproceedings{ledoux_framework_2023,
	title = {An {Open}-{Source} {Framework} for {Efficient} {Numerically}-{Tailored} {Computations}},
	url = {https://ieeexplore.ieee.org/document/10296314},
	doi = {10.1109/FPL60245.2023.00011},
	abstract = {We present a versatile open-source framework designed to facilitate efficient, numerically-tailored Matrix-Matrix Multiplications (MMMs). The framework offers two primary contributions: first, a fine-tuned, automated pipeline for arithmetic datapath generation, enabling highly customizable systolic MMM kernels; second, seamless integration of the generated kernels into user code, irrespective of the programming language employed, without necessitating modifications. We employ this framework within a cutting-edge platform, comprising a Power9 host, an OpenCAPI link, and a Xilinx Virtex UltraScale+ FPGA. The framework demonstrates a systematic enhancement in accuracy per energy cost across diverse High Performance Computing (HPC) workloads displaying a variety of numerical requirements, such as Artificial Intelligence (AI) inference and Sea Surface Height (SSH) computation. For AI inference, we consider a set of state-of-the-art neural network models, namely ResNetl8, ResNet34, ResNet50, DenseNet121, DenseNet161, DenseNet169, and VGG11, in conjunction with two datasets, two computer formats, and 27 distinct intermediate arithmetic datapaths. Our approach consistently reduces energy consumption across all cases, with a notable example being the reduction by factors of 3.3x for IEEE754-32 and 1.4x for Bfloat16 during ImageNet inference with ResNet50. This is accomplished while maintaining accuracies of 82.3\% and 86\%, comparable to those achieved with conventional Floating-Point Units (FPUs). In the context of SSH computation, our method achieves fully-reproducible results using double-precision words, surpassing the accuracy of conventional double- and quad-precision arithmetic in FPUs. Our approach enhances SSH computation accuracy by a minimum of 5× and 27× compared to IEEE754-64 and IEEE754-128, respectively, resulting in 5.6× and 15.1 × improvements in accuracy per power cost.},
	booktitle = {2023 33rd {International} {Conference} on {Field}-{Programmable} {Logic} and {Applications} ({FPL})},
	author = {Ledoux, Louis and Casas, Marc},
	month = sep,
	year = {2023},
	note = {ISSN: 1946-1488},
	pages = {19--26},
}

@misc{ledoux_open-source_2023,
	title = {An {Open}-{Source} {Framework} for {Efficient} {Numerically}-{Tailored} {Computations}},
	url = {https://hal.science/hal-04094835},
	abstract = {Many scientific computing problems can be reduced to Matrix-Matrix Multiplications (MMM), making the General Matrix Multiply (GEMM) kernels in the Basic Linear Algebra Subroutine (BLAS) of interest to the high-performance computing community.
However, these workloads have a wide range of numerical requirements.
Ill-conditioned linear systems require high-precision arithmetic to ensure correct and reproducible results.
In contrast, emerging workloads such as deep neural networks, which can have millions up to billions of parameters, have shown resilience to arithmetic tinkering and precision lowering.},
	author = {Ledoux, Louis and Casas, Marc},
	month = may,
	year = {2023},
	note = {Publisher: Barcelona Supercomputing Center
Published: BSCSymposium23},
	keywords = {approximate/trans/extended precision, automated pipeline, flopoco, full stack framework, GEMMs, High Performance Computing, matrix-matrix-multiply, OpenBLAS, OpenCAPI},
}

@misc{ledoux2023opensource,
      title={Open-Source GEMM Hardware Kernels Generator: Toward Numerically-Tailored Computations},
      author={Louis Ledoux and Marc Casas},
      year={2023},
      eprint={2305.18328},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@inproceedings{ledoux_accelerating_2019,
	address = {Lyon, France},
	title = {Accelerating {DL} inference with ({Open}){CAPI} and posit numbers},
	url = {https://hal.science/hal-04094850},
	abstract = {In order to tackle the communication-bound problems in heterogeneous systems, Louis discussed how intrinsic arithmetics can have significant impacts on energy and throughputs.

Thanks to the denser representation offered by the posit arithmetic BSC can:
- significantly divide bandwidth needed (PCIE: HOST - FPGA)
- maintain all weights on chip
- save power consumption},
	booktitle = {{OpenPOWER} summit 2019},
	publisher = {linux foundation},
	author = {Ledoux, Louis and Casas, Marc},
	month = oct,
	year = {2019},
	keywords = {acceleration, CAPI, CAPI2, FPGA, PCIE, posit, POWER9}
}

@book{boole_mathematical_2011,
	title = {The {Mathematical} {Analysis} of {LogicBeing} an {Essay} {Towards} a {Calculus} of {Deductive} {Reasoning}},
	copyright = {Public domain in the USA.},
	url = {https://www.gutenberg.org/ebooks/36884},
	language = {English},
	author = {Boole, George},
	month = jul,
	year = {2011},
	keywords = {Logic},
	annote = {LoC Class BC: Philosophy, Psychology, Religion: Logic},
}

@book{boole_mathematical_1847,
	title = {The {Mathematical} {Analysis} of {Logic}},
	isbn = {978-0-8022-0154-6},
	language = {en},
	publisher = {Philosophical Library},
	author = {Boole, George},
	year = {1847},
	note = {Google-Books-ID: zv4YAQAAIAAJ},
}

@misc{courbariaux_binarized_2016,
      title={Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
      author={Matthieu Courbariaux and Itay Hubara and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
      year={2016},
      eprint={1602.02830},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{micikevicius_mixed_2017,
abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
archivePrefix = {arXiv},
arxivId = {1710.03740},
author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
eprint = {1710.03740},
mendeley-groups = {doctoradoBSC},
pages = {1--12},
title = {{Mixed Precision Training}},
url = {http://arxiv.org/abs/1710.03740},
year = {2017}
}

@techreport{micikevicius_fp8_2022,
	title = {{FP8} {Formats} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2209.05433},
	abstract = {FP8 is a natural progression for accelerating deep learning training inference beyond the 16-bit formats common in modern processors. In this paper we propose an 8-bit ﬂoating point (FP8) binary interchange format consisting of two encodings - E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). While E5M2 follows IEEE 754 conventions for representatio of special values, E4M3’s dynamic range is extended by not representing inﬁnities and having only one mantissa bit-pattern for NaNs. We demonstrate the efﬁcacy of the FP8 format on a variety of image and language tasks, effectively matching the result quality achieved by 16-bit training sessions. Our study covers the main modern neural network architectures - CNNs, RNNs, and Transformer-based models, leaving all the hyperparameters unchanged from the 16-bit baseline training sessions. Our training experiments include large, up to 175B parameter, language models. We also examine FP8 post-training-quantization of language models trained using 16-bit formats that resisted ﬁxed point int8 quantization.},
	language = {en},
	number = {arXiv:2209.05433},
	institution = {arXiv},
	author = {Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and Mellempudi, Naveen and Oberman, Stuart and Shoeybi, Mohammad and Siu, Michael and Wu, Hao},
	month = sep,
	year = {2022},
	note = {arXiv:2209.05433 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@book{mallasen_quintana_percival_2021,
	title = {{PERCIVAL}: {Open}-{Source} {Posit} {RISC}-{V} {Core} with {Quire} {Capability}},
	shorttitle = {{PERCIVAL}},
	abstract = {The posit representation for real numbers is an alternative to the ubiquitous IEEE 754 floating-point standard. In this work, we present PERCIVAL, an application-level posit capable RISC-V core based on CVA6 that can execute all posit instructions, including the quire fused operations. This solves the obstacle encountered by previous works, which only included partial posit support or which had to emulate posits in software, thus limiting the scope or the scalability of their applications. In addition, Xposit, a RISC-V extension for posit instructions is incorporated into LLVM. Therefore, PERCIVAL is the first work that integrates the complete posit instruction set in hardware. These elements allow for the native execution of posit instructions as well as the standard floating-point ones, further permitting the comparison of these representations. FPGA and ASIC synthesis show the hardware cost of implementing 32-bit posits and highlight the significant overhead of including a quire accumulator. However, results comparing posits and IEEE floats show that the quire enables a more accurate execution of dot products. In general matrix multiplications, the accuracy error is reduced up to 4 orders of magnitude when compared with single-precision floats. Furthermore, performance comparisons show that these accuracy improvements do not hinder their execution, as posits run as fast as single-precision floats and exhibit better timing than double-precision floats, thus potentially providing an alternative representation.},
	author = {Mallasén Quintana, David and Murillo, Raul and Del Barrio, Alberto and Botella, Guillermo and Pinuel, Luis and Prieto, Manuel},
	month = nov,
	year = {2021},
}


@misc{poucher_polybenchc_2010,
	title = {{PolyBench}/{C} -- {Homepage} of {Louis}-{Noël} {Pouchet}},
	author = {Louis-Noel Pouchet},
	url = {https://web.cse.ohio-state.edu/~pouchet.2/software/polybench/},
	year = {2010}
}

@misc{mallasen_bigpercival_2023,
      title={Big-PERCIVAL: Exploring the Native Use of 64-Bit Posit Arithmetic in Scientific Computing},
      author={David Mallasén and Alberto A. Del Barrio and Manuel Prieto-Matias},
      year={2023},
      eprint={2305.06946},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@inproceedings{murillo_energy-efficient_2021,
	title = {Energy-{Efficient} {MAC} {Units} for {Fused} {Posit} {Arithmetic}},
	url = {https://ieeexplore.ieee.org/document/9643752},
	doi = {10.1109/ICCD53106.2021.00032},
	abstract = {Posit arithmetic is an alternative format to the standard IEEE 754 for floating-point numbers that claims to provide compelling advantages over floats, including higher accuracy, larger dynamic range, or bitwise compatibility across systems. The interest in the design of arithmetic units for this novel format has increased in the last few years. However, while multiple designs for posit adder and multiplier have been developed recently in the literature, fused units for posit arithmetic are still in the early stages of research. Moreover, due to the large size of accumulators needed in fused operations, the few fused posit units proposed so far still require many hardware resources. In order to contribute to the development of the posit number format, and facilitate its use in applications such as deep learning, this paper presents several designs of energy-efficient posit multiply- accumulate (MAC) units with support for standard quire format. Concretely, the proposed designs are capable of computing fused dot products of large vectors without accuracy drop, while consuming less energy than previous implementations. Experiments show that, compared to previous implementations, the proposed designs consume up to 75.49\%, 88.45\% and 83.43\% less energy and are 73.18\%, 87.36\% and 83.00\% faster for 8, 16 and 32 bitwidths, with an additional area of only 4.97\%, 7.44\% and 4.24\%, respectively.},
	booktitle = {2021 {IEEE} 39th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Murillo, Raul and Mallasén, David and Del Barrio, Alberto A. and Botella, Guillermo},
	month = oct,
	year = {2021},
	note = {ISSN: 2576-6996},
	pages = {138--145},
}


@inproceedings{murillo_customized_2020,
	title = {Customized {Posit} {Adders} and {Multipliers} using the {FloPoCo} {Core} {Generator}},
	url = {https://ieeexplore.ieee.org/document/9180771},
	doi = {10.1109/ISCAS45731.2020.9180771},
	abstract = {The posit number system, which is proposed as a replacement of IEEE floating-point numbers, is in the spotlight of Arithmetic research due to the recent breakthroughs. This format claims to provide more accurate results with the same bitwidth than standard floating point, but the run-time variability during the detection of the posit fields involves a hardware design challenge. In this work, we propose parameterized designs for multiple posit functional units, including addition and multiplication, and integrate them as templates of the FloPoCo framework. The integration of the proposed algorithms within FloPoCo can provide synthesizable VHDL code for posit arithmetic of any possible configuration 〈n, es〉. Experiments show an improvement in terms of area and energy with respect to state-of-the-art works up to 35.9\% and 30.8\%, respectively.},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Murillo, Raul and Del Barrio, Alberto A. and Botella, Guillermo},
	month = oct,
	year = {2020},
	note = {ISSN: 2158-1525},
	pages = {1--5},
}
@article{murillo_plam_2022,
	title = {{PLAM}: {A} {Posit} {Logarithm}-{Approximate} {Multiplier}},
	volume = {10},
	issn = {2168-6750},
	shorttitle = {{PLAM}},
	url = {https://ieeexplore.ieee.org/document/9530365},
	doi = {10.1109/TETC.2021.3109127},
	abstract = {The Posit™ Number System was introduced in 2017 as a replacement for floating-point numbers. Since then, the community has explored its application in several areas, such as deep learning, and produced some unit designs which are still far from being competitive with their floating-point counterparts. This article proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to significantly reduce the complexity of posit multipliers, one of the most power-hungry arithmetic units. The impact of this approach is evaluated in deep neural network inference, where there are no significant accuracy drops. Compared with state-of-the-art posit multipliers, experiments show that the proposed technique reduces the area, power, and delay of 32-bit hardware multipliers up to 72.86\%, 81.79\%, and 17.01\%, respectively.},
	number = {4},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Murillo, Raul and Del Barrio, Alberto A. and Botella, Guillermo and Kim, Min Soo and Kim, HyunJin and Bagherzadeh, Nader},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Emerging Topics in Computing},
	pages = {2079--2085},
}

@Article{harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@inproceedings{xianyi_model-driven_2012,
	title = {Model-driven {Level} 3 {BLAS} {Performance} {Optimization} on {Loongson} {3A} {Processor}},
	doi = {10.1109/ICPADS.2012.97},
	abstract = {Every mainstream processor vendor provides an optimized BLAS implementation for its CPU, as BLAS is a fundamental math library in scientific computing. The Loongson 3A CPU is a general-purpose 64-bit MIPS64 quad-core processor, developed by the Institute of Computing Technology, Chinese Academy of Sciences. To date, there has not been a sufficiently optimized BLAS on the Loongson 3A CPU. The purpose of this research is to optimize level 3 BLAS performance on the Loongson 3A CPU. We analyzed the Loongson 3A architecture and built a performance model to highlight the key point, L1 data cache misses, which is different from level 3 BLAS optimization on the mainstream x86 CPU. Therefore, we employed a variety of methods to avoid L1 cache misses in single thread optimization, including cache and register blocking, the Loongson 3A 128-bit memory accessing extension instructions, software prefetching, and single precision floating-point SIMD instructions. Furthermore, we improved parallel performance by reducing bank conflicts among multiple threads in the shared L2 cache. We created an open source BLAS project, OpenBLAS, to demonstrate the performance improvement on the Loongson 3A quad-core processor.},
	booktitle = {2012 {IEEE} 18th {International} {Conference} on {Parallel} and {Distributed} {Systems}},
	author = {Xianyi, Zhang and Qian, Wang and Yunquan, Zhang},
	month = dec,
	year = {2012},
	note = {ISSN: 1521-9097},
	keywords = {BLAS, Kernel, Loongson 3A, MIPS64, Multi-core, Optimization, Pipelines, Prefetching, Registers},
	pages = {684--691},
}

@inproceedings{wang_augem_2013,
	title = {{AUGEM}: {Automatically} generate high performance {Dense} {Linear} {Algebra} kernels on x86 {CPUs}},
	shorttitle = {{AUGEM}},
	doi = {10.1145/2503210.2503219},
	abstract = {Basic Liner algebra subprograms (BLAS) is a fundamental library in scientific computing. In this paper, we present a template-based optimization framework, AUGEM, which can automatically generate fully optimized assembly code for several dense linear algebra (DLA) kernels, such as GEMM, GEMV, AXPY and DOT, on varying multi-core CPUs without requiring any manual interference from developers. In particular, based on domain-specific knowledge about algorithms of the DLA kernels, we use a collection of parameterized code templates to formulate a number of commonly occurring instruction sequences within the optimized low-level C code of these DLA kernels. Then, our framework uses a specialized low-level C optimizer to identify instruction sequences that match the pre-defined code templates and thereby translates them into extremely efficient SSE/AVX instructions. The DLA kernels generated by our templatebased approach surpass the implementations of Intel MKL and AMD ACML BLAS libraries, on both Intel Sandy Bridge and AMD Piledriver processors.},
	booktitle = {{SC} '13: {Proceedings} of the {International} {Conference} on {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Wang, Qian and Zhang, Xianyi and Zhang, Yunquan and Yi, Qing},
	month = nov,
	year = {2013},
	note = {ISSN: 2167-4337},
	keywords = {Abstracts, Arrays, auto-tuning, code generation, DLA code optimization, Generators, Kernel, Registers, Resource management, Seals},
	pages = {1--12},
}

@misc{ibm_oc-accel_2022,
	title = {{OC}-{Accel} {Framework}},
	copyright = {Apache-2.0},
	url = {https://github.com/OpenCAPI/oc-accel},
	abstract = {OpenCAPI Acceleration Framework: develop an accelerator with OpenCAPI technology},
	author = {IBM},
	publisher = {OpenCAPI},
	month = nov,
	year = {2022},
	note = {original-date: 2019-10-30T07:03:30Z},
}
@inproceedings{ding_data_1999,
	address = {New York, NY, USA},
	series = {{SC} '99},
	title = {Data organization and {I}/{O} in a parallel ocean circulation model},
	isbn = {978-1-58113-091-1},
	url = {https://doi.org/10.1145/331532.331565},
	doi = {10.1145/331532.331565},
	booktitle = {Proceedings of the 1999 {ACM}/{IEEE} conference on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Ding, Chris H. Q. and He, Yun},
	month = jan,
	year = {1999},
	pages = {33--es},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@article{kahan_pracniques_1965,
	title = {Pracniques: further remarks on reducing truncation errors},
	volume = {8},
	issn = {0001-0782},
	shorttitle = {Pracniques},
	url = {https://doi.org/10.1145/363707.363723},
	doi = {10.1145/363707.363723},
	number = {1},
	journal = {Communications of the ACM},
	author = {Kahan, W.},
	month = jan,
	year = {1965},
	pages = {40},
}

@inproceedings{priest_algorithms_1991,
	title = {Algorithms for arbitrary precision floating point arithmetic},
	doi = {10.1109/ARITH.1991.145549},
	abstract = {The author presents techniques for performing computations of very high accuracy using only straightforward floating-point arithmetic operations of limited precision. The validity of these techniques is proved under very general hypotheses satisfied by most implementations of floating-point arithmetic. To illustrate the applications of these techniques, an algorithm is presented which computes the intersection of a line and a line segment. The algorithm is guaranteed to correctly decide whether an intersection exists and, if so, to produce the coordinates of the intersection point accurate to full precision. The algorithm is usually quite efficient; only in a few cases does guaranteed accuracy necessitate an expensive computation.{\textless}{\textgreater}},
	booktitle = {[1991] {Proceedings} 10th {IEEE} {Symposium} on {Computer} {Arithmetic}},
	author = {Priest, D.M.},
	month = jun,
	year = {1991},
	keywords = {Algorithm design and analysis, Costs, Error analysis, Floating-point arithmetic, Hardware, High performance computing, Libraries, Mathematics, Packaging, Roundoff errors},
	pages = {132--143},
}

@article{wunsch_satellite_1998,
	title = {{SATELLITE} {ALTIMETRY}, {THE} {MARINE} {GEOID}, {AND} {THE} {OCEANIC} {GENERAL} {CIRCULATION}},
	volume = {26},
	issn = {0084-6597, 1545-4495},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.earth.26.1.219},
	doi = {10.1146/annurev.earth.26.1.219},
	abstract = {For technical reasons, the general circulation of the ocean has historically been treated as a steady, laminar ﬂow ﬁeld. The recent availability of extremely highaccuracy and high-precision satellite altimetry has provided a graphic demonstration that the ocean is actually a rapidly time-evolving turbulent ﬂow ﬁeld. To render the observations quantitatively useful for oceanographic purposes has required order of magnitude improvements in a number of ﬁelds, including orbit dynamics, gravity ﬁeld estimation, and atmospheric variability. With ﬁve years of very high-quality data now available, the nature of oceanic variability on all space and time scales is emerging, including new ﬁndings about such diverse and important phenomena as mixing coefﬁcients, the frequency/wavenumber spectrum, and turbulent cascades. Because the surface elevation is both a cause and consequence of motions deep within the water column, oceanographers soon will be able to provide general circulation numerical models tested against and then combined with the altimeter data. These will be complete three-dimensional timeevolving estimates of the ocean circulation, permitting greatly improved estimates of oceanic heat, carbon, and other property ﬂuxes.},
	language = {en},
	number = {1},
	journal = {Annual Review of Earth and Planetary Sciences},
	author = {Wunsch, Carl and Stammer, Detlef},
	month = may,
	year = {1998},
	pages = {219--253},
}

@INPROCEEDINGS{barrick_ocean,
  author={Barrick, D.},
  booktitle={Ocean 72 - IEEE International Conference on Engineering in the Ocean Environment},
  title={Remote sensing of sea state by radar},
  year={1972},
  volume={},
  number={},
  pages={186-192},
  doi={10.1109/OCEANS.1972.1161190}
}

@article{severance_ieee_1998,
	title = {{IEEE} 754: {An} {Interview} with {William} {Kahan}},
	volume = {31},
	shorttitle = {{IEEE} 754},
	doi = {10.1109/MC.1998.660194},
	abstract = {Not Available},
	journal = {Computer},
	author = {Severance, Charles},
	month = apr,
	year = {1998},
	pages = {114--115},
}

@article{dowson_ariane_1997,
	title = {The {Ariane} 5 software failure},
	volume = {22},
	issn = {0163-5948},
	doi = {10.1145/251880.251992},
	number = {2},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Dowson, Mark},
	month = mar,
	year = {1997},
	pages = {84},
}

@article{nuseibeh_ariane_1997,
	title = {Ariane 5: {Who} {Dunnit}?},
	volume = {14},
	issn = {1937-4194},
	shorttitle = {Ariane 5},
	doi = {10.1109/MS.1997.589224},
	number = {3},
	journal = {IEEE Software},
	author = {Nuseibeh, B.},
	month = may,
	year = {1997},
	note = {Conference Name: IEEE Software},
	pages = {15--16},
}

@article{lelan_analysis_1997,
 author={Le Lann, G.},
  booktitle={Proceedings International Conference and Workshop on Engineering of Computer-Based Systems},
  title={An analysis of the Ariane 5 flight 501 failure-a system engineering perspective},
  year={1997},
  volume={},
  number={},
  pages={339-346},
  doi={10.1109/ECBS.1997.581900}
}

@article{edelman_mathematics_1997,
	title = {The {Mathematics} of the {Pentium} {Division} {Bug}},
	volume = {39},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/S0036144595293959},
	doi = {10.1137/S0036144595293959},
	abstract = {Despite all of the publicity surrounding the Pentium bug of 1994, the mathematical details of the bug are poorly understood. We discuss these details and supply a new proof of the Coe–Tang result that the at-risk divisors have six consecutive ones in positions 5 through 10. Also, we prove that the worst-case absolute error for arguments in [1, 2) is on the order of 1e–5.},
	language = {en},
	number = {1},
	journal = {SIAM Review},
	author = {Edelman, Alan},
	month = jan,
	year = {1997},
	pages = {54--67},
	file = {Edelman - 1997 - The Mathematics of the Pentium Division Bug.pdf:/home/lledoux/Zotero/storage/QKJH7PFI/Edelman - 1997 - The Mathematics of the Pentium Division Bug.pdf:application/pdf},
}

@article{moler_pentium_1995,
  author={Moler, C.},
  title={A Tale of Two Numbers},
  booktitle={SIAM news},
  year={1995},
  volume={28},
  number={1},
  pages={1-16},
}

@article{aittoniemi_cultural_2012,
	title = {Cultural and {Musical} {Dimensions} of {Goa} {Trance} and {Early} {Psychedelic} {Trance} in {Finland} : {The} history, translation and localization of an internationally mobile electronic dance-music scene},
	shorttitle = {Cultural and {Musical} {Dimensions} of {Goa} {Trance} and {Early} {Psychedelic} {Trance} in {Finland}},
	url = {https://helda.helsinki.fi/items/URN:NBN:fi-fe201206156005},
	abstract = {The history of Goa trance spans trough decades, its first cultural factors having been born in 60's USA and then developed in Goa, India through the 70's and 80's. After some of the same factors matured into sub-cultural traits in the western countries, the new attitudes towards the role of youth in society led to increased travelling and some discarding of the traditional sedentary lifestyles. Goa trance would develop to be the music, the identifier and the culture of full-moon parties and other celebratory meetings of a new group of travelling, globally mobile youth interested in exploratory self-development, self-actualization, mysticism and alternative lifestyles and spirituality. Goa trance came to Finland through movement of these people, by an international group of travellers following a Finnish national Ior Bock from Goa to Finland. Goa trance parties in Sipoo at his summer residence started in 1987 and went on until 1998. From 1988 onwards, a similar process would also start in other countries around the world. By 1992 new groups also going to Goa or similar destinations elsewhere already present would also start organizing Goa trance parties in Finland. First finnish experiments in Goa trance music production were conducted the same year. A historical study incorporating music analysis as the bridge between the cultural and the ethnographical is conducted in this research. It's hypothesis is that a system can be found in Goa trance music and that it reflects and represents the cultural values found by ethnographic methods. It also suggests that this system can then be used to track how Finnish Goa trance music-production and culture reflected the different facets of original and international Goa trance culture and how this transfer and synthesis took place. The study is largely material-based, relying on extensive interviews of important people in the 1990's Goa trance scene of Finland, recording artifacts on DAT-tape and c-cassette, and published musical works from the same time-period. Secondary sources include other, mainly ethnographic writings and articles on the topic and several documentary films. The socio-cultural and anthropologic studies of electronic dance-music cultures by Fikentscher, Thornton, Taylor, St. John, Saldanha, and D'Andrea are the precursors and the academic framework within which this study operates in. In addition to new ethnomusicology, the several fields of scientific methodologies applied to the material favor the cognitive, incorporating the ethnomusicology of John Blacking as well as psychoacoustics and cognitive models of musical experience. General cultural semiotics are likewise applied to support the models of behavior developed. A major part of the study is formed by music analysis. The analysis aims to find a system of characteristics that are common or unique, elements of style and then to apply these in the context of the cultural analysis. All the claims are supported by examples in notation transcribed from the materials. The methods of score-analysis include common ethnomusicological and western methodologies supplemented with modern metric theory from Hasty as adapted to analysis of electronic dance-music by Butler and several methodologies connected to it. The study also develops these methods further to form a suitable set of derived methodologies to better deconstruct the particular musical material at hand. Through the music analysis backed by ethnography, it can be seen that the music of Goa trance is a unique development of western electronic dance-music steered strongly into an oriental and mystic direction, simultaneously preserving much of the early hypnotic qualities of early rave-, acid- and techno music. It is highly functional and tied to facilitating a psychedelic experience in the trance-dance party. The early development of Goa trance music parallels that of rave-music and is interconnected, but also separate to a degree. The most prominent difference is that the music was tied to a copying and trading culture instead of depending on record labels for distribution. Finnish Goa trance music production delivered finished works to the DAT-tape trading circuit by 1995, and released works on CD and vinyl in 1996. The first releases were: 10 Years Loop EP by O*Men, Flippin' Bixies - Sörkkä Sonic and Apollo 3D by GAD. The sound of Flippin' Bixies was a more localized version of Goa trance, much more experimental and also ended up being more influential to the global soundscape that evolved from Goa trance: Psychedelic trance. Music analysis shows that a unique Finnish sound was present already in many of the early works of all these artists, and that it bears a kindredness to the kind of music that was also developing in Australia and Japan. International connections between the local music scenes and high international mobility of their agents led to very rapid exchange of music between the scenes and further development from 1997 onwards was bi-directional with new Finnish Goa-/psychedelic trance bands Texas Faggott and Kolmiokulmiosilmiö on the leading edge of it.},
	language = {eng},
	author = {Aittoniemi, Toni},
	year = {2012},
	note = {Publisher: Helsingfors universitet},
}

@book{babbage_passages_2011,
	address = {Cambridge},
	series = {Cambridge {Library} {Collection} - {Technology}},
	title = {Passages from the {Life} of a {Philosopher}},
	isbn = {978-1-108-03788-4},
	url = {https://www.cambridge.org/core/books/passages-from-the-life-of-a-philosopher/5BD0E0F1CA37BB29AA8DD44C584D46D6},
	abstract = {The mathematician and engineer Charles Babbage (1791–1871) is best remembered for his 'calculating machines', which are considered the forerunner of modern computers. Over the course of his life he wrote a number of books based on his scientific investigations, but in this volume, published in 1864, Babbage writes in a more personal vein. He points out at the beginning of the work that it 'does not aspire to the name of autobiography', though the chapters sketch out the contours of his life, beginning with his family, his childhood and formative years studying at Cambridge, and moving through various episodes in his scientific career. However, the work also diverges into his observations on other topics, as indicated by chapter titles such as 'Street Nuisances' and 'Wit'. Babbage's colourful recollections give an intimate portrait of the life of one of Britain's most influential inventors.},
	publisher = {Cambridge University Press},
	author = {Babbage, Charles},
	year = {2011},
	doi = {10.1017/CBO9781139103671},
}

@book{padua_trhilling_2015,
	title = {The Thrilling Adventures of Lovelace and Babbage},
	author = {Padua, Sydney},
	isbn = {978-0-307-90827-8},
	year = {2015},
	publisher = {pantheon books}
}

@article{torres_automatica_1914,
	title = {Automatica: Complemento de la Teoría de las Máquinas},
	author = {Torres Quevedo, Leonardo},
	journal = {Revista de Obras Públicas},
	year = {1914},
	month = nov,
	pages = {575-583}
}

@article{rojas_konrad_1997,
	title = {Konrad {Zuse}'s legacy: the architecture of the {Z1} and {Z3}},
	volume = {19},
	issn = {1934-1547},
	shorttitle = {Konrad {Zuse}'s legacy},
	url = {https://ieeexplore.ieee.org/document/586067},
	doi = {10.1109/85.586067},
	abstract = {Provides a detailed description of the architecture of the Z1 and Z3 computing machines that Konrad Zuse designed in Berlin between 1936 and 1941. The necessary basic information was obtained from a careful evaluation of the patent application Zuse filed in 1941. Additional insight was gained from a software simulation of the machine's logic. The Z1 was built using purely mechanical components; the Z3 used electromechanical relays. However, both machines shared a common logical structure, and their programming model was the same. I argue that both the Z1 and the Z3 possessed features akin to those of modern computers: the memory and processor were separate units, and the processor could handle floating-point numbers and compute the four basic arithmetical operations as well as the square root of a number. The program was stored on punched tape and was read sequentially. In the last section of this paper, I put the architecture of the Z1 and Z3 into historical perspective by offering a comparison with computing machines built in other countries.},
	number = {2},
	journal = {IEEE Annals of the History of Computing},
	author = {Rojas, R.},
	month = apr,
	year = {1997},
	note = {Conference Name: IEEE Annals of the History of Computing},
	pages = {5--16},
}

@incollection{tatnall_reconstruction_2013,
	address = {Berlin, Heidelberg},
	title = {Reconstruction of {Konrad} {Zuse}’s {Z3}},
	volume = {416},
	isbn = {978-3-642-41649-1 978-3-642-41650-7},
	url = {http://link.springer.com/10.1007/978-3-642-41650-7_26},
	abstract = {This paper describes the reconstruction of Konrad Zuse’s Machine Z3 by the author Horst Zuse from 2008. Konrad Zuse built the Z3 machine between 1939 and 1941 with some friends and a small amount of support by the government. The main idea for reconstructing the Z3 was to learn how this machine works and how much effort is necessary to build such a machine. Another main topic was to show this machine to the public.},
	language = {en},
	booktitle = {Making the {History} of {Computing} {Relevant}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zuse, Horst},
	editor = {Tatnall, Arthur and Blyth, Tilly and Johnson, Roger},
	year = {2013},
	doi = {10.1007/978-3-642-41650-7_26},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {287--296},
}

@article{ibm704_1955,
    title = {{IBM Electronic Data-Processing Machines TYPE 704}},
    organization = {International Business Machines Corporation},
    author = {IBM},
    year = {1955},
    note = {IBM 704 Manual of Operation},
}

@article{hamada_urr_1983,
	title = {{URR}: {Universal} representation of real numbers},
	volume = {1},
	issn = {1882-7055},
	shorttitle = {{URR}},
	url = {https://doi.org/10.1007/BF03037427},
	doi = {10.1007/BF03037427},
	abstract = {A new internal representation is proposed for real numbers that has been named URR for Universal Representation of Real Numbers. This approach is based on a bisection method which is applied to real number intervals. With this method, the point of division increases or decreases double-exponentially in global area.},
	language = {en},
	number = {2},
	journal = {New Generation Computing},
	author = {Hamada, Hozumi},
	month = jun,
	year = {1983},
	keywords = {Bisection Method, Double Logarithmic Scale, Float Point Arithmetic, Internal Form, Universal Representation},
	pages = {205--209},
}

@article{hayes_higher_2009,
    author = {Hayes, Bryan},
    title = {The {Higher} {Arithmetic}},
    url = {https://www.americanscientist.org/article/the-higher-arithmetic},
    abstract = {How to count to a zillion without falling off the end of the number line},
    language = {en},
    year = {2009},
    pages = {364},
    volume = {97},
    number = {5},
    doi = {10.1511/2009.80.364},
    journal = {American Scientist},
    month = feb,
}

@article{yokoo_overflowunderflow_free_1992,
	title = {Overflow/underflow-free floating-point number representations with self-delimiting variable-length exponent field},
	volume = {41},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/document/156546},
	doi = {10.1109/12.156546},
	abstract = {A class of new floating-point representations of real numbers, based on representations of the integers, is described. In the class, every representation uses a self-delimiting representation of the integers as a variable length field of the exponent, and neither overflow nor underflow appears in practice. The adopted representations of the integers are defined systematically, so that representation's of numbers greater than one have both exponent-significant and integer-fraction interpretations. Since representation errors are characterized by the length function of an underlying representation of the integers, superior systems in precision can be easily selected from the proposed class.{\textless}{\textgreater}},
	number = {8},
	journal = {IEEE Transactions on Computers},
	author = {Yokoo, H.},
	month = aug,
	year = {1992},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {1033--1039},
}

@inproceedings{azmi_tapered_1989,
	title = {On a tapered floating point system},
	url = {https://ieeexplore.ieee.org/document/72803},
	doi = {10.1109/ARITH.1989.72803},
	abstract = {R. Morris (see IEEE Trans. Comput., vol.TC-20, p.1578-9, 1971), suggested adding an extra field to the fixed floating point system, so that exponents can be stored more efficiently. The exponents are stored in the smallest possible space, passing the extra bits to the mantissa. The extra field is used to monitor the current length of the exponent. The gain in precision and/or exponent range outweighs the overhead of the extra field and the processing speed. The authors provide implementation details, error analysis, and some future research ideas. Simulation results are provided for comparison purposes.{\textless}{\textgreater}},
	booktitle = {Proceedings of 9th {Symposium} on {Computer} {Arithmetic}},
	author = {Azmi, A.M. and Lombardi, F.},
	month = sep,
	year = {1989},
	pages = {2--9},
}

@inproceedings{hamada_new_1987,
	title = {A new real number representation and its operation},
	url = {https://ieeexplore.ieee.org/document/6158698},
	doi = {10.1109/ARITH.1987.6158698},
	abstract = {A new internal representation is proposed for real numbers. It has been named URR for universal representation of real numbers. This approach is based on a bisection method which is applied to real number intervals. With this method, the point of division increases or decreases in a double exponential manner in the global range.},
	booktitle = {1987 {IEEE} 8th {Symposium} on {Computer} {Arithmetic} ({ARITH})},
	author = {Hamada, Hozumi},
	month = may,
	year = {1987},
	pages = {153--157},
}

@inproceedings{tagliavini_transprecision_2018,
	title = {A transprecision floating-point platform for ultra-low power computing},
	url = {https://ieeexplore.ieee.org/document/8342167},
	doi = {10.23919/DATE.2018.8342167},
	abstract = {In modern low-power embedded platforms, the execution of floating-point (FP) operations emerges as a major contributor to the energy consumption of compute-intensive applications with large dynamic range. Experimental evidence shows that 50\% of the energy consumed by a core and its data memory is related to FP computations. The adoption of FP formats requiring a lower number of bits is an interesting opportunity to reduce energy consumption, since it allows to simplify the arithmetic circuitry and to reduce the memory bandwidth required to transfer data between memory and registers by enabling vectorization. From a theoretical point of view, the adoption of multiple FP types perfectly fits with the principle of transprecision computing, allowing fine-grained control of approximation while meeting specified constraints on the precision of final results. In this paper we propose an extended FP type system with complete hardware support to enable transprecision computing on low-power embedded processors, including two standard formats (binary32 and binary16) and two new formats (binary8 and binary16alt). First, we introduce a software library that enables exploration of FP types by tuning both precision and dynamic range of program variables. Then, we present a methodology to integrate our library with an external tool for precision tuning, and experimental results that highlight the clear benefits of introducing the new formats. Finally, we present the design of a transprecision FP unit capable of handling 8-bit and 16-bit operations in addition to standard 32-bit operations. Experimental results on FP-intensive benchmarks show that up to 90\% of FP operations can be safely scaled down to 8-bit or 16-bit formats. Thanks to precision tuning and vectorization, execution time is decreased by 12\% and memory accesses are reduced by 27\% on average, leading to a reduction of energy consumption up to 30\%.},
	booktitle = {2018 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Tagliavini, Giuseppe and Mach, Stefan and Rossi, Davide and Marongiu, Andrea and Benini, Luca},
	month = mar,
	year = {2018},
	note = {ISSN: 1558-1101},
	pages = {1051--1056},
}

@article{osorio_bf16_2022,
	title = {A {BF16} {FMA} is {All} {You} {Need} for {DNN} {Training}},
	volume = {10},
	issn = {2168-6750},
	url = {https://ieeexplore.ieee.org/document/9823406},
	doi = {10.1109/TETC.2022.3187770},
	abstract = {Fused Multiply-Add (FMA) functional units constitute a fundamental hardware component to train Deep Neural Networks (DNNs). Its silicon area grows quadratically with the mantissa bit count of the computer number format, which has motivated the adoption of the BrainFloat16 format (BF16). BF16 features 1 sign, 8 exponent and 7 explicit mantissa bits. Some approaches to train DNNs achieve significant performance benefits by using the BF16 format. However, these approaches must combine BF16 with the standard IEEE 754 Floating-Point 32-bit (FP32) format to achieve state-of-the-art training accuracy, which limits the impact of adopting BF16. This article proposes the first approach able to train complex DNNs entirely using the BF16 format. We propose a new class of FMA operators, {\textbackslash}mathrmFMA{\textasciicircum}{\textbackslash}mathrm bf16\_{\textbackslash}mathrmn\_{\textbackslash}mathrmm FMA n\_m bf 16, that entirely rely on BF16 FMA hardware instructions and deliver the same accuracy as FP32. {\textbackslash}mathrmFMA{\textasciicircum}{\textbackslash}mathrm bf16\_{\textbackslash}mathrmn\_{\textbackslash}mathrmm FMA n\_m bf 16 operators achieve performance improvements within the 1.28-1.35× range on ResNet101 with respect to FP32. {\textbackslash}mathrmFMA{\textasciicircum}{\textbackslash}mathrm bf16\_{\textbackslash}mathrmn\_{\textbackslash}mathrmm FMA n\_m bf 16 enables training complex DNNs on simple low-end hardware devices without requiring expensive FP32 FMA functional units.},
	number = {3},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Osorio, John and Armejach, Adrià and Petit, Eric and Henry, Greg and Casas, Marc},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Emerging Topics in Computing},
	pages = {1302--1314},
}

@article{kalamkar_study_2019,
	title = {A {Study} of {BFLOAT16} for {Deep} {Learning} {Training}},
	url = {http://arxiv.org/abs/1905.12322},
	abstract = {This paper presents the ﬁrst comprehensive empirical study demonstrating the efﬁcacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classiﬁcation, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 ﬂoating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision ﬂoating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the ﬂow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorﬂow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.},
	language = {en},
	journal = {arXiv:1905.12322 [cs, stat]},
	author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.12322},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{petrov_using_2020,
	title = {Use of {Bfloat16} Format in Deep Learning Embedded Accelerators based on {FPGA} with Limited Quantity of Dedicated Multipliers},
	doi = {10.1109/COM50385.2020.9299565},
	abstract = {The hardware base of Deep Learning Neural Network (DLNN) realization methods are remote cloud services, Graphical Processing Units (GPU) and Field Programmable Gate Arrays (FPGA). One of the main differences between FPGA devices important for DLNN realization is the quantity of dedicated multipliers in DSP blocks. This article describes a method for optimization based on bfloat16 data format useful for FPGA devices with small quantities of DSP blocks.},
	booktitle = {2020 28th National Conference with International Participation ({ECOM})},
	author = {Petrov, B. B.},
	month = oct,
	year = {2020},
	keywords = {Field programmable gate arrays, Clocks, FPGA, deep learning, Adders, Deep learning, accelerator, bfloat16, Calculators, Data transfer, Neurons},
	pages = {82--85},
}

@inproceedings{burgess_bfloat16_2019,
	title = {Bfloat16 {Processing} for {Neural} {Networks}},
	doi = {10.1109/ARITH.2019.00022},
	abstract = {Bfloat16 ("BF16") is a new floating-point format tailored specifically for high-performance processing of Neural Networks and will be supported by major CPU and GPU architectures as well as Neural Network accelerators. This paper proposes a possible implementation of a BF16 multiply-accumulation operation that relaxes several IEEE Floating-Point Standard features to afford low-cost hardware implementations. Specifically, subnorms are flushed to zero; only one non-standard rounding mode (Round-Odd) is supported; NaNs are not propagated; and IEEE exception flags are not provided. The paper shows that this approach achieves the same network-level accuracy as using IEEE single-precision arithmetic ("FP32") for less than half the datapath area cost and with greater throughput.},
	booktitle = {2019 {IEEE} 26th {Symposium} on {Computer} {Arithmetic} ({ARITH})},
	author = {Burgess, N. and Milanovic, J. and Stephens, N. and Monachopoulos, K. and Mansell, D.},
	month = jun,
	year = {2019},
	note = {ISSN: 2576-2265},
	keywords = {Standards, Computer architecture, Artificial neural networks, Digital arithmetic, Error analysis, floating-point, rounding mode, neural networks, Training},
	pages = {88--91},
}

@online{nvidia_cuda11_2020,
  title={CUDA 11 Features Revealed},
  author={NVIDIA},
  year={2020},
  url={https://developer.nvidia.com/blog/cuda-11-features-revealed/}
}

@misc{intel_cblas_gemm_bf16bf16f32_compute_2022,
	title = {cblas\_gemm\_bf16bf16f32\_compute},
	abstract = {Computes a matrix-matrix product with general bfloat16 matrices (where one or both input matrices are stored in a packed data structure) and adds the result to a scalar-matrix product.},
	language = {English},
	year = {2022},
	author = {Intel}
}

@online{amd_rocm_2023,
  title={Use ROCm™ on Radeon™ GPUs},
  author={AMD},
  year={2023},
  url={https://rocm.docs.amd.com/projects/radeon/en/latest/index.html}
}

@online{phoronix_rocm_2019,
  title={Radeon ROCm 2.6 Released - Without Navi Support But Adds BFloat16},
  author={Phoronix},
  year={2019},
  url={https://www.phoronix.com/news/Radeon-ROCm-2.6-Released}
}

@online{aocl_amd_2023,
  title={AMD Optimizing CPU Libraries (AOCL)},
  author={AMD},
  year={2023},
  url={https://www.amd.com}
}

@online{intel2022dlboost,
  title={Intel® Deep Learning Boost New Deep Learning Instruction bfloat16},
  author={Intel},
  year={2022},
  url={https://www.intel.com/content/www/us/en/develop/articles/intel-deep-learning-boost-new-deep-learning-instruction-bfloat16.html}
}

@online{intel2022avx512,
  title={Intel® AVX-512 - FP16 Instruction Set for Intel® Xeon® Processor Based},
  author={Intel},
  year={2022},
  url={https://www.intel.com/content/www/us/en/develop/documentation/isc-manual/top/instruction-set-referencing-manual/512-fp16-instruction-set-for-intel-xeon-processor-based.html}
}

@article{amd2023zen,
  title={AMD's Zen 4 Genoa cores will reportedly feature AVX-512 and BFLOAT16},
  author={Overclock3D},
  journal={Overclock3D.net},
  year={2023},
  url={https://www.overclock3d.net/news/cpu_mainboard/amd_s_zen_4_genoa_cores_will_reportedly_feature_avx-512_and_bfloat16/1}
}

@online{amd2023rocm,
  title={AMD ROCm Code Suggests BFloat16 Support in Future GPU},
  author={Tom's Hardware},
  year={2023},
  url={https://forums.tomshardware.com/threads/amd-rocm-code-suggests-bfloat16-support-in-future-gpu.3609990/}
}

@online{apple2023metal,
  title={Optimize machine learning for Metal apps - Apple Developer},
  author={Apple},
  year={2023},
  url={https://developer.apple.com/documentation/metal/optimize_machine_learning_for_metal_apps}
}

@online{apple2023m2a15,
  title={Apple M2 Support Added To Upstream LLVM Along With The A15, A16},
  author={Phoronix},
  year={2023},
  url={https://www.phoronix.com/scan.php?page=news_item&px=Apple-M2-A15-A16-LLVM-Upstream}
}

@online{amd2023instinct,
  title={AMD Instinct™ Expands Ecosystem and Delivers Exascale-Class Technology},
  author={AMD},
  year={2023},
  url={https://ir.amd.com/news-events/press-releases/detail/984/amd-instinct-expands-ecosystem-and-delivers-exascale-class}
}

@online{nvidia2023ampere,
  title={NVIDIA Ampere Architecture},
  author={NVIDIA},
  year={2023},
  url={https://www.nvidia.com/en-us/data-center/nvidia-ampere-architecture/}
}

@online{intel2023datacenter,
  title={Intel® Data Center GPU Flex Series},
  author={Intel},
  year={2023},
  url={https://www.intel.com/content/www/us/en/products/details/data-center/accelerators/data-center-gpu-flex-series.html}
}

@online{intel2023nervana,
  title={Intel unveils Nervana Neural Net L-1000 for accelerated AI training},
  author={Intel},
  year={2023},
  url={https://venturebeat.com/ai/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/}
}

@online{aws2023inferentia,
  title={Serve 3,000 deep learning models on Amazon EKS with AWS Inferentia},
  author={AWS},
  year={2023},
  url={https://aws.amazon.com/blogs/aws/amazon-eks-aws-inferentia-ml-models/}
}

@online{aws2023trainium,
  title={AI Accelerator - AWS Trainium},
  author={AWS},
  year={2023},
  url={https://aws.amazon.com/machine-learning/trainium/}
}


@misc{arm_isa_2023,
	title = {Arm {A64} {Instruction} {Set} {Architecture}},
	author={ARM},
	year = {2023},
	url = {https://developer.arm.com/documentation/ddi0596/2021-12/SVE-Instructions/BFDOT--vectors---BFloat16-floating-point-dot-product-},
}

@Manual{apache_arrow_2023,
  title = {arrow: Integration to 'Apache' 'Arrow'},
  author = {Neal Richardson and Ian Cook and Nic Crane and Dewey Dunnington and Romain François and Jonathan Keane and Dragoș Moldovan-Grünfeld and Jeroen Ooms and Jacob Wujciak-Jens and {Apache Arrow}},
  year = {2023},
  note = {R package version 14.0.1, https://arrow.apache.org/docs/r/},
  url = {https://github.com/apache/arrow/},
}

@inproceedings{peltenburg_fletcher_2019,
	title = {Fletcher: {A} {Framework} to {Efficiently} {Integrate} {FPGA} {Accelerators} with {Apache} {Arrow}},
	shorttitle = {Fletcher},
	url = {https://ieeexplore.ieee.org/document/8892145},
	doi = {10.1109/FPL.2019.00051},
	abstract = {Modern big data systems are highly heterogeneous. The components found in their many layers of abstraction are often implemented in a wide variety of programming languages and frameworks. Due to language implementation differences, interfaces between these components, including hardware accelerated components, are often burdened by serialization overhead. Serialization bandwidth of many high-level language frameworks is an order of magnitude lower than contemporary FPGA accelerator interface bandwidth, especially when objects are small but numerous. Therefore, serialization bounds the effective end-to-end performance of FPGA-accelerated solutions integrated with applications written in high-level languages. The Apache Arrow project defines a language agnostic columnar in-memory format optimized for big data applications, preventing the need to serialize or even make copies during communication between components. To enable FPGA accelerators to benefit from the approach of Arrow, we first investigate the properties of its format in relation to hardware interfaces and establish that the format is usable. Second, we present the Fletcher framework, that automatically generates highly efficient hardware interfaces to access data of potentially complex, nested Arrow data types. Our approach allows 11 of the languages supported by Apache Arrow libraries to efficiently communicate large data sets with FPGA accelerators at system bandwidth. Furthermore, on the hardware side, the generated interfaces deliver any data type that Arrow can represent as groups of streams, providing a better starting point for data-flow-oriented kernel development, compared to manually creating custom interfaces to address issues related to pointer arithmetic, bus word misalignment and latency. For example applications, as measured on an AWS EC2 F1 and CAPI2-enabled POWER9 system, accelerated end-to-end application performance improves by 1.3x - 49x compared to a hardware accelerated solution that still requires serialization.},
	booktitle = {2019 29th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Peltenburg, Johan and van Straten, Jeroen and Wijtemans, Lars and van Leeuwen, Lars and Al-Ars, Zaid and Hofstee, Peter},
	month = sep,
	year = {2019},
	note = {ISSN: 1946-1488},
	pages = {270--277},
}

@inproceedings{peltenburg_supporting_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Supporting {Columnar} {In}-memory {Formats} on {FPGA}: {The} {Hardware} {Design} of {Fletcher} for {Apache} {Arrow}},
	isbn = {978-3-030-17227-5},
	shorttitle = {Supporting {Columnar} {In}-memory {Formats} on {FPGA}},
	doi = {10.1007/978-3-030-17227-5_3},
	abstract = {As a columnar in-memory format, Apache Arrow has seen increased interest from the data analytics community. Fletcher is a framework that generates hardware interfaces based on this format, to be used in FPGA accelerators. This allows efficient integration of FPGA accelerators with various high-level software languages, while providing an easy-to-use hardware interface for the FPGA developer. The abstract descriptions of data sets stored in the Arrow format, that form the input of the interface generation step, can be complex. To generate efficient interfaces from it is challenging. In this paper, we introduce the hardware components of Fletcher that help solve this challenge. These components allow FPGA developers to express access to complex Arrow data records through row indices of tabular data sets, rather than through byte addresses. The data records are delivered as streams of the same abstract types as found in the data set, rather than as memory bus words. The generated interfaces allow for full system bandwidth to be utilized and have a low area profile. All components are open sourced and available for other researchers and developers to use in their projects.},
	language = {en},
	booktitle = {Applied {Reconfigurable} {Computing}},
	publisher = {Springer International Publishing},
	author = {Peltenburg, Johan and van Straten, Jeroen and Brobbel, Matthijs and Hofstee, H. Peter and Al-Ars, Zaid},
	editor = {Hochberger, Christian and Nelson, Brent and Koch, Andreas and Woods, Roger and Diniz, Pedro},
	year = {2019},
	keywords = {Apache Arrow, Fletcher, FPGA},
	pages = {32--47},
}

@article{street_nuclear_1999,
	title = {Nuclear {Feature} {Extraction} {For} {Breast} {Tumor} {Diagnosis}},
	volume = {1993},
	doi = {10.1117/12.148698},
	abstract = {Interactive image processing techniques, along with a linear-programming-based inductive classifier, have been used to create a highly accurate system for diagnosis of breast tumors. A small fraction of a fine needle aspirate slide is selected and digitized. With an interactive interface, the user initializes active contour models, known as snakes, near the boundaries of a set of cell nuclei. The customized snakes are deformed to the exact shape of the nuclei. This allows for precise, automated analysis of nuclear size, shape and texture. Ten such features are computed for each nucleus, and the mean value, largest (or "worst") value and standard error of each feature are found over the range of isolated cells. After 569 images were analyzed in this fashion, different combinations of features were tested to find those which best separate benign from malignant samples. Ten-fold cross-validation accuracy of 97\% was achieved using a single separating plane on three of the thirty ...},
	journal = {Proc. Soc. Photo-Opt. Inst. Eng.},
	author = {Street, Nick and Wolberg, William and Mangasarian, O},
	month = jan,
	year = {1999},
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	pages = {179--188},
}

@article{schlimmer_concept_1987,
	title = {Concept acquisition through representational adjustment},
	url = {https://escholarship.org/uc/item/48r6d4z0},
	abstract = {Though the experiences of life exhibit unceasing variety, people are able to find constancy and deal with their world in a regular and predictable manner. This thesis promotes the hypothesis that the necessary abstractions can be learned. The specific task studied is inducing a concept description from examples. A model is presented. that relies on a weighted, symbolic description of concepts. Though the description is distributed, novel examples are classified holistically by combining each portion's contribution. Each new example also refines the concept description: internal weights are updated and new symbolic structures are introduced. These actions improve description quality as measured by classification accuracy over novel examples.Initially the concept description is highly distributed, being composed of many simple components. As learning progresses, sophisticated descriptive structures are added, and eventually the description is coalesced into a few highly predictive components. This qualitative change in the representation of the concept is a unique feature of the model.The model extends previous work by allowing for noisy examples, unknown values, and concept change over time. To bolster claims of robustness, several experiments illustrating the model's behavior are reported. Key results illustrate that the model should scale-up to larger tasks than those studied and have a number of potential applications.},
	language = {en},
	author = {Schlimmer, Jeffrey C.},
	month = jul,
	year = {1987},
}

@inproceedings{raposo_positnn_2021,
	title = {Positnn: {Training} {Deep} {Neural} {Networks} with {Mixed} {Low}-{Precision} {Posit}},
	shorttitle = {Positnn},
	url = {https://ieeexplore.ieee.org/document/9413919},
	doi = {10.1109/ICASSP39728.2021.9413919},
	abstract = {Low-precision formats have proven to be an efficient way to reduce not only the memory footprint but also the hardware resources and power consumption of deep learning computations. Under this premise, the posit numerical format appears to be a highly viable substitute for the IEEE floating-point, but its application to neural networks training still requires further research. Some preliminary results have shown that 8-bit (and even smaller) posits may be used for inference and 16-bit for training, while maintaining the model accuracy. The presented research aims to evaluate the feasibility to train deep convolutional neural networks using posits. For such purpose, a software framework was developed to use simulated posits and quires in end-to-end training and inference. This implementation allows using any bit size, configuration, and even mixed precision, suitable for different precision requirements in various stages. The obtained results suggest that 8-bit posits can substitute 32-bit floats during training with no negative impact on the resulting loss and accuracy.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Raposo, Gonçalo and Tomás, Pedro and Roma, Nuno},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	pages = {7908--7912},
}

@article{lu_evaluations_2021,
	title = {Evaluations on {Deep} {Neural} {Networks} {Training} {Using} {Posit} {Number} {System}},
	volume = {70},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/document/9066876},
	doi = {10.1109/TC.2020.2985971},
	abstract = {The training of Deep Neural Networks (DNNs) brings enormous memory requirements and computational complexity, which makes it a challenge to train DNN models on resource-constrained devices. Training DNNs with reduced-precision data representation is crucial to mitigate this problem. In this article, we conduct a thorough investigation on training DNNs with low-bit posit numbers, a Type-III universal number (Unum). Through a comprehensive analysis of quantization with various data formats, it is demonstrated that the posit format shows great potential to be employed in the training of DNNs. Moreover, a DNN training framework using 8-bit posit is proposed with a novel tensor-wise scaling scheme. The experiments show the same performance as the state-of-the-art (SOTA) across multiple datasets (MNIST, CIFAR-10, ImageNet, and Penn Treebank) and model architectures (LeNet-5, AlexNet, ResNet, MobileNet-V2, and LSTM). We further design an energy-efficient hardware prototype for our framework. Compared to the standard floating-point counterpart, our design achieves a reduction of 68, 51, and 75 percent in terms of area, power, and memory capacity, respectively.},
	number = {2},
	journal = {IEEE Transactions on Computers},
	author = {Lu, Jinming and Fang, Chao and Xu, Mingyang and Lin, Jun and Wang, Zhongfeng},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {174--187},
}

@article{murillo_deep_2020,
	title = {Deep {PeNSieve}: {A} deep learning framework based on the posit number system},
	volume = {102},
	issn = {1051-2004},
	shorttitle = {Deep {PeNSieve}},
	url = {https://www.sciencedirect.com/science/article/pii/S105120042030107X},
	doi = {10.1016/j.dsp.2020.102762},
	abstract = {The Posit Number System (PNS) was introduced by John L. Gustafson in 2017. The interesting properties of this novel format can be exploited under the scenario of deep neural networks. In this paper, we propose Deep PeNSieve, a framework for entirely performing both training and inference of deep neural networks employing the PNS. Furthermore, an 8-bit posit quantization approach using fused operations is introduced. In comparison with the state-of-the-art posit frameworks, the proposal has been able to train more complex networks than the feedforward ones, achieving similar accuracies as the floating-point format. The case of CIFAR-10 is especially remarkable, as 16-bit posits even obtain 4\% higher top-1 for such dataset. Overall, results show that the proposed quantization approach can preserve model accuracy in the same manner as common quantization techniques.},
	journal = {Digital Signal Processing},
	author = {Murillo, Raul and Del Barrio, Alberto A. and Botella, Guillermo},
	month = jul,
	year = {2020},
	keywords = {Deep neural network, Floating-point arithmetic, Inference, Posit arithmetic, Training},
	pages = {102762},
	file = {ScienceDirect Snapshot:/home/binaryman/Zotero/storage/PYYCMGW7/S105120042030107X.html:text/html},
}

@article{krizhevsky_learning_2009,
	year = {2009},
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	language = {en},
	author = {Krizhevsky, Alex},
}

@inproceedings{ciocirlan_accuracy_2021,
	title = {The {Accuracy} and {Efficiency} of {Posit} {Arithmetic}},
	url = {https://ieeexplore.ieee.org/document/9643619},
	doi = {10.1109/ICCD53106.2021.00024},
	abstract = {Motivated by the increasing interest in the posit numeric format, in this paper we evaluate the accuracy and efficiency of posit arithmetic in contrast to the traditional IEEE 754 32-bit floating-point (FP32) arithmetic. We first design and implement a Posit Arithmetic Unit (PAU), called POSAR, with flexible bit-sized arithmetic suitable for applications that can trade accuracy for savings in chip area. Next, we analyze the accuracy and efficiency of POSAR with a series of benchmarks including mathematical computations, ML kernels, NAS Parallel Benchmarks (NPB), and Cifar-10 CNN. This analysis is done on our implementation of POSAR integrated into a RISC-V Rocket Chip core in comparison with the IEEE 754-based Floting Point Unit (FPU) of Rocket Chip. Our analysis shows that POSAR can outperform the FPU, but the results are not spectacular. For NPB, 32-bit posit achieves better accuracy than FP32 and improves the execution by up to 2\%. However, POSAR with 32-bit posit needs 30\% more FPGA resources compared to the FPU. For classic ML algorithms, we find that 8-bit posits are not suitable to replace FP32 because they exhibit low accuracy leading to wrong results. Instead, 16-bit posit offers the best option in terms of accuracy and efficiency. For example, 16-bit posit achieves the same Top-1 accuracy as FP32 on a Cifar-10 CNN with a speedup of 18\%.},
	booktitle = {2021 {IEEE} 39th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Ciocirlan, Stefan Dan and Loghin, Dumitrel and Ramapantulu, Lavanya and Ţăpuş, Nicolae and Teo, Yong Meng},
	month = oct,
	year = {2021},
	note = {ISSN: 2576-6996},
	pages = {83--87},
}

@book{muller_elementary_2016,
	address = {Boston, MA},
	title = {Elementary {Functions}},
	isbn = {978-1-4899-7981-0 978-1-4899-7983-4},
	url = {http://link.springer.com/10.1007/978-1-4899-7983-4},
	language = {en},
	publisher = {Birkhäuser},
	author = {Muller, Jean-Michel},
	year = {2016},
	doi = {10.1007/978-1-4899-7983-4},
	keywords = {Computer Arithmetic, CORDIC algorithm, Elementary Functions, Floating-Point Arithmetic, Polynomial Approximation, Rational Approximation, shift-and-add algorithms, Table-Based Methods},
}

@book{cody_software_1980,
	address = {USA},
	title = {Software {Manual} for the {Elementary} {Functions} ({Prentice}-{Hall} series in computational mathematics)},
	isbn = {978-0-13-822064-8},
	publisher = {Prentice-Hall, Inc.},
	author = {Cody, William James},
	month = jun,
	year = {1980},
}

@inproceedings{ng_argument_2006,
	title = {{ARGUMENT} {REDUCTION} {FORHUGE} {ARGUMENTS}: {Good} to the {Last} {Bit}},
	shorttitle = {{ARGUMENT} {REDUCTION} {FORHUGE} {ARGUMENTS}},
	abstract = {When the argument is large, most early [floating-point] software writers did not attempt to perform argument reduction precisely. Instead they returned junk, or 0.0, or an error message. As a result many users (and some implementors) have formed the impression that obtaining the correct function value for large inputs is simply impossible. That is why the current AT\&T System V Release 4 still requires an error message be signaled and the result be 0.0 for any trigonometric functions when the argument is huge. That restriction is unnecessary. The correct answer can be computed quite efficiently.},
	author = {Ng, K. C. and O’Connor, Derek},
	year = {2006},
}

@article{Mller1965QuasiDI,
  title={Quasi double-precision in floating point addition},
  author={Ole M{\o}ller},
  journal={BIT Numerical Mathematics},
  year={1965},
  volume={5},
  pages={37-50},
  url={https://api.semanticscholar.org/CorpusID:119991676}
}

@Book{MullerEtAl2018,
  title        = {Handbook of Floating-Point Arithmetic, 2nd edition},
  author    = {Muller, Jean-Michel and Brunie, Nicolas and de Dinechin, Florent and Jeannerod, Claude-Pierre and Joldes, Mioara and
          Lef{\`e}vre, Vincent and Melquiond, Guillaume and Revol,
          Nathalie and  Torres, Serge},
  publisher    = {{B}irkh\"auser {B}oston },
  pages    = {632},
  note        = {{ACM} {G}.1.0; {G}.1.2; {G}.4; {B}.2.0; {B}.2.4; {F}.2.1.,
                  ISBN 978-3-319-76525-9},
  year        = {2018},
}

@inproceedings{anderson2006accurate,
  title={Accurate Math Functions on the Intel IA-32 Architecture: A Performance-Driven Design},
  author={Anderson, C. S. and Story, S. and Astafiev, N.},
  booktitle={7th Conference on Real Numbers and Computers},
  pages={93--105},
  year={2006}
}

@inproceedings{dedinechin2005postultimate,
  title={Towards the Post-Ultimate libm},
  author={de Dinechin, Florent and Ershov, Alexey V. and Gast, Nicolas},
  booktitle={17th Symposium on Computer Arithmetic (ARITH-17)},
  organization={IEEE},
  pages={288--295},
  year={2005}
}

@article{dedinechin2007fast,
  title={Fast and Correctly Rounded Logarithms in Double-Precision},
  author={de Dinechin, Florent and Lauter, Christoph Q. and Muller, J.-M.},
  journal={Theoretical Informatics and Applications},
  volume={41},
  pages={85--102},
  year={2007}
}

@inproceedings{demmel2013fast,
  title={Fast Reproducible Floating-Point Summation},
  author={Demmel, J. and Nguyen, H. D.},
  booktitle={21th Symposium on Computer Arithmetic (ARITH-21)},
  organization={IEEE},
  pages={163--172},
  year={2013}
}

@book{granlund_gnu_2015,
	address = {London, GBR},
	title = {{GNU} {MP} 6.0 {Multiple} {Precision} {Arithmetic} {Library}},
	isbn = {978-988-8381-96-8},
	abstract = {GNU MP is a portable library written in C for arbitrary precision arithmetic on integers, rational numbers, and floating-point numbers. It aims to provide the fastest possible arithmetic for all applications that need higher precision than is directly supported by the basic C types.},
	publisher = {Samurai Media Limited},
	author = {Granlund, Torbjrn and Gmp Development Team},
	month = oct,
	year = {2015},
}

@inproceedings{cousot1977abstract,
  title={Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},
  author={Cousot, Patrick and Cousot, Radhia},
  booktitle={Symposium on principles of programming languages},
  pages={238--252},
  year={1977},
  organization={ACM}
}

@article{kim1994floating,
  title={A floating-point to fixed-point assembly program translator for the TMS 320C25},
  author={Kim, Seehyun and Sung, Wonyong},
  journal={Transactions on circuits and systems II},
  volume={41},
  number={11},
  pages={730--739},
  year={1994}
}

@article{kim1998fixed,
  title={Fixed-point optimization utility for C and C++ based digital signal processing programs},
  author={Kim, Seehyun and Kum, Ki-Il and Sung, Wonyong},
  journal={Transactions on circuits and systems II},
  volume={45},
  number={11},
  pages={1455--1464},
  year={1998}
}

@article{martel2009enhancing,
  title={Enhancing the implementation of mathematical formulas for fixed-point and floating-point arithmetics},
  author={Martel, Matthieu},
  journal={Formal methods in system design},
  volume={35},
  number={3},
  pages={265--278},
  year={2009}
}

@misc{sentieys2014automatic,
  title={Automatic fixed-point conversion: a gateway to high-level power optimization},
  author={Sentieys, Olivier and Menard, Daniel and Novo, David and Parashar, Karthick},
  year={2014},
  note={Tutorial at IEEE/ACM Design Automation and Test in Europe}
}

@inproceedings{osorio_rios_evaluating_2020,
	title = {Evaluating {Mixed}-{Precision} {Arithmetic} for {3D} {Generative} {Adversarial} {Networks} to {Simulate} {High} {Energy} {Physics} {Detectors}},
	url = {https://ieeexplore.ieee.org/abstract/document/9356207},
	doi = {10.1109/ICMLA51294.2020.00017},
	abstract = {Several hardware companies are proposing native Brain Float 16-bit (BF16) support for neural network training. The usage of Mixed Precision (MP) arithmetic with floating-point 32-bit (FP32) and 16-bit half-precision aims at improving memory and floating-point operations throughput, allowing faster training of bigger models. This paper proposes a binary analysis tool enabling the emulation of lower precision numerical formats in Neural Network implementation without the need for hardware support. This tool is used to analyze BF16 usage in the training phase of a 3D Generative Adversarial Network (3DGAN) simulating High Energy Physics detectors. The binary tool allows us to confirm that BF16 can provide results with similar accuracy as the full-precision 3DGAN version and the costly reference numerical simulation using double precision arithmetic.},
	booktitle = {2020 19th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Osorio Ríos, John and Armejach, Adrià and Khattak, Gulrukh and Petit, Eric and Vallecorsa, Sofia and Casas, Marc},
	month = dec,
	year = {2020},
	pages = {49--56},
}

@misc{dubey_activation_2022,
	title = {Activation {Functions} in {Deep} {Learning}: {A} {Comprehensive} {Survey} and {Benchmark}},
	shorttitle = {Activation {Functions} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.14545},
	doi = {10.48550/arXiv.2109.14545},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/ActivationFunctions\}.},
	publisher = {arXiv},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	month = jun,
	year = {2022},
	note = {arXiv:2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{lederer_activation_2021,
	title = {Activation {Functions} in {Artificial} {Neural} {Networks}: {A} {Systematic} {Overview}},
	shorttitle = {Activation {Functions} in {Artificial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2101.09957},
	doi = {10.48550/arXiv.2101.09957},
	abstract = {Activation functions shape the outputs of artificial neurons and, therefore, are integral parts of neural networks in general and deep learning in particular. Some activation functions, such as logistic and relu, have been used for many decades. But with deep learning becoming a mainstream research topic, new activation functions have mushroomed, leading to confusion in both theory and practice. This paper provides an analytic yet up-to-date overview of popular activation functions and their properties, which makes it a timely resource for anyone who studies or applies neural networks.},
	publisher = {arXiv},
	author = {Lederer, Johannes},
	month = jan,
	year = {2021},
	note = {arXiv:2101.09957 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{ding_activation_2018,
	title = {Activation functions and their characteristics in deep neural networks},
	url = {https://ieeexplore.ieee.org/document/8407425},
	doi = {10.1109/CCDC.2018.8407425},
	abstract = {Deep neural networks have gained remarkable achievements in many research areas, especially in computer vision, and natural language processing. The great successes of deep neural networks depend on several aspects in which the development of activation function is one of the most important elements. Being aware of this, a number of researches have concentrated on the performance improvements after the revision of a certain activation function in some specified neural networks. We have noticed that there are few papers to review thoroughly the activation functions employed by the neural networks. Therefore, considering the impact of improving the performance of neural networks with deep architectures, the status and the developments of commonly used activation functions will be investigated in this paper. More specifically, the definitions, the impacts on the neural networks, and the advantages and disadvantages of quite a few activation functions will be discussed in this paper. Furthermore, experimental results on the dataset MNIST are employed to compare the performance of different activation functions.},
	booktitle = {2018 {Chinese} {Control} {And} {Decision} {Conference} ({CCDC})},
	author = {Ding, Bin and Qian, Huimin and Zhou, Jun},
	month = jun,
	year = {2018},
	note = {ISSN: 1948-9447},
	pages = {1836--1841},
}

@inproceedings{sefat_accelerating_2019,
	title = {Accelerating {HotSpots} in {Deep} {Neural} {Networks} on a {CAPI}-{Based} {FPGA}},
	url = {https://ieeexplore.ieee.org/abstract/document/8855410},
	doi = {10.1109/HPCC/SmartCity/DSS.2019.00048},
	abstract = {This paper introduces a new energy-efficient FPGA accelerator targeting the hotspots in Deep Neural Network (DNN) applications. Our design leverages the Coherent Accelerator Processor Interface (CAPI) which provides a coherent view of system memory to attached accelerators. Our implementation bypasses the need for device driver code and significantly reduces the communication and I/O overhead. Performance is further improved by a tiling transformation that exploits data locality in the computation kernel via the CAPI Power Service Layer (PSL) cache. A new adder tree configuration is proposed which achieves a tunable balance between resource utilization and power consumption. An implementation on a CAPI-supported Kintex FPGA board achieves up to 155 GOPs/s and 15.79 GOPs/watt, improving on the state-of-the-art of FPGA-based DNN implementations.},
	booktitle = {2019 {IEEE} 21st {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 17th {International} {Conference} on {Smart} {City}; {IEEE} 5th {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author = {Sefat, Md Syadus and Aslan, Semih and Kellington, Jeffrey W and Qasem, Apan},
	month = aug,
	year = {2019},
	pages = {248--256},
}

@inproceedings{haghi_hardwaresoftware_2020,
	title = {A {Hardware}/{Software} {Co}-{Design} of {K}-mer {Counting} {Using} a {CAPI}-{Enabled} {FPGA}},
	url = {https://ieeexplore.ieee.org/abstract/document/9221608},
	doi = {10.1109/FPL50879.2020.00020},
	abstract = {Advances in Next Generation Sequencing (NGS) technologies have caused the proliferation of genomic applications to detect DNA mutations and guide personalized medicine. These applications have an enormous computational cost due to the large amount of genomic data they process. Although leveraging FPGAs can improve the processing time of such amount of data, the limited memory capacity of FPGAs often restricts the potential gains. To overcome this limitation, IBM CAPI (Coherent Accelerator Processor Interface) supported platforms provide FPGAs with direct access to the CPU memory. This paper proposes a hardware/software co-design for k-mer counting, one of the most time-consuming phases of genomic applications. The proposed co-design targets CAPI-enabled FPGAs and is integrated into SMUFIN, a state-of-the-art reference-free method for finding DNA mutations. Results show that the proposed co-design outperforms the CPU-only design by a factor of 2.14×, it consumes 2.93× less energy, and it requires 1.57× less memory.},
	booktitle = {2020 30th {International} {Conference} on {Field}-{Programmable} {Logic} and {Applications} ({FPL})},
	author = {Haghi, Abbas and Alvarez, Lluc and Polo, Jordà and Diamantopoulos, Dionysios and Hagleitner, Christoph and Moreto, Miquel},
	month = aug,
	year = {2020},
	note = {ISSN: 1946-1488},
	pages = {57--64},
}

@article{tichy_end_2016,
	title = {The {End} of ({Numeric}) {Error}: {An} interview with {John} {L}. {Gustafson}},
	volume = {2016},
	shorttitle = {The {End} of ({Numeric}) {Error}},
	url = {https://dl.acm.org/doi/10.1145/2913029},
	doi = {10.1145/2913029},
	abstract = {Crunching numbers was the prime task of early computers. The common element of these early computers is they all used integer arithmetic. John Gustafson, one of the foremost experts in scientific computing, has proposed a new number format that provides more accurate answers than standard floats, yet saves space and energy. The new format might well revolutionize the way we do numerical calculations.},
	number = {April},
	journal = {Ubiquity},
	author = {Tichy, Walter},
	month = apr,
	year = {2016},
	pages = {1:1--1:14},
	file = {Full Text PDF:/home/binaryman/Zotero/storage/Q8IXG4KA/Tichy - 2016 - The End of (Numeric) Error An interview with John.pdf:application/pdf},
}

@article{lawson_basic_1979,
	title = {Basic {Linear} {Algebra} {Subprograms} for {Fortran} {Usage}},
	volume = {5},
	issn = {0098-3500},
	url = {https://dl.acm.org/doi/10.1145/355841.355847},
	doi = {10.1145/355841.355847},
	number = {3},
	journal = {ACM Trans. Math. Softw.},
	author = {Lawson, C. L. and Hanson, R. J. and Kincaid, D. R. and Krogh, F. T.},
	month = sep,
	year = {1979},
	pages = {308--323},
}

@article{dongarra_set_1990,
	title = {A set of level 3 basic linear algebra subprograms},
	volume = {16},
	issn = {0098-3500},
	url = {https://dl.acm.org/doi/10.1145/77626.79170},
	doi = {10.1145/77626.79170},
	abstract = {This paper describes an extension to the set of Basic Linear Algebra Subprograms. The extensions are targeted at matrix-vector operations that should provide for efficient and portable implementations of algorithms for high-performance computers},
	number = {1},
	journal = {ACM Trans. Math. Softw.},
	author = {Dongarra, J. J. and Du Croz, Jeremy and Hammarling, Sven and Duff, I. S.},
	month = mar,
	year = {1990},
	pages = {1--17},
}

@article{van_der_walt_numpy_2011,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	issn = {1558-366X},
	shorttitle = {The {NumPy} {Array}},
	url = {https://ieeexplore.ieee.org/document/5725236},
	doi = {10.1109/MCSE.2011.37},
	abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
	number = {2},
	journal = {Computing in Science \& Engineering},
	author = {van der Walt, Stefan and Colbert, S. Chris and Varoquaux, Gael},
	month = mar,
	year = {2011},
	note = {Conference Name: Computing in Science \& Engineering},
	pages = {22--30},
}
@BOOK{lapack99,
	AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
	Blackford, S. and Demmel, J. and Dongarra, J. and
	Du Croz, J. and Greenbaum, A. and Hammarling, S. and
	McKenney, A. and Sorensen, D.},
	TITLE = {{LAPACK} Users' Guide},
	EDITION = {Third},
	PUBLISHER = {Society for Industrial and Applied Mathematics},
	YEAR = {1999},
	ADDRESS = {Philadelphia, PA},
	ISBN = {0-89871-447-8 (paperback)}
}

@book{dongarra1979linpack,
  title={LINPACK users' guide},
  author={Dongarra, Jack J and Moler, Cleve Barry and Bunch, James R and Stewart, Gilbert W},
  year={1979},
  publisher={SIAM}
}

@article{goto_anatomy_2008,
	title = {Anatomy of high-performance matrix multiplication},
	volume = {34},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/1356052.1356053},
	doi = {10.1145/1356052.1356053},
	abstract = {We present the basic principles that underlie the high-performance implementation of the matrix-matrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve near-peak performance.},
	number = {3},
	journal = {ACM Trans. Math. Softw.},
	author = {Goto, Kazushige and Geijn, Robert A. van de},
	month = may,
	year = {2008},
	keywords = {basic linear algebra subprogrms, Linear algebra, matrix multiplication},
	pages = {12:1--12:25},
}

@incollection{wang_intel_2014,
	address = {Cham},
	title = {Intel {Math} {Kernel} {Library}},
	isbn = {978-3-319-06486-4},
	url = {https://doi.org/10.1007/978-3-319-06486-4_7},
	abstract = {In order to achieve optimal performance on multi-core and multi-processor systems, we need to fully use the features of parallelism and manage the memory hierarchical characters efficiently. The performance of sequential codes relies on the instruction-level and register-level SIMD parallelism, and also on high-speed cache-blocking functions. Threading applications need advanced planning to achieve satisfactory load balancing.},
	language = {en},
	booktitle = {High-{Performance} {Computing} on the {Intel}® {Xeon} {Phi}™: {How} to {Fully} {Exploit} {MIC} {Architectures}},
	publisher = {Springer International Publishing},
	author = {Wang, Endong and Zhang, Qing and Shen, Bo and Zhang, Guangyong and Lu, Xiaowei and Wu, Qing and Wang, Yajuan},
	editor = {Wang, Endong and Zhang, Qing and Shen, Bo and Zhang, Guangyong and Lu, Xiaowei and Wu, Qing and Wang, Yajuan},
	year = {2014},
	doi = {10.1007/978-3-319-06486-4_7},
	keywords = {Audio Signal Processing, Discrete Fourier Transform, Fast Fourier Transform, Head File, Linear Algebra Operation},
	pages = {167--188},
}

@article{clint_whaley_automated_2001,
	series = {New {Trends} in {High} {Performance} {Computing}},
	title = {Automated empirical optimizations of software and the {ATLAS} project},
	volume = {27},
	issn = {0167-8191},
	url = {https://www.sciencedirect.com/science/article/pii/S0167819100000879},
	doi = {10.1016/S0167-8191(00)00087-9},
	abstract = {This paper describes the automatically tuned linear algebra software (ATLAS) project, as well as the fundamental principles that underly it. ATLAS is an instantiation of a new paradigm in high performance library production and maintenance, which we term automated empirical optimization of software (AEOS); this style of library management has been created in order to allow software to keep pace with the incredible rate of hardware advancement inherent in Moore's Law. ATLAS is the application of this new paradigm to linear algebra software, with the present emphasis on the basic linear algebra subprograms (BLAS), a widely used, performance-critical, linear algebra kernel library.},
	journal = {Parallel Computing},
	author = {Clint Whaley, R. and Petitet, Antoine and Dongarra, Jack J.},
	month = jan,
	year = {2001},
	keywords = {AEOS, ATLAS, BLAS, Portable performance},
	pages = {3--35},
}

@misc{cuBLAS_2022,
  author={NVIDIA and Vingelmann, Péter and Fitzek, Frank H.P.},
  title={CUDA, release: 10.2.89},
  year={2020},
  url={https://developer.nvidia.com/cuda-toolkit},
}

@article{goto_high-performance_2008,
	title = {High-performance implementation of the level-3 {BLAS}},
	volume = {35},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/1377603.1377607},
	doi = {10.1145/1377603.1377607},
	abstract = {A simple but highly effective approach for transforming high-performance implementations on cache-based architectures of matrix-matrix multiplication into implementations of other commonly used matrix-matrix computations (the level-3 BLAS) is presented. Exceptional performance is demonstrated on various architectures.},
	number = {1},
	journal = {ACM Trans. Math. Softw.},
	author = {Goto, Kazushige and Van De Geijn, Robert},
	month = jul,
	year = {2008},
	keywords = {basic linear algebra subprograms, libraries, Linear algebra, matrix-matrix operations},
	pages = {4:1--4:14},
}

@inproceedings{donthi_survey_2003,
	title = {A survey of dynamically reconfigurable {FPGA} devices},
	url = {https://ieeexplore.ieee.org/abstract/document/1194605},
	doi = {10.1109/SSST.2003.1194605},
	abstract = {The FPGA market has evolved at an extremely rapid pace, with larger and faster devices being released to the industry by different vendors. One has to select a target FPGA device that suits one's application. This paper presents a survey of currently available dynamically reconfigurable FPGA devices. The devices were: the Atmel AT40k family, the Xilinx Virtex family, the Lattice Semiconductors ORCA and ispXPGA families, and the Altera APEX20k family. The degree of dynamic reconfigurability of these devices was compared based on partial or full reconfiguration. The architectures of these devices were evaluated based on the granularity and reconfiguration time.},
	booktitle = {Proceedings of the 35th {Southeastern} {Symposium} on {System} {Theory}, 2003.},
	author = {Donthi, S. and Haggard, R.L.},
	month = mar,
	year = {2003},
	note = {ISSN: 0094-2898},
	pages = {422--426},
}

@article{mencer_history_2020,
	title = {The {History}, {Status}, and {Future} of {FPGAs}: {Hitting} a nerve with field-programmable gate arrays},
	volume = {18},
	issn = {1542-7730},
	shorttitle = {The {History}, {Status}, and {Future} of {FPGAs}},
	url = {https://dl.acm.org/doi/10.1145/3411757.3411759},
	doi = {10.1145/3411757.3411759},
	abstract = {This article is a summary of a three-hour discussion at Stanford University in September 2019 among the authors. It has been written with combined experiences at and with organizations such as Zilog, Altera, Xilinx, Achronix, Intel, IBM, Stanford, MIT, Berkeley, University of Wisconsin, the Technion, Fairchild, Bell Labs, Bigstream, Google, DIGITAL (DEC), SUN, Nokia, SRI, Hitachi, Silicom, Maxeler Technologies, VMware, Xerox PARC, Cisco, and many others. These organizations are not responsible for the content, but may have inspired the authors in some ways, to arrive at the colorful ride through FPGA space described above.},
	number = {3},
	journal = {Queue},
	author = {Mencer, Oskar and Allison, Dennis and Blatt, Elad and Cummings, Mark and Flynn, Michael J. and Harris, Jerry and Hewitt, Carl and Jacobson, Quinn and Lavasani, Maysam and Moazami, Mohsen and Murray, Hal and Nikravesh, Masoud and Nowatzyk, Andreas and Shand, Mark and Shirazi, Shahram},
	month = jul,
	year = {2020},
	pages = {Pages 10:71--Pages 10:82},
}

@article{chan_altera_2009,
	title = {Altera {EP300} {Design} \& {Development} {Oral} {History} {Panel}},
	language = {en},
	author = {Chan, Yiu-Fai and Frankovich, Robert and Hartmann, Robert and McCarthy, Clive and Wong, Don},
	year = {2009},
}

@article{galli_xcell_99,
	title = {Xcell: the quaterly},
	language = {en},
	author = {Galli, Dave and Seither, Mike and Alfke, Peter},
	year = {1999}
}

@misc{ieeespectrum_chip_2017,
	author = {IEEE Spectrum},
	year = {2017},
	title = {Chip {Hall} of {Fame}: {Xilinx} {XC2064} {FPGA} - {IEEE} {Spectrum}},
	shorttitle = {Chip {Hall} of {Fame}},
	url = {https://spectrum.ieee.org/chip-hall-of-fame-xilinx-xc2064-fpga},
	abstract = {Hardware that can transform itself on command has proven incredibly useful},
	language = {en},
}

@inproceedings{trimberger_effects_1995,
	address = {New York, NY, USA},
	series = {{DAC} '95},
	title = {Effects of {FPGA} architecture on {FPGA} routing},
	isbn = {978-0-89791-725-4},
	url = {https://dl.acm.org/doi/10.1145/217474.217592},
	doi = {10.1145/217474.217592},
	booktitle = {Proceedings of the 32nd annual {ACM}/{IEEE} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Trimberger, Stephen},
	month = jan,
	year = {1995},
	pages = {574--578},
}

@inproceedings{michael_j_alexander_new_1995,
	title = {New {Performance}-{Driven} {FPGA} {Routing} {Algorithms}},
	url = {https://ieeexplore.ieee.org/document/1586766},
	doi = {10.1109/DAC.1995.250010},
	abstract = {Motivated by the goal of increasing the performance of FPGA-based designs, we propose effective Steiner and arborescence FPGA routing algorithms. Our graph-based Steiner tree constructions have provably-good performance bounds and outperform the best known ones in practice, while our arborescence heuristics produce routing solutions with optimal source-sink pathlengths at a reasonably low wirelength penalty. We have incorporated our algorithms into an actual FPGA router which routed a number of industrial circuits using channel widths considerably smaller than was previously possible.},
	booktitle = {32nd {Design} {Automation} {Conference}},
	author = {Michael J. Alexander, Gabriel Robins},
	month = jun,
	year = {1995},
	note = {ISSN: 0738-100X},
	pages = {562--567},
}

@inproceedings{betz_vpr_1997,
	address = {Berlin, Heidelberg},
	series = {{FPL} '97},
	title = {{VPR}: {A} new packing, placement and routing tool for {FPGA} research},
	isbn = {978-3-540-63465-2},
	shorttitle = {{VPR}},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Field}-{Programmable} {Logic} and {Applications}},
	publisher = {Springer-Verlag},
	author = {Betz, Vaughn and Rose, Jonathan},
	month = sep,
	year = {1997},
	pages = {213--222},
}

@inproceedings{siddiqi_faster_2022,
	address = {New York, NY, USA},
	series = {{MLCAD} '22},
	title = {Faster {FPGA} {Routing} by {Forecasting} and {Pre}-{Loading} {Congestion} {Information}},
	isbn = {978-1-4503-9486-4},
	url = {https://doi.org/10.1145/3551901.3556492},
	doi = {10.1145/3551901.3556492},
	abstract = {Field Programmable Gate Array (FPGA) routing is one of the most time consuming tasks within the FPGA design flow, requiring hours and even days to complete for some large industrial designs. This is becoming a major concern for FPGA users and tool developers. This paper proposes a simple, yet effective, framework that reduces the runtime of PathFinder based routers. A supervised Machine Learning (ML) algorithm is developed to forecast costs (from the placement phase) associated with possible congestion and hot spot creation in the routing phase. These predicted costs are used to guide the router to avoid highly congested regions while routing nets, thus reducing the total number of iterations and rip-up and reroute operations involved. Results obtained indicate that the proposed ML approach achieves on average a 43 reduction in the number of routing iterations and 28.6 reduction in runtime when implemented in the state-of-the-art enhanced PathFinder algorithm.},
	booktitle = {Proceedings of the 2022 {ACM}/{IEEE} {Workshop} on {Machine} {Learning} for {CAD}},
	publisher = {Association for Computing Machinery},
	author = {Siddiqi, Umair and Martin, Timothy and Van Den Eijnden, Sam and Shamli, Ahmed and Grewal, Gary and Sait, Sadiq and Areibi, Shawki},
	month = sep,
	year = {2022},
	keywords = {congestion, FPGA routing, supervised machine learning},
	pages = {15--20},
}

@article{zhou_rwroute_2021,
	title = {{RWRoute}: {An} {Open}-source {Timing}-driven {Router} for {Commercial} {FPGAs}},
	volume = {15},
	issn = {1936-7406},
	shorttitle = {{RWRoute}},
	url = {https://doi.org/10.1145/3491236},
	doi = {10.1145/3491236},
	abstract = {One of the key obstacles to pervasive deployment of FPGA accelerators in data centers is their cumbersome programming model. Open source tooling is suggested as a way to develop alternative EDA tools to remedy this issue. Open source FPGA CAD tools have traditionally targeted academic hypothetical architectures, making them impractical for commercial devices. Recently, there have been efforts to develop open source back-end tools targeting commercial devices. These tools claim to follow an alternate data-driven approach that allows them to be more adaptable to the domain requirements such as faster compile time. In this paper, we present RWRoute, the first open source timing-driven router for UltraScale+ devices. RWRoute is built on the RapidWright framework and includes the essential and pragmatic features found in commercial FPGA routers that are often missing from open source tools. Another valuable contribution of this work is an open-source lightweight timing model with high fidelity timing approximations. By leveraging a combination of architectural knowledge, repeating patterns, and extensive analysis of Vivado timing reports, we obtain a slightly pessimistic, lumped delay model within 2\% average accuracy of Vivado for UltraScale+ devices. Compared to Vivado, RWRoute results in a 4.9× compile time improvement at the expense of 10\% Quality of Results (QoR) loss for 665 synthetic and six real designs. A main benefit of our router is enabling fast partial routing at the back-end of a domain-specific flow. Our initial results indicate that more than 9× compile time improvement is achievable for partial routing. The results of this paper show how such a router can be beneficial for a low touch flow to reduce dependency on commercial tools.},
	number = {1},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	author = {Zhou, Yun and Maidee, Pongstorn and Lavin, Chris and Kaviani, Alireza and Stroobandt, Dirk},
	month = nov,
	year = {2021},
	keywords = {commercial FPGAs, FPGA routing, RapidWright, timing model, timing-driven, Ultrascale+},
	pages = {8:1--8:27},
}

@inproceedings{lemieux_analytical_2002,
	address = {Berlin, Heidelberg},
	series = {{FPL} '02},
	title = {Analytical {Framework} for {Switch} {Block} {Design}},
	isbn = {978-3-540-44108-3},
	abstract = {One popular FPGA interconnection network is based on the islandstyle model, where rows and columns of logic blocks are separated by channels containing routing wires. Switch blocks are placed at the intersections of the horizontal and vertical channels to allow the wires to be connected together. Previous switch block design has focused on the analysis of individual switch blocks or the use of ad hoc design with experimental evaluation. This paper presents an analytical framework which considers the design of a continuous fabric of switch blocks containing wire segments of any length. The framework is used to design new switch blocks which are experimentally shown to be as effective as the best ones known to date.With this framework, we hope to inspire new ways of looking at switch block design.},
	booktitle = {Proceedings of the {Reconfigurable} {Computing} {Is} {Going} {Mainstream}, 12th {International} {Conference} on {Field}-{Programmable} {Logic} and {Applications}},
	publisher = {Springer-Verlag},
	author = {Lemieux, Guy G. and Lewis, David M.},
	month = sep,
	year = {2002},
	pages = {122--131},
}

@inproceedings{sano_fpga-based_2017,
	address = {New York, NY, USA},
	series = {{HEART} '17},
	title = {{FPGA}-based {Stream} {Computing} for {High}-{Performance} {N}-{Body} {Simulation} using {Floating}-{Point} {DSP} {Blocks}},
	isbn = {978-1-4503-5316-8},
	url = {https://doi.org/10.1145/3120895.3120909},
	doi = {10.1145/3120895.3120909},
	abstract = {Recent advancement of FPGAs allows high-performance and low-power computing by constructing deeply-pipelined custom hardware using floating-point DSP blocks. In this paper, we present a stream-computing architecture and design for FPGA-based high-performance N-body simulation, which is different from the parallel-computing-and-reduction approach of the GRAPE systems, which are predecessors of custom N-body machines. The proposed architecture is composed of a force-pipeline module (FPM) and an integral-pipeline module (IPM). FPM has a scalable structure based on n cascade-connected pairs of computing elements (CEs) and streamed register files (SRFs) so that we can scale the performance by increasing n. We also present the performance model. The measure performance of the system prototyped with a single Arria10 FPGA has good agreement with the model, and scales well with n at a higher efficiency when the problem size is large. We demonstrate that the system with n = 64 CEs operating at 180 MHz achieves 10944 MFCPS (million force calculation per second) for N = 262144 particles.},
	booktitle = {Proceedings of the 8th {International} {Symposium} on {Highly} {Efficient} {Accelerators} and {Reconfigurable} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Sano, Kentaro and Abiko, Shin and Ueno, Tomohiro},
	month = jun,
	year = {2017},
	keywords = {Arria10 FPGA, custom computing, floating-point DSP blocks, high-performance computation, N-body simulation, stream computing},
	pages = {1--6},
}

@article{sano_fpga-based_2017-1,
	title = {{FPGA}-{Based} {Scalable} and {Power}-{Efficient} {Fluid} {Simulation} using {Floating}-{Point} {DSP} {Blocks}},
	volume = {28},
	issn = {1558-2183},
	url = {https://ieeexplore.ieee.org/abstract/document/7893769},
	doi = {10.1109/TPDS.2017.2691770},
	abstract = {High-performance and low-power computation is required for large-scale fluid dynamics simulation. Due to the inefficient architecture and structure of CPUs and GPUs, they now have a difficulty in improving power efficiency for the target application. Although FPGAs become promising alternatives for power-efficient and high-performance computation due to their new architecture having floating-point (FP) DSP blocks, their relatively narrow memory bandwidth requires an appropriate way to fully exploit the advantage. This paper presents an architecture and design for scalable fluid simulation based on data-flow computing with a state-of-the-art FPGA. To exploit available hardware resources including FP DSPs, we introduce spatial and temporal parallelism to further scale the performance by adding more stream processing elements (SPEs) in an array. Performance modeling and prototype implementation allow us to explore the design space for both the existing Altera Arria10 and the upcoming Intel Stratix10 FPGAs. We demonstrate that Arria10 10AX115 FPGA achieves 519 GFlops at 9.67 GFlops/W only with a stream bandwidth of 9.0 GB/s, which is 97.9 percent of the peak performance of 18 implemented SPEs. We also estimate that Stratix10 FPGA can scale up to 6844 GFlops by combining spatial and temporal parallelism adequately.},
	number = {10},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Sano, Kentaro and Yamamoto, Satoru},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	pages = {2823--2837},
	file = {IEEE Xplore Abstract Record:/home/binaryman/Zotero/storage/9CNS2CZF/7893769.html:text/html},
}

@article{ajayi2019openroad,
  title={OpenROAD: Toward a Self-Driving, Open-Source Digital Layout Implementation Tool Chain},
  author={Ajayi, T and Blaauw, D and Chan, TB and Cheng, CK and Chhabria, VA and Choo, DK and Coltella, M and Dobre, S and Dreslinski, R and Foga{\c{c}}a, M and others},
  journal={Proc. GOMACTECH},
  pages={1105--1110},
  year={2019}
}

@inproceedings{ajayi2019toward,
  title={Toward an open-source digital flow: First learnings from the openroad project},
  author={Ajayi, Tutu and Chhabria, Vidya A and Foga{\c{c}}a, Mateus and Hashemi, Soheil and Hosny, Abdelrahman and Kahng, Andrew B and Kim, Minsoo and Lee, Jeongsup and Mallappa, Uday and Neseem, Marina and others},
  booktitle={Proceedings of the 56th Annual Design Automation Conference 2019},
  pages={1--4},
  year={2019}
}

@article{clark_asap7_2016,
	title = {{ASAP7}: {A} 7-nm {finFET} predictive process design kit},
	volume = {53},
	issn = {0026-2692},
	shorttitle = {{ASAP7}},
	url = {https://www.sciencedirect.com/science/article/pii/S002626921630026X},
	doi = {10.1016/j.mejo.2016.04.006},
	abstract = {We describe a 7-nm predictive process design kit (PDK) called the ASAP7 PDK, developed in collaboration with ARM Ltd. for academic use. The PDK is realistic, based on current assumptions for the 7-nm technology node, but is not tied to any specific foundry. The initial version assumes EUV lithography for key layers, a decision based on its present near cost-effectiveness and resulting simpler layout rules. Non-EUV layers assume appropriate multiple patterning schemes, i.e., self-aligned quadruple patterning (SAQP), self-aligned double patterning (SADP) or litho-etch litho-etch (LELE), based on 193-nm optical immersion lithography. The specific design rule derivation is explained for key layers at the front end of line (FEOL), middle of line (MOL), and back end of line (BEOL) of the predictive process modeled. The MOL and BEOL DRC rules rely on estimation of time dependent dielectric breakdown requirements using layer alignments determined with projected machine to machine overlay assumptions, with significant guard-bands where possible. A high density, low-power standard cell architecture, developed using design/technology co-optimization (DTCO), as well as example SRAM cells are shown. The PDK transistor electrical assumptions are also explained, as are the FEOL design rules, and the models include basic design corners. The transistor models support four threshold voltage (Vth) levels for both NMOS and PMOS transistors. Cadence Virtuoso technology files and associated schematic and layout editing, as well as netlisting are supported. DRC, LVS, and full parasitic extraction is enabled through Mentor Calibre decks.},
	journal = {Microelectronics Journal},
	author = {Clark, Lawrence T. and Vashishtha, Vinay and Shifren, Lucian and Gujja, Aditya and Sinha, Saurabh and Cline, Brian and Ramamurthy, Chandarasekaran and Yeric, Greg},
	month = jul,
	year = {2016},
	keywords = {7-nm technology, Design rules, Extreme ultraviolet lithography, Predictive process design kit, Process scaling, Self-aligned multiple patterning},
	pages = {105--115},
}

@inproceedings{vashishtha_design_2017,
	title = {Design technology co-optimization of back end of line design rules for a 7 nm predictive process design kit},
	url = {https://ieeexplore.ieee.org/document/7918308},
	doi = {10.1109/ISQED.2017.7918308},
	abstract = {This paper discusses the back-end-of-line (BEOL) layers for a 7 nm predictive process design kit (PDK). The rationale behind choosing a particular lithographic process-EUV lithography, self-aligned double patterning (SADP), and litho-etch litho-etch (LELE)-for different layers, in addition to some design rule values, is described. The rules are based on the literature and on design technology co-optimization (DTCO) evaluation of standard cell based designs and automated place-and-route experiments. Decomposition criteria and design rules to ensure conflict-free coloring of SADP metal topologies and manufacturable SADP photolithography masks are discussed in detail. Their efficacy is demonstrated through successful coloring and photolithography mask derivation for target metal shape layouts, which represent corner cases, by using the Mentor Graphics Calibre and multi-patterning tools. Edge placement errors, misalignment, and critical dimension uniformity are included in the analysis.},
	booktitle = {2017 18th {International} {Symposium} on {Quality} {Electronic} {Design} ({ISQED})},
	author = {Vashishtha, Vinay and Dosi, Ankita and Masand, Lovish and Clark, Lawrence T.},
	month = mar,
	year = {2017},
	note = {ISSN: 1948-3287},
	keywords = {Back end of line, design rules, EUV lithography, FinFETs, LELE, Lithography, Metals, multi-patterning, Pins, Routing, SADP, Standards, Ultraviolet sources},
	pages = {149--154},
}

@inproceedings{vashishtha_robust_2017,
	title = {Robust 7-nm {SRAM} design on a predictive {PDK}},
	url = {https://ieeexplore.ieee.org/document/8050316},
	doi = {10.1109/ISCAS.2017.8050316},
	abstract = {SRAMs are ubiquitous in modern VLSI design but have become difficult to design in advanced finFET processes due to fin quantization and large variability at small geometries. In this paper six transistor SRAM design on a 7-nm predictive PDK is presented. The SRAMs use differential sense amplifier based sensing to support long bit-lines and high array efficiency. Different SRAM cells are evaluated statistically, resulting in the choice of a 122 cell due to its easier lithography and superior write margins. A novel switched capacitor reduced column VDD is presented, which has excellent across corner voltage characteristics and speed. The analysis shows yield to a minimum VDD of 500 mV.},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Vashishtha, Vinay and Vangala, Manoj and Sharma, Parv and Clark, Lawrence T.},
	month = may,
	year = {2017},
	note = {ISSN: 2379-447X},
	keywords = {finFET, Organizations, Random access memory, SRAM Statistical Design, Statistical analysis, Timing, Very large scale integration, Write Assist},
	pages = {1--4},
}

@book{thomas_verilog_2008,
	edition = {5th ed.},
	title = {The {Verilog} {Hardware} {Description} {Language}},
	isbn = {978-0-387-84930-0},
	abstract = {Thomas \& Moorbys The Verilog Hardware Description Language has become the standard reference text for Verilog. This edition presents the new IEEE 1364-2001 standard of the language. The examples have all been updated to illustrate the new features of the language. A cross referenced guide to the new and old features is provided. Thus, designers already familiar with Verilog can quickly learn the new features. Newcomers to the language can use it as a guide for reading old specifications. The Verilog Hardware Description Language, Fifth Edition, is a valuable resource for engineers and students interested in describing, simulating, and synthesizing digital systems; the extensive number of simulatable examples and wide range of representation styles covered ensure its quick use in design. The book is also ready for use in university courses, having been used for introductory logic design and simulation through advanced VLSI design courses. An appendix with tutorial help and a work-along style is keyed into the introduction for new students. Material supporting a computer-aided design course on the inner working of simulators is also included.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Thomas, Donald and Moorby, Philip},
	month = sep,
	year = {2008},
}

@book{palnitkar_verilog_1996,
	address = {USA},
	title = {Verilog {HDL}: a guide to digital design and synthesis},
	isbn = {978-0-13-451675-2},
	shorttitle = {Verilog {HDL}},
	publisher = {Prentice-Hall, Inc.},
	author = {Palnitkar, Samir},
	month = jan,
	year = {1996},
}

@article{ieee_standard_2019,
	title = {{IEEE} {Standard} for {VHDL} {Language} {Reference} {Manual}},
	author = {IEEE},
	url = {https://ieeexplore.ieee.org/document/8938196},
	doi = {10.1109/IEEESTD.2019.8938196},
	abstract = {VHSIC Hardware Description Language (VHDL) is defined. VHDL is a formal notation intended for use in all phases of the creation of electronic systems. Because it is both machine readable and human readable, it supports the development, verification, synthesis, and testing of hardware designs; the communication of hardware design data; and the maintenance, modification, and procurement of hardware. Its primary audiences are the implementors of tools supporting the language and the advanced users of the language.},
	journal = {IEEE Std 1076-2019},
	month = dec,
	year = {2019},
	note = {Conference Name: IEEE Std 1076-2019},
	keywords = {Hardware design languages, computer languages, Computer languages, electronic systems, hardware, hardware design, IEEE 1076(TM), IEEE Standards, VHDL},
	pages = {1--673},
}

@article{de_haro_ompssfpga_2021,
	title = {{OmpSs}@{FPGA} {Framework} for {High} {Performance} {FPGA} {Computing}},
	volume = {70},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/document/9445632},
	doi = {10.1109/TC.2021.3086106},
	abstract = {This article presents the new features of the OmpSs@FPGA framework. OmpSs is a data-flow programming model that supports task nesting and dependencies to target asynchronous parallelism and heterogeneity. OmpSs@FPGA is the extension of the programming model addressed specifically to FPGAs. OmpSs environment is built on top of Mercurium source to source compiler and Nanos++ runtime system. To address FPGA specifics Mercurium compiler implements several FPGA related features as local variable caching, wide memory accesses or accelerator replication. In addition, part of the Nanos++ runtime has been ported to hardware. Driven by the compiler this new hardware runtime adds new features to FPGA codes, such as task creation and dependence management, providing both performance increases and ease of programming. To demonstrate these new capabilities, different high performance benchmarks have been evaluated over different FPGA platforms using the OmpSs programming model. The results demonstrate that programs that use the OmpSs programming model achieve very competitive performance with low to moderate porting effort compared to other FPGA implementations.},
	number = {12},
	journal = {IEEE Transactions on Computers},
	author = {de Haro, Juan Miguel and Bosch, Jaume and Filgueras, Antonio and Vidal, Miquel and Jiménez-González, Daniel and Álvarez, Carlos and Martorell, Xavier and Ayguadé, Eduard and Labarta, Jesús},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Field programmable gate arrays, Random access memory, Hardware, FPGA, high-level synthesis, parallel architectures, Programming, Reconfigurable devices, reconfigurable hardware, Runtime, Task analysis, task-based programming models},
	pages = {2029--2042},
	file = {Full Text:/home/lledoux/Zotero/storage/VEEECHZ8/de Haro et al. - 2021 - OmpSs@FPGA Framework for High Performance FPGA Com.pdf:application/pdf;IEEE Xplore Abstract Record:/home/lledoux/Zotero/storage/JZKIVCEM/9445632.html:text/html},
}

@inproceedings{bosch_exploiting_2017,
	address = {New York, NY, USA},
	series = {{ANDARE} '17},
	title = {Exploiting {Parallelism} on {GPUs} and {FPGAs} with {OmpSs}},
	isbn = {978-1-4503-5363-2},
	url = {https://doi.org/10.1145/3152821.3152880},
	doi = {10.1145/3152821.3152880},
	abstract = {This paper presents the OmpSs approach to deal with heterogeneous programming on GPU and FPGA accelerators. The OmpSs programming model is based on the Mercurium compiler and the Nanos++ runtime. Applications are annotated with compiler directives specifying task-based parallelism. The Mercurium compiler transforms the code to exploit the parallelism in the SMP host cores, and also to spawn work on CUDA/OpenCL devices, and FPGA accelerators. For the CUDA/OpenCL devices, the programmer needs only to insert the annotations and provide the kernel function to be compiled by the native CUDA/OpenCL compiler. In the case of the FPGAs, OmpSs uses the High-Level Synthesis tools from FPGA vendors to generate the IP configurations for the FPGA. In this paper we present the performance obtained on the matrix multiply benchmark in the Xilinx Zynq Ultrascale+, as a result of using OmpSs on this benchmark.},
	booktitle = {Proceedings of the 1st {Workshop} on {AutotuniNg} and {aDaptivity} {AppRoaches} for {Energy} efficient {HPC} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bosch, Jaume and Filgueras, Antonio and Vidal, Miquel and Jimenez-Gonzalez, Daniel and Alvarez, Carlos and Martorell, Xavier},
	month = sep,
	year = {2017},
	keywords = {GPU, FPGA, OmpSs Programming Model, Parallelism},
	pages = {1--5},
	file = {Submitted Version:/home/lledoux/Zotero/storage/IC896N8S/Bosch et al. - 2017 - Exploiting Parallelism on GPUs and FPGAs with OmpS.pdf:application/pdf},
}

@inproceedings{fernandez_task-based_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Task-{Based} {Programming} with {OmpSs} and {Its} {Application}},
	isbn = {978-3-319-14313-2},
	doi = {10.1007/978-3-319-14313-2_51},
	abstract = {OmpSs is a task-based programming model that aims to provide portability and flexibility for sequential codes while the performance is achieved by the dynamic exploitation of the parallelism at task level. OmpSs targets the programming of heterogeneous and multi-core architectures and offers asynchronous parallelism in the execution of the tasks. The main extension of OmpSs, now incorporated in the recent OpenMP 4.0 standard, is the concept of data dependences between tasks.},
	language = {en},
	booktitle = {Euro-{Par} 2014: {Parallel} {Processing} {Workshops}},
	publisher = {Springer International Publishing},
	author = {Fernández, Alejandro and Beltran, Vicenç and Martorell, Xavier and Badia, Rosa M. and Ayguadé, Eduard and Labarta, Jesus},
	editor = {Lopes, Luís and Žilinskas, Julius and Costan, Alexandru and Cascella, Roberto G. and Kecskemeti, Gabor and Jeannot, Emmanuel and Cannataro, Mario and Ricci, Laura and Benkner, Siegfried and Petit, Salvador and Scarano, Vittorio and Gracia, José and Hunold, Sascha and Scott, Stephen L. and Lankes, Stefan and Lengauer, Christian and Carretero, Jesús and Breitbart, Jens and Alexander, Michael},
	year = {2014},
	keywords = {Compiler Framework, Heterogeneous Computing, OpenMP Standard, Programming Model, Task Graph},
	pages = {601--612},
	file = {Full Text PDF:/home/lledoux/Zotero/storage/DADGCHNG/Fernández et al. - 2014 - Task-Based Programming with OmpSs and Its Applicat.pdf:application/pdf},
}

@article{dagum_openmp_1998,
	title = {{OpenMP}: an industry standard {API} for shared-memory programming},
	volume = {5},
	issn = {1558-190X},
	shorttitle = {{OpenMP}},
	url = {https://ieeexplore.ieee.org/document/660313},
	doi = {10.1109/99.660313},
	abstract = {At its most elemental level, OpenMP is a set of compiler directives and callable runtime library routines that extend Fortran (and separately, C and C++ to express shared memory parallelism. It leaves the base language unspecified, and vendors can implement OpenMP in any Fortran compiler. Naturally, to support pointers and allocatables, Fortran 90 and Fortran 95 require the OpenMP implementation to include additional semantics over Fortran 77. OpenMP leverages many of the X3H5 concepts while extending them to support coarse grain parallelism. The standard also includes a callable runtime library with accompanying environment variables.},
	number = {1},
	journal = {IEEE Computational Science and Engineering},
	author = {Dagum, L. and Menon, R.},
	month = jan,
	year = {1998},
	note = {Conference Name: IEEE Computational Science and Engineering},
	keywords = {Parallel processing, Coherence, Hardware, Computer architecture, ANSI standards, Message passing, Parallel programming, Power system modeling, Scalability, Software systems},
	pages = {46--55},
	file = {IEEE Xplore Abstract Record:/home/lledoux/Zotero/storage/ETAHY86B/660313.html:text/html},
}

@misc{kadosh_quantifying_2023,
	title = {Quantifying {OpenMP}: {Statistical} {Insights} into {Usage} and {Adoption}},
	shorttitle = {Quantifying {OpenMP}},
	url = {http://arxiv.org/abs/2308.08002},
	doi = {10.48550/arXiv.2308.08002},
	abstract = {In high-performance computing (HPC), the demand for efficient parallel programming models has grown dramatically since the end of Dennard Scaling and the subsequent move to multi-core CPUs. OpenMP stands out as a popular choice due to its simplicity and portability, offering a directive-driven approach for shared-memory parallel programming. Despite its wide adoption, however, there is a lack of comprehensive data on the actual usage of OpenMP constructs, hindering unbiased insights into its popularity and evolution. This paper presents a statistical analysis of OpenMP usage and adoption trends based on a novel and extensive database, HPCORPUS, compiled from GitHub repositories containing C, C++, and Fortran code. The results reveal that OpenMP is the dominant parallel programming model, accounting for 45\% of all analyzed parallel APIs. Furthermore, it has demonstrated steady and continuous growth in popularity over the past decade. Analyzing specific OpenMP constructs, the study provides in-depth insights into their usage patterns and preferences across the three languages. Notably, we found that while OpenMP has a strong "common core" of constructs in common usage (while the rest of the API is less used), there are new adoption trends as well, such as simd and target directives for accelerated computing and task for irregular parallelism. Overall, this study sheds light on OpenMP's significance in HPC applications and provides valuable data for researchers and practitioners. It showcases OpenMP's versatility, evolving adoption, and relevance in contemporary parallel programming, underlining its continued role in HPC applications and beyond. These statistical insights are essential for making informed decisions about parallelization strategies and provide a foundation for further advancements in parallel programming models and techniques.},
	publisher = {arXiv},
	author = {Kadosh, Tal and Hasabnis, Niranjan and Mattson, Timothy and Pinter, Yuval and Oren, Gal},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08002 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
}

@article{stone_opencl_2010,
	title = {{OpenCL}: {A} {Parallel} {Programming} {Standard} for {Heterogeneous} {Computing} {Systems}},
	volume = {12},
	issn = {1558-366X},
	shorttitle = {{OpenCL}},
	url = {https://ieeexplore.ieee.org/document/5457293},
	doi = {10.1109/MCSE.2010.69},
	abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Stone, John E. and Gohara, David and Shi, Guochun},
	month = may,
	year = {2010},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {High performance computing, Hardware, Kernel, Computer architecture, Computer interfaces, Concurrent computing, Runtime, Parallel programming, Microprocessors, Software standards},
	pages = {66--73},
	file = {Accepted Version:/home/lledoux/Zotero/storage/TBZY7WDD/Stone et al. - 2010 - OpenCL A Parallel Programming Standard for Hetero.pdf:application/pdf},
}

@misc{zhao_using_2020,
	title = {Using {Vivado}-{HLS} for {Structural} {Design}: a {NoC} {Case} {Study}},
	shorttitle = {Using {Vivado}-{HLS} for {Structural} {Design}},
	url = {http://arxiv.org/abs/1710.10290},
	doi = {10.48550/arXiv.1710.10290},
	abstract = {There have been ample successful examples of applying Xilinx Vivado's "function-to-module" high-level synthesis (HLS) where the subject is algorithmic in nature. In this work, we carried out a design study to assess the effectiveness of applying Vivado-HLS in structural design. We employed Vivado-HLS to synthesize C functions corresponding to standalone network-on-chip (NoC) routers as well as complete multi-endpoint NoCs. Interestingly, we find that describing a complete NoC comprising router submodules faces fundamental difficulties not present in describing the routers as standalone modules. Ultimately, we succeeded in using Vivado-HLS to produce router and NoC modules that are exact cycle- and bit-accurate replacements of our reference RTL-based router and NoC modules. Furthermore, the routers and NoCs resulting from HLS and RTL are comparable in resource utilization and critical path delay. Our experience subjectively suggests that HLS is able to simplify the design effort even though much of the structural details had to be provided in the HLS description through a combination of coding discipline and explicit pragmas. The C++ source code can be found at https://github.com/zhipengzhaocmu/HLS\_NoC.},
	publisher = {arXiv},
	author = {Zhao, Zhipeng and Hoe, James C.},
	month = aug,
	year = {2020},
	note = {arXiv:1710.10290 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	annote = {Comment: A poster with the same title was presented at the 2017 International Symposium on Field Programmable Gate Arrays},
	file = {arXiv Fulltext PDF:/home/lledoux/Zotero/storage/M2F8UWUP/Zhao and Hoe - 2020 - Using Vivado-HLS for Structural Design a NoC Case.pdf:application/pdf;arXiv.org Snapshot:/home/lledoux/Zotero/storage/N7ZM6STB/1710.html:text/html},
}

@inproceedings{zwagerman_high_2015,
	title = {High {Level} {Synthesis}, a {Use} {Case} {Comparison} with {Hardware} {Description} {Language}},
	url = {https://www.semanticscholar.org/paper/High-Level-Synthesis%2C-a-Use-Case-Comparison-with-Zwagerman/377bf86a879c70b7fff7392bfda70cafc368536d},
	abstract = {3 INTRODUCTION 8 RESEARCH BACKGROUND 9 PROBLEM RESEARCH 11 HARDWARE TOOL KIT 12 IMPLEMENTATION 13 VISUAL FILTER EXAMPLE 15 XILINX TRD 17 IMPLEMENTING IN HLS 20 IMPLEMENTING IN HDL 25 WORKING IMPLEMENTATION 28 ANALYSIS 29 1. RESOURCES 29 Optimal Routing Utilization 29 Congested Routing Utilization 30 Summary 32 2. SPEED 32 Summary 32 3. NON-RECURRING ENGINEERING 32 Summary 33 ANALYSIS SUMMARY 33 CONCLUSION 34 FUTURE WORK 35 WORKS CITED 36},
	author = {Zwagerman, Michael D.},
	year = {2015},
}

@article{decaluwe_myhdl_2004,
	author = {Decaluwe, Jan},
	title = {MyHDL: A Python-Based Hardware Description Language},
	journal = {Linux Journal},
	year = {2004},
	pages = {84--87},
	url = {https://dl.acm.org/doi/fullHtml/10.5555/1029015.1029020},
}

@article{pant_conversion_2015,
	title = {Conversion of {MATLAB} code in {VHDL} using {HDL} {Coder} \& {Implementation} of {Code} on {FPGA}},
	volume = {14},
	abstract = {The advancement of technology in these days give rises to many tools where implementation of an electronics circuit, algorithms and different control or communication systems is possible. MATLAB is one of the tools which are able design and test systems and algorithm. To check systems performance on a practical system there is a need to implement it on a target device or FPGA. HDL coder in MATLAB is the one of the tool by which convert MATLAB code (floating point design) to VHDL code (bit stream) so that the code may run in FPGA kit. This paper consists of how to convert our MATLAB code to VHDL code by HDL coder and what is the source utility of the converted code.},
	language = {en},
	author = {Pant, Harshit and Bourai, Himanshu and Rana, Gaurav Singh and Yadav, S C},
	year = {2015},
}

@article{elsayed_comparative_2022,
	title = {A {Comparative} {Study} between {MATLAB} {HDL} {Coder} and {VHDL} for {FPGAs} {Design} and implementation},
	volume = {4},
	issn = {2636-4425},
	url = {https://jisse.journals.ekb.eg/article_260808.html},
	doi = {10.21608/jisse.2022.136645.1056},
	abstract = {Nowadays, FPGA has become a very useful platform for multiple digital applications. Initially, the hardware programming languages like VHDL or Verilog were the only method for designing the FPGA. In this method, the designer should be able to transform the algorithm of the application into digital blocks. This consumes time and effort. Recently MATLAB realized the FPGA importance and decided to introduce a new tool for FPGA Design; this tool is MATLAB HDL Coder. The idea is to write a very easy MATLAB script and it will be converted to HDL using HDL Coder. Then this HDL code will go through the FPGA regular implementation path. This paper studies and compares, by example, the two methods. The comparison is done for Speed, FPGA utilization, and time for design/implementation. The digital unit under test was AES. The choice of this unit is based on having large input data, it makes many feedbacks, and It needs high speed. The test result doesn't recommend MATLAB HDL Coder for implementation but it recommends it to fast-proof ideas and fast prototypes. This is because the idea of just writing a simple script describing the algorithm results in a very complicated combinational circuit, which has a very low frequency. The recommended future research is to find a way to force the MATLAB script to be implemented in pipeline architecture. The expected result is to improve the performance in two directions utilization and frequency, but It'll lose the main advantage which is fast implementation.},
	number = {4},
	journal = {Journal of International Society for Science and Engineering},
	author = {Elsayed, Ghada and Kayed, Somaya Ismail},
	month = dec,
	year = {2022},
	note = {Publisher: INTERNATIONAL SOCIETY FOR SCIENCE AND ENGINEERING},
	pages = {92--98},
}

@phdthesis{sarge_evaluating_2018,
	type = {Thesis},
	title = {Evaluating {Simulink} {HDL} coder as a framework for flexible and modular hardware description},
	copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
	url = {https://dspace.mit.edu/handle/1721.1/119717},
	abstract = {This thesis investigates the performance and viability of Simulink and HDL Coder from MathWorks as an alternative workflow for producing hardware description. Several designs were implemented towards this end. An FFT-based signal analyzer served as a pathfinding application to better understand the tools. In order to directly evaluate the ability of the workflow to faithfully recreate hardware operations, an existing architecture for nonlinear equalization was re-implemented and benchmarked. Finally, a new implementation of polynomial nonlinear equalization was created and benchmarked to explore the possible performance, parameterizability, and flexibility of hardware generated from a Simulink design. It was found that while the generated hardware does not perform quite as well as a hand-optimized design, it does perform well enough to be practical and also can be capable of greater flexibility in structure than a design created with a more traditional workflow.},
	language = {eng},
	school = {Massachusetts Institute of Technology},
	author = {Sarge, Valerie Youngmi},
	year = {2018},
	note = {Accepted: 2018-12-18T19:47:03Z},
}

@article{pongratz_performance_2019,
	title = {Performance {Evaluation} of {MathWorks} {HDL} {Coder} as a {Vendor} {Independent} {DFE} {Generation}},
	abstract = {In conclusion, it was found out that in the bigger designs HDL Coder did not have a problem meeting the timing requirements for the selected designs and the results are comparable. However, it does not map optimally to the resources on the FPGA (Field Programmable Gate Array) fabric and it was diﬃcult to change the mapping of resources according to constraints. On analysis, the HDL Coder blocks consume a lot of DSP (Digital Signal Processing) slices compared to the vendor dependent counterparts. Though, the tool is faster and easier to learn, work with and many optimization methods can be done automatically by the tool. Additionally, there is no need to run synthesis on a separate vendor cause this can be done by HDL Coder which improves the workﬂow. Using HDL Coder in a design methodology would increase productivity because of it being vendor independent. Whereas, the performance results can vary depending on design structure.},
	language = {en},
	author = {Pongratz, Elisabeth and Cherian, Roshan},
	year = {2019},
}

@inproceedings{assef_fpga_2019,
	title = {{FPGA} {Implementation} and {Evaluation} of an {Approximate} {Hilbert} {Transform}-{Based} {Envelope} {Detector} for {Ultrasound} {Imaging} {Using} the {DSP} {Builder} {Development} {Tool}},
	url = {https://ieeexplore.ieee.org/abstract/document/8857671},
	doi = {10.1109/EMBC.2019.8857671},
	abstract = {In this paper, we present the FPGA implementation of an approximate Hilbert Transform-based envelope detector to compute the magnitude of the received ultrasound echo signals in real-time using a Model-based design flow. The proposed architecture exploits the negative odd-symmetry and interleaved zero-valued coefficients of a Hilbert Transform-based FIR filter to reduce hardware resource requirements and complexity. The hardware design is modeled using the DSP Builder development tool allowing the automatic generation of HDL algorithms directly from the Matlab/Simulink environment. The generated VHDL code was synthesized for an Intel Stratix IV FPGA and validated on a Terasic DE4-230 board. The accuracy and performance of the envelope detector are analyzed with real ultrasound phantom data for different filter orders, coefficient length and two filter design methods: Equiripple and Least-Squares. The normalized residual sum of squares (NRSS) and the normalized root mean square error (NRMSE) cost functions are used for comparison with the reference absolute value of the Matlab Hilbert function. The results demonstrate that the proposed method yields similar results to conventional envelope detection methods, while being simpler to implement and requiring lower computational cost.},
	booktitle = {2019 41st {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Assef, Amauri Amorin and de Oliveira, Jonathan and Maia, Joaquim Miguel and Costa, Eduardo Tavares},
	month = jul,
	year = {2019},G
	note = {ISSN: 1558-4615},
	keywords = {Clocks, Envelope detectors, Field programmable gate arrays, Finite impulse response filters, Hardware, Matlab, Transforms},
	pages = {2813--2816},
}

@article{kintali_model-based_2014,
	title = {Model-{Based} {Design} {Using} {Simulink}, {HDL} {Coder}, and {DSP} {Builder} for {Intel} {FPGAs}},
	abstract = {This document describes how HDL Coder™ from MathWorks can be used with DSP Builder for Intel® FPGAs in an integrated FPGA workflow. We use an example to show how designers can integrate models built with DSP Builder Advanced Blockset into a Simulink® model, and how HDL Coder can generate HDL code for the complete design. This capability allows designers to reuse existing DSP Builder models when using HDL Coder to create new designs, or to incorporate target-optimized Intel FPGA IP blocks created for use with HDL Coder within Simulink models.},
	language = {en},
	year = {2014},
	author = {Kintali, Kiran and Gu, Yongfeng and Cigan, Eric},
}
@article{balasubramanian2009high,
  title={High speed gate level synchronous full adder designs},
  author={Balasubramanian, Padmanabhan and Mastorakis, Nikos E},
  journal={WSEAS Transactions on Circuits and Systems},
  volume={8},
  number={2},
  pages={290--300},
  year={2009},
  publisher={World Scientific and Engineering Academy and Society (WSEAS) Stevens Point~…}
}

@misc{luccioni2022estimating,
      title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
      author={Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
      year={2022},
      eprint={2211.02001},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dally_model_2022,
	title = {On the {Model} of {Computation}: {Point}: {We} {Must} {Extend} {Our} {Model} of {Computation} to {Account} for {Cost} and {Location}},
	shorttitle = {On the {Model} of {Computation}},
	url = {https://cacm.acm.org/magazines/2022/9/263792-on-the-model-of-computation-point/fulltext},
	abstract = {We must extend our model of computation to account for cost and location.},
	year = {2022},
	language = {en},
	author = {Dally, William},
}

@inproceedings{thanh-hoang_does_2015,
	title = {Does arithmetic logic dominate data movement? a systematic comparison of energy-efficiency for {FFT} accelerators},
	shorttitle = {Does arithmetic logic dominate data movement?},
	url = {https://ieeexplore.ieee.org/document/7245708},
	doi = {10.1109/ASAP.2015.7245708},
	abstract = {In this paper, we perform a systematic comparison to study the energy cost of varying data formats and data types w.r.t. arithmetic logic and data movement for accelerator-based heterogeneous systems in which both compute-intensive (FFT accelerator) and data-intensive accelerators (DLT accelerator) are added. We explore evaluation for a wide range of design processes (e.g. 32nm bulk-CMOS and projected 7nm FinFET) and memory systems (e.g. DDR3 and HMC). First, our result shows that when varying data formats, the energy costs of using floating point over fixed point are 5.3\% (DDR3), 6.2\% (HMC) for core and 0.8\% (DDR3), 1.5\% (HMC) for system in 32nm process. These energy costs are negligible as 0.2\% and 0.01\% for core and system in 7nm FinFET process in DDR3 memory and slightly increasing in HMC. Second, we identify that the core and system energy of systems using fixed point, 16-bit, FFT accelerator is nearly half of using 32-bit if data movement is also accelerated. This evidence implies that system energy is highly proportional to the amount of moving data when varying data types.},
	booktitle = {2015 {IEEE} 26th {International} {Conference} on {Application}-specific {Systems}, {Architectures} and {Processors} ({ASAP})},
	author = {Thanh-Hoang, Tung and Shambayati, Amirali and Hoffmann, Henry and Chien, Andrew A.},
	month = jul,
	year = {2015},
	note = {ISSN: 2160-052X},
	keywords = {Acceleration, Computer architecture, FinFETs, Hardware, Logic gates, Registers, Systematics},
	pages = {66--67},
}

@techreport{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	number = {arXiv:1810.04805},
	institution = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@techreport{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	number = {arXiv:2005.14165},
	institution = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, I.},
	year = {2019},
}

@article{kimovski_beyond_2023,
	title = {Beyond von {Neumann} in the {Computing} {Continuum}: {Architectures}, {Applications}, and {Future} {Directions}},
	issn = {1941-0131},
	shorttitle = {Beyond von {Neumann} in the {Computing} {Continuum}},
	url = {https://ieeexplore.ieee.org/abstract/document/10207712},
	doi = {10.1109/MIC.2023.3301010},
	abstract = {The article discusses the emerging non-von Neumann computer architectures and their integration in the computing continuum for supporting modern distributed applications, including artificial intelligence, big data, and scientific computing. It provides a detailed summary of the available and emerging non-von Neumann architectures, which range from power-efficient single-board accelerators to quantum and neuromorphic computers. Furthermore, it explores their potential benefits for revolutionizing data processing and analysis in various societal, science, and industry fields. The paper provides a detailed analysis of the most widely used class of distributed applications and discusses the difficulties in their execution over the computing continuum, including communication, interoperability, orchestration, and sustainability issues.},
	journal = {IEEE Internet Computing},
	author = {Kimovski, Dragi and Saurabh, Nishant and Jansen, Matthijs and Aral, Atakan and Al-Dulaimy, Auday and Bondi, André B. and Galletta, Antonino and Papadopoulos, Alessandro V. and Iosup, Alexandru and Prodan, Radu},
	year = {2023},
	note = {Conference Name: IEEE Internet Computing},
	keywords = {Artificial intelligence, Computational modeling, Computer architecture, Distributed databases, Internet, Neurons, Quantum computing},
	pages = {1--11},
}

@inproceedings{ahn_scalable_2015,
	address = {New York, NY, USA},
	series = {{ISCA} '15},
	title = {A scalable processing-in-memory accelerator for parallel graph processing},
	isbn = {978-1-4503-3402-0},
	url = {https://doi.org/10.1145/2749469.2750386},
	doi = {10.1145/2749469.2750386},
	abstract = {The explosion of digital data and the ever-growing need for fast data analysis have made in-memory big-data processing in computer systems increasingly important. In particular, large-scale graph processing is gaining attention due to its broad applicability from social science to machine learning. However, scalable hardware design that can efficiently process large graphs in main memory is still an open problem. Ideally, cost-effective and scalable graph processing systems can be realized by building a system whose performance increases proportionally with the sizes of graphs that can be stored in the system, which is extremely challenging in conventional systems due to severe memory bandwidth limitations. In this work, we argue that the conventional concept of processing-in-memory (PIM) can be a viable solution to achieve such an objective. The key modern enabler for PIM is the recent advancement of the 3D integration technology that facilitates stacking logic and memory dies in a single package, which was not available when the PIM concept was originally examined. In order to take advantage of such a new technology to enable memory-capacity-proportional performance, we design a programmable PIM accelerator for large-scale graph processing called Tesseract. Tesseract is composed of (1) a new hardware architecture that fully utilizes the available memory bandwidth, (2) an efficient method of communication between different memory partitions, and (3) a programming interface that reflects and exploits the unique hardware design. It also includes two hardware prefetchers specialized for memory access patterns of graph processing, which operate based on the hints provided by our programming model. Our comprehensive evaluations using five state-of-the-art graph processing workloads with large real-world graphs show that the proposed architecture improves average system performance by a factor of ten and achieves 87\% average energy reduction over conventional systems.},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Ahn, Junwhan and Hong, Sungpack and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
	month = jun,
	year = {2015},
	pages = {105--117},
}

@article{chi_prime_2016,
	title = {{PRIME}: a novel processing-in-memory architecture for neural network computation in {ReRAM}-based main memory},
	volume = {44},
	issn = {0163-5964},
	shorttitle = {{PRIME}},
	url = {https://doi.org/10.1145/3007787.3001140},
	doi = {10.1145/3007787.3001140},
	abstract = {Processing-in-memory (PIM) is a promising solution to address the "memory wall" challenges for future computer systems. Prior proposed PIM architectures put additional computation logic in or near memory. The emerging metal-oxide resistive random access memory (ReRAM) has showed its potential to be used for main memory. Moreover, with its crossbar array structure, ReRAM can perform matrix-vector multiplication efficiently, and has been widely studied to accelerate neural network (NN) applications. In this work, we propose a novel PIM architecture, called PRIME, to accelerate NN applications in ReRAM based main memory. In PRIME, a portion of ReRAM crossbar arrays can be configured as accelerators for NN applications or as normal memory for a larger memory space. We provide microarchitecture and circuit designs to enable the morphable functions with an insignificant area overhead. We also design a software/hardware interface for software developers to implement various NNs on PRIME. Benefiting from both the PIM architecture and the efficiency of using ReRAM for NN computation, PRIME distinguishes itself from prior work on NN acceleration, with significant performance improvement and energy saving. Our experimental results show that, compared with a state-of-the-art neural processing unit design, PRIME improves the performance by {\textasciitilde}2360× and the energy consumption by {\textasciitilde}895×, across the evaluated machine learning benchmarks.},
	number = {3},
	journal = {SIGARCH Comput. Archit. News},
	author = {Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
	month = jun,
	year = {2016},
	keywords = {neural network, processing in memory, resistive random access memory},
	pages = {27--39},
}

@article{gokhale_processing_1995,
	title = {Processing in memory: the {Terasys} massively parallel {PIM} array},
	volume = {28},
	issn = {1558-0814},
	shorttitle = {Processing in memory},
	url = {https://ieeexplore.ieee.org/abstract/document/375174},
	doi = {10.1109/2.375174},
	abstract = {SRC researchers have designed and fabricated a processor-in-memory (PIM) chip, a standard 4-bit memory augmented with a single-bit ALU controlling each column of memory. In principle, PIM chips can replace the memory of any processor, including a supercomputer. To validate the notion of integrating SIMD computing into conventional processors on a more modest scale, we have built a half dozen Terasys workstations, which are Sun Microsystems Sparcstation-2 workstations in which 8 megabytes of address space consist of PIM memory holding 32K single-bit ALUs. We have designed and implemented a high-level parallel language, called data parallel bit C (dbC), for Terasys and demonstrated that dbC applications using the PIM memory as a SIMD array run at the speed of multiple Cray-YMP processors. Thus, we can deliver supercomputer performance for a small fraction of supercomputer cost. Since the successful creation of the Terasys research prototype, we have begun work on processing in memory in a supercomputer setting. In a collaborative research project, we are working with Cray Computer to incorporate a new Cray-designed implementation of the PIM chips into two octants of Cray-3 memory.{\textless}{\textgreater}},
	number = {4},
	journal = {Computer},
	author = {Gokhale, M. and Holmes, B. and Iobst, K.},
	month = apr,
	year = {1995},
	note = {Conference Name: Computer},
	keywords = {Application software, Computer architecture, Costs, Degradation, Prototypes, Random access memory, Sun, Supercomputers, Workstations},
	pages = {23--31},
}

@inproceedings{singh_review_2018,
	title = {A {Review} of {Near}-{Memory} {Computing} {Architectures}: {Opportunities} and {Challenges}},
	shorttitle = {A {Review} of {Near}-{Memory} {Computing} {Architectures}},
	url = {https://ieeexplore.ieee.org/abstract/document/8491877},
	doi = {10.1109/DSD.2018.00106},
	abstract = {The conventional approach of moving stored data to the CPU for computation has become a major performance bottleneck for emerging scale-out data-intensive applications due to their limited data reuse. At the same time, the advancement in integration technologies have made the decade-old concept of coupling compute units close to the memory (called Near-Memory Computing) more viable. Processing right at the "home" of data can completely diminish the data movement problem of data-intensive applications. This paper focuses on analyzing and organizing the extensive body of literature on near-memory computing across various dimensions: starting from the memory level where this paradigm is applied, to the granularity of the application that could be executed on the near-memory units. We highlight the challenges as well as the critical need of evaluation methodologies that can be employed in designing these special architectures. Using a case study, we present our methodology and also identify topics for future research to unlock the full potential of near-memory computing.},
	booktitle = {2018 21st {Euromicro} {Conference} on {Digital} {System} {Design} ({DSD})},
	author = {Singh, Gagandeep and Chelini, Lorenzo and Corda, Stefano and Javed Awan, Ahsan and Stuijk, Sander and Jordans, Roel and Corporaal, Henk and Boonstra, Albert-Jan},
	month = aug,
	year = {2018},
	keywords = {application characterization, Central Processing Unit, Coherence, computer architecture, data centric computing, Graphics processing units, Memory management, modeling, near data processing, near-memory computing, processing in memory, Programming, survey, Tools},
	pages = {608--617},
}

@article{farahani_drama_2015,
	title = {{DRAMA}: {An} {Architecture} for {Accelerated} {Processing} {Near} {Memory}},
	volume = {14},
	issn = {1556-6064},
	shorttitle = {{DRAMA}},
	url = {https://ieeexplore.ieee.org/abstract/document/6846276},
	doi = {10.1109/LCA.2014.2333735},
	abstract = {Improving energy efficiency is crucial for both mobile and high-performance computing systems while a large fraction of total energy is consumed to transfer data between storage and processing units. Thus, reducing data transfers across the memory hierarchy of a processor (i.e., off-chip memory, on-chip caches, and register file) can greatly improve the energy efficiency. To this end, we propose an architecture, DRAMA, that 3D-stacks coarse-grain reconfigurable accelerators (CGRAs) atop off-chip DRAM devices. DRAMA does not require changes to the DRAM device architecture, apart from through-silicon vias (TSVs) that connect the DRAM device's internal I/O bus to the CGRA layer. We demonstrate that DRAMA can reduce the energy consumption to transfer data across the memory hierarchy by 66-95 percent while achieving speedups of up to 18× over a commodity processor.},
	number = {1},
	journal = {IEEE Computer Architecture Letters},
	author = {Farmahini-Farahani, Amin and Ho Ahn, Jung and Morrow, Katherine and Sung Kim, Nam},
	month = jan,
	year = {2015},
	note = {Conference Name: IEEE Computer Architecture Letters},
	keywords = {Acceleration, Arrays, Kernel, Memory management, Near memory processing, DRAM, 3D-stacking, energy-efficient computing, accelerator, Random access memory, Registers},
	pages = {26--29},
}

@article{agostini_bridging_2022,
	title = {Bridging {Python} to {Silicon}: {The} {SODA} {Toolchain}},
	volume = {42},
	issn = {0272-1732, 1937-4143},
	shorttitle = {Bridging {Python} to {Silicon}},
	url = {https://ieeexplore.ieee.org/document/9786533/},
	doi = {10.1109/MM.2022.3178580},
	number = {5},
	journal = {IEEE Micro},
	author = {Agostini, Nicolas Bohm and Curzel, Serena and Zhang, Jeff Jun and Limaye, Ankur and Tan, Cheng and Amatya, Vinay and Minutoli, Marco and Castellana, Vito Giovanni and Manzano, Joseph and Brooks, David and Wei, Gu-Yeon and Tumeo, Antonino},
	month = sep,
	year = {2022},
	pages = {78--88},
}

@Article{cavalcante_ara_2020,
  author = {Matheus Cavalcante and Fabian Schuiki and Florian Zaruba and Michael Schaffner and Luca Benini},
  journal= {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  title  = {Ara: A 1-GHz+ Scalable and Energy-Efficient RISC-V Vector Processor With Multiprecision Floating-Point Support in 22-nm FD-SOI},
  year   = {2020},
  volume = {28},
  number = {2},
  pages  = {530-543},
  doi    = {10.1109/TVLSI.2019.2950087}
}

@INPROCEEDINGS{perotti_ara_2022,
  author={Perotti, Matteo and Cavalcante, Matheus and Wistoff, Nils and Andri, Renzo and Cavigelli, Lukas and Benini, Luca},
  booktitle={2022 IEEE 33rd International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
  title={A “New Ara” for Vector Computing: An Open Source Highly Efficient RISC-V V 1.0 Vector Processor Design},
  year={2022},
  volume={},
  number={},
  pages={43-51},
  doi={10.1109/ASAP54787.2022.00017}}

@inproceedings{oberman_floating_1999,
	title = {Floating point division and square root algorithms and implementation in the {AMD}-{K7}/sup {TM}/ microprocessor},
	url = {https://ieeexplore.ieee.org/document/762835},
	doi = {10.1109/ARITH.1999.762835},
	abstract = {This paper presents the AMD-K7 IEEE 754 and /spl times/87 compliant floating point division and square root algorithms and implementation. The AMD-K7 processor employs an iterative implementation of a series expansion to converge quadratically to the quotient and square root. Highly accurate initial approximations and a high performance shared floating point multiplier assist in achieving low division and square root latencies at high operating frequencies. A novel time-sharing technique allows independent floating point multiplication operations to proceed while division or square root computation is in progress. Exact IEEE 754 rounding for all rounding modes and target precisions has been verified by conventional directed and random testing procedures, along with the formulation of a mechanically-checked formal proof using the ACL2 theorem prover.},
	booktitle = {Proceedings 14th {IEEE} {Symposium} on {Computer} {Arithmetic} ({Cat}. {No}.{99CB36336})},
	author = {Oberman, S.F.},
	month = apr,
	year = {1999},
	note = {ISSN: 1063-6889},
	keywords = {Clocks, Decoding, Delay, Frequency conversion, Microprocessors, Out of order, Performance gain, Pipelines, Processor scheduling, Testing},
	pages = {106--115},
}

@article{soderquist_division_1997,
	title = {Division and square root: choosing the right implementation},
	volume = {17},
	issn = {1937-4143},
	shorttitle = {Division and square root},
	url = {https://ieeexplore.ieee.org/document/612224},
	doi = {10.1109/40.612224},
	abstract = {Floating-point support has become a mandatory feature of new microprocessors due to the prevalence of business, technical, and recreational applications that use these operations. Spreadsheets, CAD tools, and games, for instance, typically feature floating-point-intensive code. Over the past few years, the leading architectures have incorporated several generations of floating-point units (FPUs). However, while addition and multiplication implementations have become increasingly efficient, support for division and square root has remained uneven. The design community has reached no consensus on the type of algorithm to use for these two functions, and quality and performance of the implementations vary widely. This situation originates in skepticism about the importance of division and square root and an insufficient understanding of the design alternatives. Quantifying what constitutes good performance is challenging. One rule thumb, for example, states that the latency of division should be three times that of multiplication; this figure is based on division frequencies in a selection of typical scientific applications. Even if we accept this doctrine at face value, implementing division-and square root-involves much more than relative latencies. We must also consider area, throughput, complexity, and the interaction with other operations. This article explores the various trade-offs involved and illuminates the consequences of different design choices, thus enabling designers to make informed decisions.},
	number = {4},
	journal = {IEEE Micro},
	author = {Soderquist, P. and Leeser, M.},
	month = jul,
	year = {1997},
	note = {Conference Name: IEEE Micro},
	keywords = {Algorithm design and analysis, Delay, Floating-point arithmetic, Hardware, Microprocessors, Registers, Sun, Throughput},
	pages = {56--66},
}

@article{even_parametric_2005,
	title = {A parametric error analysis of {Goldschmidt}'s division algorithm},
	volume = {70},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000004000960},
	doi = {10.1016/j.jcss.2004.08.004},
	abstract = {Back in the 1960s Goldschmidt presented a variation of Newton–Raphson iterations for division that is well suited for pipelining. The problem in using Goldschmidt's division algorithm is to present an error analysis that enables one to save hardware by using just the right amount of precision for intermediate calculations while still providing correct rounding. Previous implementations relied on combining formal proof methods (that span thousands of lines) with millions of test vectors. These techniques yield correct designs but the analysis is hard to follow and is not quite tight. We present a simple parametric error analysis of Goldschmidt's division algorithm. This analysis sheds more light on the effect of the different parameters on the error. In addition, we derive closed error formulae that allow to determine optimal parameter choices in four practical settings. We apply our analysis to show that a few bits of precision can be saved in the floating-point division (FP-DIV) micro-architecture of the AMD-K7TMmicroprocessor. These reductions in precision apply to the initial approximation and to the lengths of the multiplicands in the multiplier. When translated to cost, the reductions reflect a savings of 10.6\% in the overall cost of the FP-DIV micro-architecture.},
	number = {1},
	journal = {Journal of Computer and System Sciences},
	author = {Even, Guy and Seidel, Peter-M. and Ferguson, Warren E.},
	month = feb,
	year = {2005},
	keywords = {Floating point arithmetic, Goldschmidt's division algorithm, Parametric error analysis},
	pages = {118--139},
}

@article{gorodecky_scalable_2023,
	title = {Scalable architecture of constant division on {FPGA}},
	year = {2023},
	abstract = {This paper proposes a method for hardware integer division by a constant, based only on combinational logic, i.e. without requiring storage and feedback in calculations. The proposed scheme for division consists of adders and encoders, where encoders are systems of Boolean functions. The proposed divisor provides at the output the quotient and the residue (at the same time or separately). Experiments conducted on FPGA demonstrate up to three times improvement in area cost compare to the optimized divisor circuits provided by the Xilinx tools, while the delay is improved by 25\% for dividends with less than 48-bit. It is also shown in this paper that the proposed approach is scalable, and in comparison to the state-of-the-art, the proposed approach improves the area or the delay, or both for many constant values and input bit sizes.},
	language = {en},
	author = {Gorodecky, Danila and de Lisboa, Universidade},
}

@article{seo_dual-purpose_2023,
	title = {Dual-{Purpose} {Hardware} {Algorithms} and {Architectures} – {Part} 1: {Floating}-{Point} {Division}},
	year = {2023},
	abstract = {Division is a time-consuming, but frequently-used arithmetic operation, so an enormous amount of effort has been made to improve the performance of dividers. Most of the division algorithms in the literature are ofﬂine algorithms that minimize the execution time of a single division, whereas some others are online algorithms that maximize the throughput (\# divisions executed per time). In this paper, we propose an interval-analysisbased normal-binary ﬂoating-point division algorithm that can be used for both ofﬂine and online division. We implement two ofﬂine and four online dividers using the algorithm and compare them with recently-proposed ofﬂine and online dividers. The simulation results show that the ofﬂine versions are the best for a Binary64 ofﬂine division, whereas the online versions are the best for a Binary64 online division.},
	language = {en},
	author = {Seo, Jihee and Kim, Dae Hyun},
}

@article{seo_dual-purpose_2023-1,
	title = {Dual-{Purpose} {Hardware} {Algorithms} and {Architectures} – {Part} 2: {Integer} {Division}},
	year = {2023},
	abstract = {Integer division is different from ﬂoating-point division in that (1) the execution time of an integer division is highly dependent on the leading 1 locations of operands, (2) x and −x have different magnitude parts if x is a two’s complement integer, and (3) rounding is not necessary. In this paper, we apply the interval-analysis-based division algorithm proposed in “Dual-Purpose Hardware Algorithms and Architecture – Part 1: Floating-Point Division” [1] to ofﬂine and online integer division. We implement four online integer dividers using the algorithm, compare them with other dividers, and present detailed simulation results with in-depth analysis of the dividers. We ﬁnd that the online dividers outperform the ofﬂine dividers when the waiting time for online operands goes up and the number of quotient bits to obtain goes down.},
	language = {en},
	author = {Seo, Jihee and Kim, Dae Hyun},
}

@article{badizadegan_newton-raphson_2023,
	title = {Newton-{Raphson} {Integer} {Division} for {Area}-{Constrained} {Microcontrollers}},
	year = {2023},
	abstract = {Many small microcontrollers today are equipped with single-cycle multipliers, but use division algorithms that compute only one bit at a time, resulting in division operations with up to 32 (or more) CPU cycles of latency, stalling the core during computing. This is primarily due to area constraints, since fast division algorithms based on iterative approximation require relatively large lookup tables to produce a useful speedup over slower algorithms.},
	language = {en},
	author = {Badizadegan, Nima},
}

@inproceedings{soria-pardos_sargantana_2022,
	title = {Sargantana: {A} 1 {GHz}+ {In}-{Order} {RISC}-{V} {Processor} with {SIMD} {Vector} {Extensions} in 22nm {FD}-{SOI}},
	shorttitle = {Sargantana},
	url = {https://ieeexplore.ieee.org/document/9996608},
	doi = {10.1109/DSD57027.2022.00042},
	abstract = {The RISC-V open Instruction Set Architecture (ISA) has proven to be a solid alternative to licensed ISAs. In the past 5 years, a plethora of industrial and academic cores and accelerators have been developed implementing this open ISA. In this paper, we present Sargantana, a 64-bit processor based on RISC-V that implements the RV64G ISA, a subset of the vector instructions extension (RVV 0.7.1), and custom application-specific instructions. Sargantana features a highly optimized 7-stage pipeline implementing out-of-order write-back, register renaming, and a non-blocking memory pipeline. Moreover, Sar-gantana features a Single Instruction Multiple Data (SIMD) unit that accelerates domain-specific applications. Sargantana achieves a 1.26 GHz frequency in the typical corner, and up to 1.69 GHz in the fast corner using 22nm FD-SOI commercial technology. As a result, Sargantana delivers a 1.77× higher Instructions Per Cycle (IPC) than our previous 5-stage in-order DVINO core, reaching 2.44 CoreMark/MHz. Our core design delivers comparable or even higher performance than other state-of-the-art academic cores performance under Autobench EEMBC benchmark suite. This way, Sargantana lays the foundations for future RISC-V based core designs able to meet industrial-class performance requirements for scientific, real-time, and high-performance computing applications.},
	booktitle = {2022 25th {Euromicro} {Conference} on {Digital} {System} {Design} ({DSD})},
	author = {Soria-Pardos, Víctor and Doblas, Max and López–Paradís, Guillem and Candón, Gerard and Rodas, Narcís and Carril, Xavier and Fontova–Musté, Pau and Leyva, Neiel and Marco-Sola, Santiago and Moretó, Miquel},
	month = aug,
	year = {2022},
	note = {ISSN: 2771-2508},
	keywords = {Benchmark testing, computer architecture, Computer architecture, domain-specific accelerators, High performance computing, Out of order, Pipelines, RISC-V, Runtime, Solids, vector instructions},
	pages = {254--261},
}

@inproceedings{reddi_mlperf_2020,
	address = {Virtual Event},
	series = {{ISCA} '20},
	title = {{MLPerf} inference benchmark},
	isbn = {978-1-72814-661-4},
	url = {https://doi.org/10.1109/ISCA45697.2020.00045},
	doi = {10.1109/ISCA45697.2020.00045},
	abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 47th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {IEEE Press},
	author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
	month = sep,
	year = {2020},
	keywords = {benchmarking, inference, machine learning},
	pages = {446--459},
}

@inproceedings{stine_freepdk_2007,
	title = {{FreePDK}: {An} {Open}-{Source} {Variation}-{Aware} {Design} {Kit}},
	shorttitle = {{FreePDK}},
	url = {https://ieeexplore.ieee.org/abstract/document/4231502},
	doi = {10.1109/MSE.2007.44},
	abstract = {This paper discusses an open source, variation aware Process Design Kit (PDK), based on Scalable CMOS design rules, down to 45nm, for use in VLSI research, education and small businesses. This kit includes all the necessary layout design rules and extraction command decks to capture layout dependent systematic variation and perform statistical circuit analysis. The kit also includes a standard cell and pad library with the necessary support files to enable full chip place and route and verification for System on Chip designs. Test chips designed with this PDK are designed in such a way so that they can be fabricated by fabrication facilities allowing validation of the design rules so that the rules may be used in future multi-project runs and design contests.},
	booktitle = {2007 {IEEE} {International} {Conference} on {Microelectronic} {Systems} {Education} ({MSE}'07)},
	author = {Stine, James E. and Castellanos, Ivan and Wood, Michael and Henson, Jeff and Love, Fred and Davis, W. Rhett and Franzon, Paul D. and Bucher, Michael and Basavarajaiah, Sunil and Oh, Julie and Jenkal, Ravi},
	month = jun,
	year = {2007},
	keywords = {Business, Design engineering, Design methodology, Fabrication, Intellectual property, Libraries, Open source software, Process design, System-on-a-chip, Very large scale integration},
	pages = {173--174},
}

@inproceedings{stine_freepdk_2009,
	title = {{FreePDK} v2.0: {Transitioning} {VLSI} education towards nanometer variation-aware designs},
	shorttitle = {{FreePDK} v2.0},
	url = {https://ieeexplore.ieee.org/abstract/document/5270820},
	doi = {10.1109/MSE.2009.5270820},
	abstract = {This paper discusses an extension to an open source, variation aware process design kit (PDK), based on scalable CMOS design rules. This PDK is designed for 45 nm feature sizes and is utilized for use in VLSI research, computer architecture, education and small businesses. This kit includes all the necessary layout design rules and extraction command decks to capture layout dependent systematic variation and perform statistical circuit analysis. The kit also includes a standard cell library, MIPSreg processor and associated GNU-compliant compiler and the necessary support files to enable full chip place and route and verification for system on chip designs. An analog and digital system test chip is also included with this PDK-extension allowing exploration of nanometer-based VLSI designs.},
	booktitle = {2009 {IEEE} {International} {Conference} on {Microelectronic} {Systems} {Education}},
	author = {Stine, James E. and Chen, Jun and Castellanos, Ivan and Sundararajan, Gopal and Qayam, Mohammad and Kumar, Praveen and Remington, Justin and Sohoni, Sohum},
	month = jul,
	year = {2009},
	keywords = {Circuit analysis, Circuit testing, CMOS process, Computer architecture, Computer science education, Digital systems, Process design, Software libraries, System-on-a-chip, Very large scale integration},
	pages = {100--103},
}

@article{li_robustness-aware_2021,
	title = {Robustness-aware 2-bit quantization with real-time performance for neural network},
	volume = {455},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221007177},
	doi = {10.1016/j.neucom.2021.05.006},
	abstract = {Quantized neural networks (NN) with reduced bit precision are practical solutions to minimize computational and memory resource requirements and play a vital role in machine learning. However, it is still challenging to avoid significant accuracy degradation due to numerical approximation and lower redundancy. In this paper, a novel robustness-aware 2-bit quantization scheme (RAQ) is proposed for NN, based on binary NN and generative adversarial networks (GAN), which improve performance by enriching binary NN information, extracting the structural information and considering the robustness of the quantized NN. Specifically, using a shift-add operation to replace the multiply-accumulate in the quantization process can speed the NN. A structural loss is proposed to represent the difference between the original NN and quantized NN, such that the structural information of data is preserved after quantization. The structural information learned from NN plays an important role in improving the performance and allows for further fine-tuning of the quantized NN by applying the Lipschitz constraint to the structural loss. For the first time, we consider the robustness of the quantized NN and propose a non-sensitive perturbation loss function by introducing an extraneous term of the spectral norm. The experiments were conducted on CIFAR-10, SVHN and ImageNet datasets with popular NN (such as MobileNetV2, ResNet20, etc.). Extensive experiments show that our new 2-bit quantization scheme is more efficient than the state-of-the-art quantization methods. Our scheme effectively reduced the latency by 2 × and the accuracy decline by 1–4\%. Meanwhile, the experimental results also demonstrate that the RAQ is robust with adversarial attacks, we not only eliminate the robustness gap between full-precision and quantized models, but also improve the robustness over full-precision ones by 10\%.},
	journal = {Neurocomputing},
	author = {Li, Xiaobin and Jiang, Hongxu and Zhang, Runhua and Tian, Fangzheng and Huang, Shuangxi and Xu, Donghuan},
	month = sep,
	year = {2021},
	keywords = {Quantization, Robustness, Shift-add operation, Structural information distillation},
	pages = {12--22},
}

@inproceedings{rouhani_pushing_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point},
	isbn = {978-1-71382-954-6},
	abstract = {In this paper, we explore the limits of Microsoft Floating Point (MSFP), a new class of datatypes developed for production cloud-scale inferencing on custom hardware. Through the co-evolution of hardware design and algorithms, MSFP16 incurs 3 × lower cost compared to Bfloat16 and MSFP12 has 4 × lower cost compared to INT8 while delivering a comparable or better accuracy. MSFP incurs negligible impact to accuracy (\&lt;1\%), requires no changes to the model topology, and is integrated with a mature cloud production pipeline. MSFP supports various classes of deep learning models including CNNs, RNNs, and Transformers without modification. Finally, we characterize the accuracy and implementation of MSFP and demonstrate its efficacy on a number of production scenarios, including models that power major online scenarios such as web search, question-answering, and image classification.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Rouhani, Bita and Lo, Daniel and Zhao, Ritchie and Liu, Ming and Fowers, Jeremy and Ovtcharov, Kalin and Vinogradsky, Anna and Massengill, Sarah and Yang, Lita and Bittner, Ray and Forin, Alessandro and Zhu, Haishan and Na, Taesik and Patel, Prerak and Che, Shuai and Koppaka, Lok Chand and Song, Xia and Som, Subhojit and Das, Kaustav and Tiwary, Saurabh and Reinhardt, Steve and Lanka, Sitaram and Chung, Eric and Burger, Doug},
	month = dec,
	year = {2020},
	pages = {10271--10281},
}

@misc{rouhani2023microscaling,
    title = {Microscaling Data Formats for Deep Learning},
    author = {Bita Darvish Rouhani and Ritchie Zhao and Ankit More and Mathew Hall and Alireza Khodamoradi and Summer Deng and Dhruv Choudhary and Marius Cornea and Eric Dellinger and Kristof Denolf and Stosic Dusan and Venmugil Elango and Maximilian Golub and Alexander Heinecke and Phil James-Roxby and Dharmesh Jani and Gaurav Kolhe and Martin Langhammer and Ada Li and Levi Melnick and Maral Mesmakhosroshahi and Andres Rodriguez and Michael Schulte and Rasoul Shafipour and Lei Shao and Michael Siu and Pradeep Dubey and Paulius Micikevicius and Maxim Naumov and Colin Verrilli and Ralph Wittig and Doug Burger and Eric Chung},
    year = {2023},
    eprint = {2310.10537},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@inproceedings{sousa2024ptfloat,
    title        = {PT-Float: A Floating-Point Unit with Dynamically Varying Exponent and Fraction Sizes},
    author       = {Jose T. de Sousa and Joao D. Lopes and Micaela Serodio and Horacio C. Neto and Mario P. Vestias},
    booktitle    = {Proceedings of the 2024 IEEE International Symposium on Computer Arithmetic (ARITH)},
    year         = {2024},
    address      = {Malaga, Spain},
    publisher    = {IEEE},
    note         = {Presented at the 31st IEEE International Symposium on Computer Arithmetic (ARITH 2024)}
}

@misc{serodio2021unum,
    title     = {Unum Type-IV: A Floating-Point Unit with Dynamically Varying Exponent and Mantissa Sizes},
    author    = {Micaela Moraes Serodio},
    year      = {2021},
    month     = {September},
    school    = {Instituto Superior Tecnico, University of Lisbon},
    type      = {Master's Thesis},
    address   = {Lisbon, Portugal}
}

@inproceedings{darvish_rouhani_shared_2023,
	address = {New York, NY, USA},
	series = {{ISCA} '23},
	title = {With {Shared} {Microexponents}, {A} {Little} {Shifting} {Goes} a {Long} {Way}},
	isbn = {9798400700958},
	url = {https://doi.org/10.1145/3579371.3589351},
	doi = {10.1145/3579371.3589351},
	abstract = {This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.},
	booktitle = {Proceedings of the 50th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Darvish Rouhani, Bita and Zhao, Ritchie and Elango, Venmugil and Shafipour, Rasoul and Hall, Mathew and Mesmakhosroshahi, Maral and More, Ankit and Melnick, Levi and Golub, Maximilian and Varatkar, Girish and Shao, Lai and Kolhe, Gaurav and Melts, Dimitry and Klar, Jasmine and L'Heureux, Renee and Perry, Matt and Burger, Doug and Chung, Eric and Deng, Zhaoxia (Summer) and Naghshineh, Sam and Park, Jongsoo and Naumov, Maxim},
	month = jun,
	year = {2023},
	pages = {1--13},
}

@misc{rouhani_microscaling_2023,
	title = {Microscaling {Data} {Formats} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2310.10537},
	doi = {10.48550/arXiv.2310.10537},
	abstract = {Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.},
	publisher = {arXiv},
	author = {Rouhani, Bita Darvish and Zhao, Ritchie and More, Ankit and Hall, Mathew and Khodamoradi, Alireza and Deng, Summer and Choudhary, Dhruv and Cornea, Marius and Dellinger, Eric and Denolf, Kristof and Dusan, Stosic and Elango, Venmugil and Golub, Maximilian and Heinecke, Alexander and James-Roxby, Phil and Jani, Dharmesh and Kolhe, Gaurav and Langhammer, Martin and Li, Ada and Melnick, Levi and Mesmakhosroshahi, Maral and Rodriguez, Andres and Schulte, Michael and Shafipour, Rasoul and Shao, Lei and Siu, Michael and Dubey, Pradeep and Micikevicius, Paulius and Naumov, Maxim and Verrilli, Colin and Wittig, Ralph and Burger, Doug and Chung, Eric},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{talpes_dojo_2022,
	title = {{DOJO}: {The} {Microarchitecture} of {Tesla}’s {Exa}-{Scale} {Computer}},
	shorttitle = {{DOJO}},
	url = {https://ieeexplore.ieee.org/document/9895534},
	doi = {10.1109/HCS55958.2022.9895534},
	abstract = {Tesla’s in-house supercomputer for Machine Learning},
	booktitle = {2022 {IEEE} {Hot} {Chips} 34 {Symposium} ({HCS})},
	author = {Talpes, Emil and Williams, Douglas and Sarma, Debjit Das},
	month = aug,
	year = {2022},
	note = {ISSN: 2573-2048},
	keywords = {Machine learning, Microarchitecture, Supercomputers},
	pages = {1--28},
}
